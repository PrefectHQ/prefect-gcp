{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"prefect-gcp Welcome! prefect-gcp is a collection of prebuilt Prefect tasks that can be used to quickly construct Prefect flows. Getting Started Python setup Requires an installation of Python 3.7+. We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation . Installation Install prefect-gcp with pip : pip install prefect-gcp To use Cloud Storage: pip install \"prefect-gcp[cloud_storage]\" To use BigQuery: pip install \"prefect-gcp[bigquery]\" To use Secret Manager: pip install \"prefect-gcp[secret_manager]\" Write and run a flow from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = cloud_storage_download_blob_as_bytes ( \"bucket\" , \"blob\" , gcp_credentials ) return contents example_cloud_storage_download_blob_flow () Resources If you encounter any bugs while using prefect-gcp , feel free to open an issue in the prefect-gcp repository. If you have any questions or issues while using prefect-gcp , you can find help in either the Prefect Discourse forum or the Prefect Slack community . Development If you'd like to install a version of prefect-gcp for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-gcp.git cd prefect-gcp/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Home"},{"location":"#prefect-gcp","text":"","title":"prefect-gcp"},{"location":"#welcome","text":"prefect-gcp is a collection of prebuilt Prefect tasks that can be used to quickly construct Prefect flows.","title":"Welcome!"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#python-setup","text":"Requires an installation of Python 3.7+. We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation .","title":"Python setup"},{"location":"#installation","text":"Install prefect-gcp with pip : pip install prefect-gcp To use Cloud Storage: pip install \"prefect-gcp[cloud_storage]\" To use BigQuery: pip install \"prefect-gcp[bigquery]\" To use Secret Manager: pip install \"prefect-gcp[secret_manager]\"","title":"Installation"},{"location":"#write-and-run-a-flow","text":"from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = cloud_storage_download_blob_as_bytes ( \"bucket\" , \"blob\" , gcp_credentials ) return contents example_cloud_storage_download_blob_flow ()","title":"Write and run a flow"},{"location":"#resources","text":"If you encounter any bugs while using prefect-gcp , feel free to open an issue in the prefect-gcp repository. If you have any questions or issues while using prefect-gcp , you can find help in either the Prefect Discourse forum or the Prefect Slack community .","title":"Resources"},{"location":"#development","text":"If you'd like to install a version of prefect-gcp for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-gcp.git cd prefect-gcp/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Development"},{"location":"bigquery/","text":"prefect_gcp.bigquery Tasks for interacting with GCP BigQuery bigquery_create_table async Creates table in BigQuery. Parameters: Name Type Description Default dataset str Name of a dataset in that the table will be created. required table str Name of a table to create. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required clustering_fields List[str] List of fields to cluster the table by. None time_partitioning TimePartitioning bigquery.TimePartitioning object specifying a partitioning of the newly created table None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str The location of the dataset that will be written to. 'US' external_config Optional[google.cloud.bigquery.external_config.ExternalConfig] The external data source . # noqa None Returns: Type Description str Table name. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) schema = [ SchemaField ( \"number\" , field_type = \"INTEGER\" , mode = \"REQUIRED\" ), SchemaField ( \"text\" , field_type = \"STRING\" , mode = \"REQUIRED\" ), SchemaField ( \"bool\" , field_type = \"BOOLEAN\" ) ] result = bigquery_create_table ( dataset = \"dataset\" , table = \"test_table\" , schema = schema , gcp_credentials = gcp_credentials ) return result example_bigquery_create_table_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_create_table ( dataset : str , table : str , gcp_credentials : \"GcpCredentials\" , schema : Optional [ List [ SchemaField ]] = None , clustering_fields : List [ str ] = None , time_partitioning : TimePartitioning = None , project : Optional [ str ] = None , location : str = \"US\" , external_config : Optional [ ExternalConfig ] = None , ) -> str : \"\"\" Creates table in BigQuery. Args: dataset: Name of a dataset in that the table will be created. table: Name of a table to create. schema: Schema to use when creating the table. gcp_credentials: Credentials to use for authentication with GCP. clustering_fields: List of fields to cluster the table by. time_partitioning: `bigquery.TimePartitioning` object specifying a partitioning of the newly created table project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: The location of the dataset that will be written to. external_config: The [external data source](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_table#nested_external_data_configuration). # noqa Returns: Table name. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow(): gcp_credentials = GcpCredentials(project=\"project\") schema = [ SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"), SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"), SchemaField(\"bool\", field_type=\"BOOLEAN\") ] result = bigquery_create_table( dataset=\"dataset\", table=\"test_table\", schema=schema, gcp_credentials=gcp_credentials ) return result example_bigquery_create_table_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s . %s \" , dataset , table ) if not external_config and not schema : raise ValueError ( \"Either a schema or an external config must be provided.\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) try : partial_get_dataset = partial ( client . get_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_get_dataset ) except NotFound : logger . debug ( \"Dataset %s not found, creating\" , dataset ) partial_create_dataset = partial ( client . create_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_create_dataset ) table_ref = dataset_ref . table ( table ) try : partial_get_table = partial ( client . get_table , table_ref ) await to_thread . run_sync ( partial_get_table ) logger . info ( \" %s . %s already exists\" , dataset , table ) except NotFound : logger . debug ( \"Table %s not found, creating\" , table ) table_obj = Table ( table_ref , schema = schema ) # external data configuration if external_config : table_obj . external_data_configuration = external_config # cluster for optimal data sorting/access if clustering_fields : table_obj . clustering_fields = clustering_fields # partitioning if time_partitioning : table_obj . time_partitioning = time_partitioning partial_create_table = partial ( client . create_table , table_obj ) await to_thread . run_sync ( partial_create_table ) return table bigquery_insert_stream async Insert records in a Google BigQuery table via the streaming API . Parameters: Name Type Description Default dataset str Name of a dataset where the records will be written to. required table str Name of a table to write to. required records List[dict] The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description List List of inserted rows. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) records = [ { \"number\" : 1 , \"text\" : \"abc\" , \"bool\" : True }, { \"number\" : 2 , \"text\" : \"def\" , \"bool\" : False }, ] result = bigquery_insert_stream ( dataset = \"integrations\" , table = \"test_table\" , records = records , gcp_credentials = gcp_credentials ) return result example_bigquery_insert_stream_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_insert_stream ( dataset : str , table : str , records : List [ dict ], gcp_credentials : \"GcpCredentials\" , project : Optional [ str ] = None , location : str = \"US\" , ) -> List : \"\"\" Insert records in a Google BigQuery table via the [streaming API](https://cloud.google.com/bigquery/streaming-data-into-bigquery). Args: dataset: Name of a dataset where the records will be written to. table: Name of a table to write to. records: The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. gcp_credentials: Credentials to use for authentication with GCP. project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: List of inserted rows. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow(): gcp_credentials = GcpCredentials(project=\"project\") records = [ {\"number\": 1, \"text\": \"abc\", \"bool\": True}, {\"number\": 2, \"text\": \"def\", \"bool\": False}, ] result = bigquery_insert_stream( dataset=\"integrations\", table=\"test_table\", records=records, gcp_credentials=gcp_credentials ) return result example_bigquery_insert_stream_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Inserting into %s . %s as a stream\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) partial_insert = partial ( client . insert_rows_json , table = table_ref , json_rows = records ) response = await to_thread . run_sync ( partial_insert ) errors = [] output = [] for row in response : output . append ( row ) if \"errors\" in row : errors . append ( row [ \"errors\" ]) if errors : raise ValueError ( errors ) return output bigquery_load_cloud_storage async Run method for this Task. Invoked by calling this Task within a Flow context, after initialization. Parameters: Name Type Description Default uri str GCS path to load data from. required dataset str The id of a destination dataset to write the records to. required table str The name of a destination table to write the records to. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] The schema to use when creating the table. None job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_uri . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_cloud_storage ( dataset = \"dataset\" , table = \"test_table\" , uri = \"uri\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_cloud_storage_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_cloud_storage ( dataset : str , table : str , uri : str , gcp_credentials : \"GcpCredentials\" , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Run method for this Task. Invoked by _calling_ this Task within a Flow context, after initialization. Args: uri: GCS path to load data from. dataset: The id of a destination dataset to write the records to. table: The name of a destination table to write the records to. gcp_credentials: Credentials to use for authentication with GCP. schema: The schema to use when creating the table. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: The response from `load_table_from_uri`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_cloud_storage( dataset=\"dataset\", table=\"test_table\", uri=\"uri\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_cloud_storage_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from cloud storage\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True job_config = LoadJobConfig ( ** job_config ) if schema : job_config . schema = schema result = None try : partial_load = partial ( _result_sync , client . load_table_from_uri , uri , table_ref , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except Exception as exception : logger . exception ( exception ) if result is not None and result . errors is not None : for error in result . errors : logger . exception ( error ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result bigquery_load_file async Loads file into BigQuery. Parameters: Name Type Description Default dataset str ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. required table str Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. required path Union[str, pathlib.Path] A string or path-like object of the file to be loaded. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None job_config Optional[dict] An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None rewind bool if True, seek to the beginning of the file handle before reading the file. False size Optional[int] Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_file . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_file ( dataset = \"dataset\" , table = \"test_table\" , path = \"path\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_file_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_file ( dataset : str , table : str , path : Union [ str , Path ], gcp_credentials : \"GcpCredentials\" , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , rewind : bool = False , size : Optional [ int ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Loads file into BigQuery. Args: dataset: ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. table: Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. path: A string or path-like object of the file to be loaded. gcp_credentials: Credentials to use for authentication with GCP. schema: Schema to use when creating the table. job_config: An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). rewind: if True, seek to the beginning of the file handle before reading the file. size: Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: location of the dataset that will be written to. Returns: The response from `load_table_from_file`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_file( dataset=\"dataset\", table=\"test_table\", path=\"path\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from file\" , dataset , table ) if not os . path . exists ( path ): raise ValueError ( f \" { path } does not exist\" ) elif not os . path . isfile ( path ): raise ValueError ( f \" { path } is not a file\" ) client = gcp_credentials . get_bigquery_client ( project = project ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True # TODO: test if autodetect is needed when schema is passed job_config = LoadJobConfig ( ** job_config ) if schema : # TODO: test if schema can be passed directly in job_config job_config . schema = schema try : with open ( path , \"rb\" ) as file_obj : partial_load = partial ( _result_sync , client . load_table_from_file , file_obj , table_ref , rewind = rewind , size = size , location = location , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except IOError : logger . exception ( f \"Could not open and read from { path } \" ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result bigquery_query async Runs a BigQuery query. Parameters: Name Type Description Default query str String of the query to execute. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required query_params Optional[List[tuple]] List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the Google documentation for more details on how both the query and the query parameters should be formatted. None dry_run_max_bytes Optional[int] If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a ValueError if the maximum is exceeded. None dataset Optional[str] Name of a destination dataset to write the query results to, if you don't want them returned; if provided, table must also be provided. None table Optional[str] Name of a destination table to write the query results to, if you don't want them returned; if provided, dataset must also be provided. None to_dataframe bool If provided, returns the results of the query as a pandas dataframe instead of a list of bigquery.table.Row objects. False job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be queried. 'US' Returns: Type Description List[Row] A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Examples: Queries the public names database, returning 10 results. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" , project = \"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ ( \"corpus\" , \"STRING\" , \"romeoandjuliet\" ), ( \"min_word_count\" , \"INT64\" , 250 ) ] result = bigquery_query ( query , gcp_credentials , query_params = query_params ) return result example_bigquery_query_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_query ( query : str , gcp_credentials : \"GcpCredentials\" , query_params : Optional [ List [ tuple ]] = None , # 3-tuples dry_run_max_bytes : Optional [ int ] = None , dataset : Optional [ str ] = None , table : Optional [ str ] = None , to_dataframe : bool = False , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> List [ \"Row\" ]: \"\"\" Runs a BigQuery query. Args: query: String of the query to execute. gcp_credentials: Credentials to use for authentication with GCP. query_params: List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the [Google documentation](https://cloud.google.com/bigquery/docs/parameterized-queries#bigquery-query-params-python) for more details on how both the query and the query parameters should be formatted. dry_run_max_bytes: If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a `ValueError` if the maximum is exceeded. dataset: Name of a destination dataset to write the query results to, if you don't want them returned; if provided, `table` must also be provided. table: Name of a destination table to write the query results to, if you don't want them returned; if provided, `dataset` must also be provided. to_dataframe: If provided, returns the results of the query as a pandas dataframe instead of a list of `bigquery.table.Row` objects. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be queried. Returns: A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Example: Queries the public names database, returning 10 results. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\", project=\"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ (\"corpus\", \"STRING\", \"romeoandjuliet\"), (\"min_word_count\", \"INT64\", 250) ] result = bigquery_query( query, gcp_credentials, query_params=query_params ) return result example_bigquery_query_flow() ``` \"\"\" # noqa logger = get_run_logger () logger . info ( \"Running BigQuery query\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) # setup job config job_config = QueryJobConfig ( ** job_config or {}) if query_params is not None : job_config . query_parameters = [ ScalarQueryParameter ( * qp ) for qp in query_params ] # perform dry_run if requested if dry_run_max_bytes is not None : saved_info = dict ( dry_run = job_config . dry_run , use_query_cache = job_config . use_query_cache ) job_config . dry_run = True job_config . use_query_cache = False partial_query = partial ( client . query , query , job_config = job_config ) response = await to_thread . run_sync ( partial_query ) total_bytes_processed = response . total_bytes_processed if total_bytes_processed > dry_run_max_bytes : raise RuntimeError ( f \"Query will process { total_bytes_processed } bytes which is above \" f \"the set maximum of { dry_run_max_bytes } for this task.\" ) job_config . dry_run = saved_info [ \"dry_run\" ] job_config . use_query_cache = saved_info [ \"use_query_cache\" ] # if writing to a destination table if dataset is not None : table_ref = client . dataset ( dataset ) . table ( table ) job_config . destination = table_ref partial_query = partial ( _result_sync , client . query , query , job_config = job_config , ) result = await to_thread . run_sync ( partial_query ) if to_dataframe : return result . to_dataframe () else : return list ( result )","title":"BigQuery"},{"location":"bigquery/#prefect_gcp.bigquery","text":"Tasks for interacting with GCP BigQuery","title":"bigquery"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_create_table","text":"Creates table in BigQuery. Parameters: Name Type Description Default dataset str Name of a dataset in that the table will be created. required table str Name of a table to create. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required clustering_fields List[str] List of fields to cluster the table by. None time_partitioning TimePartitioning bigquery.TimePartitioning object specifying a partitioning of the newly created table None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str The location of the dataset that will be written to. 'US' external_config Optional[google.cloud.bigquery.external_config.ExternalConfig] The external data source . # noqa None Returns: Type Description str Table name. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) schema = [ SchemaField ( \"number\" , field_type = \"INTEGER\" , mode = \"REQUIRED\" ), SchemaField ( \"text\" , field_type = \"STRING\" , mode = \"REQUIRED\" ), SchemaField ( \"bool\" , field_type = \"BOOLEAN\" ) ] result = bigquery_create_table ( dataset = \"dataset\" , table = \"test_table\" , schema = schema , gcp_credentials = gcp_credentials ) return result example_bigquery_create_table_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_create_table ( dataset : str , table : str , gcp_credentials : \"GcpCredentials\" , schema : Optional [ List [ SchemaField ]] = None , clustering_fields : List [ str ] = None , time_partitioning : TimePartitioning = None , project : Optional [ str ] = None , location : str = \"US\" , external_config : Optional [ ExternalConfig ] = None , ) -> str : \"\"\" Creates table in BigQuery. Args: dataset: Name of a dataset in that the table will be created. table: Name of a table to create. schema: Schema to use when creating the table. gcp_credentials: Credentials to use for authentication with GCP. clustering_fields: List of fields to cluster the table by. time_partitioning: `bigquery.TimePartitioning` object specifying a partitioning of the newly created table project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: The location of the dataset that will be written to. external_config: The [external data source](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_table#nested_external_data_configuration). # noqa Returns: Table name. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow(): gcp_credentials = GcpCredentials(project=\"project\") schema = [ SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"), SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"), SchemaField(\"bool\", field_type=\"BOOLEAN\") ] result = bigquery_create_table( dataset=\"dataset\", table=\"test_table\", schema=schema, gcp_credentials=gcp_credentials ) return result example_bigquery_create_table_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s . %s \" , dataset , table ) if not external_config and not schema : raise ValueError ( \"Either a schema or an external config must be provided.\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) try : partial_get_dataset = partial ( client . get_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_get_dataset ) except NotFound : logger . debug ( \"Dataset %s not found, creating\" , dataset ) partial_create_dataset = partial ( client . create_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_create_dataset ) table_ref = dataset_ref . table ( table ) try : partial_get_table = partial ( client . get_table , table_ref ) await to_thread . run_sync ( partial_get_table ) logger . info ( \" %s . %s already exists\" , dataset , table ) except NotFound : logger . debug ( \"Table %s not found, creating\" , table ) table_obj = Table ( table_ref , schema = schema ) # external data configuration if external_config : table_obj . external_data_configuration = external_config # cluster for optimal data sorting/access if clustering_fields : table_obj . clustering_fields = clustering_fields # partitioning if time_partitioning : table_obj . time_partitioning = time_partitioning partial_create_table = partial ( client . create_table , table_obj ) await to_thread . run_sync ( partial_create_table ) return table","title":"bigquery_create_table()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_insert_stream","text":"Insert records in a Google BigQuery table via the streaming API . Parameters: Name Type Description Default dataset str Name of a dataset where the records will be written to. required table str Name of a table to write to. required records List[dict] The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description List List of inserted rows. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) records = [ { \"number\" : 1 , \"text\" : \"abc\" , \"bool\" : True }, { \"number\" : 2 , \"text\" : \"def\" , \"bool\" : False }, ] result = bigquery_insert_stream ( dataset = \"integrations\" , table = \"test_table\" , records = records , gcp_credentials = gcp_credentials ) return result example_bigquery_insert_stream_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_insert_stream ( dataset : str , table : str , records : List [ dict ], gcp_credentials : \"GcpCredentials\" , project : Optional [ str ] = None , location : str = \"US\" , ) -> List : \"\"\" Insert records in a Google BigQuery table via the [streaming API](https://cloud.google.com/bigquery/streaming-data-into-bigquery). Args: dataset: Name of a dataset where the records will be written to. table: Name of a table to write to. records: The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. gcp_credentials: Credentials to use for authentication with GCP. project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: List of inserted rows. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow(): gcp_credentials = GcpCredentials(project=\"project\") records = [ {\"number\": 1, \"text\": \"abc\", \"bool\": True}, {\"number\": 2, \"text\": \"def\", \"bool\": False}, ] result = bigquery_insert_stream( dataset=\"integrations\", table=\"test_table\", records=records, gcp_credentials=gcp_credentials ) return result example_bigquery_insert_stream_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Inserting into %s . %s as a stream\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) partial_insert = partial ( client . insert_rows_json , table = table_ref , json_rows = records ) response = await to_thread . run_sync ( partial_insert ) errors = [] output = [] for row in response : output . append ( row ) if \"errors\" in row : errors . append ( row [ \"errors\" ]) if errors : raise ValueError ( errors ) return output","title":"bigquery_insert_stream()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_cloud_storage","text":"Run method for this Task. Invoked by calling this Task within a Flow context, after initialization. Parameters: Name Type Description Default uri str GCS path to load data from. required dataset str The id of a destination dataset to write the records to. required table str The name of a destination table to write the records to. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] The schema to use when creating the table. None job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_uri . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_cloud_storage ( dataset = \"dataset\" , table = \"test_table\" , uri = \"uri\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_cloud_storage_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_cloud_storage ( dataset : str , table : str , uri : str , gcp_credentials : \"GcpCredentials\" , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Run method for this Task. Invoked by _calling_ this Task within a Flow context, after initialization. Args: uri: GCS path to load data from. dataset: The id of a destination dataset to write the records to. table: The name of a destination table to write the records to. gcp_credentials: Credentials to use for authentication with GCP. schema: The schema to use when creating the table. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: The response from `load_table_from_uri`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_cloud_storage( dataset=\"dataset\", table=\"test_table\", uri=\"uri\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_cloud_storage_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from cloud storage\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True job_config = LoadJobConfig ( ** job_config ) if schema : job_config . schema = schema result = None try : partial_load = partial ( _result_sync , client . load_table_from_uri , uri , table_ref , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except Exception as exception : logger . exception ( exception ) if result is not None and result . errors is not None : for error in result . errors : logger . exception ( error ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result","title":"bigquery_load_cloud_storage()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_file","text":"Loads file into BigQuery. Parameters: Name Type Description Default dataset str ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. required table str Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. required path Union[str, pathlib.Path] A string or path-like object of the file to be loaded. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None job_config Optional[dict] An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None rewind bool if True, seek to the beginning of the file handle before reading the file. False size Optional[int] Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_file . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_file ( dataset = \"dataset\" , table = \"test_table\" , path = \"path\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_file_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_file ( dataset : str , table : str , path : Union [ str , Path ], gcp_credentials : \"GcpCredentials\" , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , rewind : bool = False , size : Optional [ int ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Loads file into BigQuery. Args: dataset: ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. table: Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. path: A string or path-like object of the file to be loaded. gcp_credentials: Credentials to use for authentication with GCP. schema: Schema to use when creating the table. job_config: An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). rewind: if True, seek to the beginning of the file handle before reading the file. size: Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: location of the dataset that will be written to. Returns: The response from `load_table_from_file`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_file( dataset=\"dataset\", table=\"test_table\", path=\"path\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from file\" , dataset , table ) if not os . path . exists ( path ): raise ValueError ( f \" { path } does not exist\" ) elif not os . path . isfile ( path ): raise ValueError ( f \" { path } is not a file\" ) client = gcp_credentials . get_bigquery_client ( project = project ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True # TODO: test if autodetect is needed when schema is passed job_config = LoadJobConfig ( ** job_config ) if schema : # TODO: test if schema can be passed directly in job_config job_config . schema = schema try : with open ( path , \"rb\" ) as file_obj : partial_load = partial ( _result_sync , client . load_table_from_file , file_obj , table_ref , rewind = rewind , size = size , location = location , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except IOError : logger . exception ( f \"Could not open and read from { path } \" ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result","title":"bigquery_load_file()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_query","text":"Runs a BigQuery query. Parameters: Name Type Description Default query str String of the query to execute. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required query_params Optional[List[tuple]] List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the Google documentation for more details on how both the query and the query parameters should be formatted. None dry_run_max_bytes Optional[int] If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a ValueError if the maximum is exceeded. None dataset Optional[str] Name of a destination dataset to write the query results to, if you don't want them returned; if provided, table must also be provided. None table Optional[str] Name of a destination table to write the query results to, if you don't want them returned; if provided, dataset must also be provided. None to_dataframe bool If provided, returns the results of the query as a pandas dataframe instead of a list of bigquery.table.Row objects. False job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be queried. 'US' Returns: Type Description List[Row] A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Examples: Queries the public names database, returning 10 results. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" , project = \"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ ( \"corpus\" , \"STRING\" , \"romeoandjuliet\" ), ( \"min_word_count\" , \"INT64\" , 250 ) ] result = bigquery_query ( query , gcp_credentials , query_params = query_params ) return result example_bigquery_query_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_query ( query : str , gcp_credentials : \"GcpCredentials\" , query_params : Optional [ List [ tuple ]] = None , # 3-tuples dry_run_max_bytes : Optional [ int ] = None , dataset : Optional [ str ] = None , table : Optional [ str ] = None , to_dataframe : bool = False , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> List [ \"Row\" ]: \"\"\" Runs a BigQuery query. Args: query: String of the query to execute. gcp_credentials: Credentials to use for authentication with GCP. query_params: List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the [Google documentation](https://cloud.google.com/bigquery/docs/parameterized-queries#bigquery-query-params-python) for more details on how both the query and the query parameters should be formatted. dry_run_max_bytes: If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a `ValueError` if the maximum is exceeded. dataset: Name of a destination dataset to write the query results to, if you don't want them returned; if provided, `table` must also be provided. table: Name of a destination table to write the query results to, if you don't want them returned; if provided, `dataset` must also be provided. to_dataframe: If provided, returns the results of the query as a pandas dataframe instead of a list of `bigquery.table.Row` objects. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be queried. Returns: A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Example: Queries the public names database, returning 10 results. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\", project=\"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ (\"corpus\", \"STRING\", \"romeoandjuliet\"), (\"min_word_count\", \"INT64\", 250) ] result = bigquery_query( query, gcp_credentials, query_params=query_params ) return result example_bigquery_query_flow() ``` \"\"\" # noqa logger = get_run_logger () logger . info ( \"Running BigQuery query\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) # setup job config job_config = QueryJobConfig ( ** job_config or {}) if query_params is not None : job_config . query_parameters = [ ScalarQueryParameter ( * qp ) for qp in query_params ] # perform dry_run if requested if dry_run_max_bytes is not None : saved_info = dict ( dry_run = job_config . dry_run , use_query_cache = job_config . use_query_cache ) job_config . dry_run = True job_config . use_query_cache = False partial_query = partial ( client . query , query , job_config = job_config ) response = await to_thread . run_sync ( partial_query ) total_bytes_processed = response . total_bytes_processed if total_bytes_processed > dry_run_max_bytes : raise RuntimeError ( f \"Query will process { total_bytes_processed } bytes which is above \" f \"the set maximum of { dry_run_max_bytes } for this task.\" ) job_config . dry_run = saved_info [ \"dry_run\" ] job_config . use_query_cache = saved_info [ \"use_query_cache\" ] # if writing to a destination table if dataset is not None : table_ref = client . dataset ( dataset ) . table ( table ) job_config . destination = table_ref partial_query = partial ( _result_sync , client . query , query , job_config = job_config , ) result = await to_thread . run_sync ( partial_query ) if to_dataframe : return result . to_dataframe () else : return list ( result )","title":"bigquery_query()"},{"location":"cloud_run/","text":"prefect_gcp.cloud_run Integrations with Google Cloud Run Job. Note this module is experimental. The intefaces within may change without notice. Examples: Run a job using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials ) . run () Run a job that runs the command echo hello world using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials command = [ \"echo\" , \"hello world\" ] ) . run () CloudRunJob pydantic-model Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. Source code in prefect_gcp/cloud_run.py class CloudRunJob ( Infrastructure ): \"\"\" <span class=\"badge-api experimental\"/> Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. \"\"\" _block_type_slug = \"cloud-run-job\" _block_type_name = \"GCP Cloud Run Job\" _description = \"Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.\" # noqa _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa type : Literal [ \"cloud-run-job\" ] = Field ( \"cloud-run-job\" , description = \"The slug for this task type.\" ) image : str = Field ( ... , title = \"Image Name\" , description = ( \"The image to use for a new Cloud Run Job. This value must \" \"refer to an image within either Google Container Registry \" \"or Google Artifact Registry.\" ), ) region : str = Field ( ... , description = \"The region where the Cloud Run Job resides.\" ) credentials : GcpCredentials # cannot be Field; else it shows as Json # Job settings cpu : Optional [ int ] = Field ( default = None , title = \"CPU\" , description = ( \"The amount of compute allocated to the Cloud Run Job. \" \"The int must be valid based on the rules specified at \" \"https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .\" ), ) memory : Optional [ int ] = Field ( default = None , title = \"Memory\" , description = \"The amount of memory allocated to the Cloud Run Job.\" , ) memory_unit : Optional [ Literal [ \"G\" , \"Gi\" , \"M\" , \"Mi\" ]] = Field ( default = None , title = \"Memory Units\" , description = ( \"The unit of memory. See \" \"https://cloud.google.com/run/docs/configuring/memory-limits#setting \" \"for additional details.\" ), ) args : Optional [ List [ str ]] = Field ( default = None , description = ( \"Arguments to be passed to your Cloud Run Job's entrypoint command.\" ), ) env : Dict [ str , str ] = Field ( default_factory = dict , description = \"Environment variables to be passed to your Cloud Run Job.\" , ) # Cleanup behavior keep_job : Optional [ bool ] = Field ( default = False , title = \"Keep Job After Completion\" , description = \"Keep the completed Cloud Run Job on Google Cloud Platform.\" , ) timeout : Optional [ int ] = Field ( default = None , title = \"Job Timeout\" , description = ( \"The length of time that Prefect will wait for a Cloud Run Job to complete \" \"before raising an exception.\" ), ) # For private use _job_name : str = None _execution : Optional [ Execution ] = None @property def job_name ( self ): \"\"\"Create a unique and valid job name.\"\"\" if self . _job_name is None : # get `repo` from `gcr.io/<project_name>/repo/other` components = self . image . split ( \"/\" ) image_name = components [ 2 ] # only alphanumeric and '-' allowed for a job name modified_image_name = image_name . replace ( \":\" , \"-\" ) . replace ( \".\" , \"-\" ) # make 50 char limit for final job name, which will be '<name>-<uuid>' if len ( modified_image_name ) > 17 : modified_image_name = modified_image_name [: 17 ] name = f \" { modified_image_name } - { uuid4 () . hex } \" self . _job_name = name return self . _job_name @property def memory_string ( self ): \"\"\"Returns the string expected for memory resources argument.\"\"\" if self . memory and self . memory_unit : return str ( self . memory ) + self . memory_unit return None @validator ( \"image\" ) def _remove_image_spaces ( cls , value ): \"\"\"Deal with spaces in image names.\"\"\" if value is not None : return value . strip () @validator ( \"cpu\" ) def _convert_cpu_to_k8s_quantity ( cls , value ): \"\"\"Set CPU integer to the format expected by API. See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ See also: https://cloud.google.com/run/docs/configuring/cpu#setting-jobs \"\"\" # noqa return str ( value * 1000 ) + \"m\" @root_validator def _check_valid_memory ( cls , values ): \"\"\"Make sure memory conforms to expected values for API. See: https://cloud.google.com/run/docs/configuring/memory-limits#setting \"\"\" # noqa if ( values . get ( \"memory\" ) is not None and values . get ( \"memory_units\" ) is None ) or ( values . get ( \"memory_units\" ) is not None and values . get ( \"memory\" ) is None ): raise ValueError ( \"A memory value and unit must both be supplied to specify a memory\" \" value other than the default memory value.\" ) return values def _create_job_error ( self , exc ): \"\"\"Provides a nicer error for 404s when trying to create a Cloud Run Job.\"\"\" # TODO consider lookup table instead of the if/else, # also check for documented errors if exc . status_code == 404 : raise RuntimeError ( f \"Failed to find resources at { exc . uri } . Confirm that region\" f \" ' { self . region } ' is the correct region for your Cloud Run Job and\" f \" that { self . credentials . project } is the correct GCP project. If\" f \" your project ID is not correct, you are using a Credentials block\" f \" with permissions for the wrong project.\" ) from exc raise exc def _job_run_submission_error ( self , exc ): \"\"\"Provides a nicer error for 404s when submitting job runs.\"\"\" if exc . status_code == 404 : pat1 = r \"The requested URL [^ ]+ was not found on this server\" # pat2 = ( # r\"Resource '[^ ]+' of kind 'JOB' in region '[\\w\\-0-9]+' \" # r\"in project '[\\w\\-0-9]+' does not exist\" # ) if re . findall ( pat1 , str ( exc )): raise RuntimeError ( f \"Failed to find resources at { exc . uri } . \" f \"Confirm that region ' { self . region } ' is \" f \"the correct region for your Cloud Run Job \" f \"and that ' { self . credentials . project } ' is the \" f \"correct GCP project. If your project ID is not \" f \"correct, you are using a Credentials \" f \"block with permissions for the wrong project.\" ) from exc else : raise exc raise exc @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result def _create_job_and_wait_for_registration ( self , client : Resource ) -> None : \"\"\"Create a new job wait for it to finish registering.\"\"\" try : self . logger . info ( f \"Creating Cloud Run Job { self . job_name } \" ) Job . create ( client = client , namespace = self . credentials . project , body = self . _jobs_body (), ) except googleapiclient . errors . HttpError as exc : self . _create_job_error ( exc ) try : self . _wait_for_job_creation ( client = client , timeout = self . timeout ) except Exception : self . logger . exception ( \"Encountered an exception while waiting for job run creation\" ) if not self . keep_job : self . logger . info ( f \"Deleting Cloud Run Job { self . job_name } from Google Cloud Run.\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete\" f \" Cloud Run Job { self . job_name !r} \" ) raise def _begin_job_execution ( self , client : Resource ) -> Execution : \"\"\"Submit a job run for execution and return the execution object.\"\"\" try : self . logger . info ( f \"Submitting Cloud Run Job { self . job_name !r} for execution.\" ) submission = Job . run ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) job_execution = Execution . get ( client = client , namespace = submission [ \"metadata\" ][ \"namespace\" ], execution_name = submission [ \"metadata\" ][ \"name\" ], ) command = ( \" \" . join ( self . command ) if self . command else \"default container command\" ) self . logger . info ( f \"Cloud Run Job { self . job_name !r} : Running command { command !r} \" ) except Exception as exc : self . _job_run_submission_error ( exc ) return job_execution def _watch_job_execution_and_get_result ( self , client : Resource , execution : Execution , poll_interval : int ) -> CloudRunJobResult : \"\"\"Wait for execution to complete and then return result.\"\"\" try : job_execution = self . _watch_job_execution ( client = client , job_execution = execution , timeout = self . timeout , poll_interval = poll_interval , ) except Exception : self . logger . exception ( \"Received an unexpected exception while monitoring Cloud Run Job \" f \" { self . job_name !r} \" ) raise if job_execution . succeeded (): status_code = 0 self . logger . info ( f \"Job Run { self . job_name } completed successfully\" ) else : status_code = 1 error_msg = job_execution . condition_after_completion ()[ \"message\" ] self . logger . error ( f \"Job Run { self . job_name } did not complete successfully. { error_msg } \" ) self . logger . info ( f \"Job Run logs can be found on GCP at: { job_execution . log_uri } \" ) if not self . keep_job : self . logger . info ( f \"Deleting completed Cloud Run Job { self . job_name !r} from Google Cloud\" \" Run...\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete Cloud\" f \" Run Job { self . job_name } \" ) return CloudRunJobResult ( identifier = self . job_name , status_code = status_code ) def _jobs_body ( self ) -> dict : \"\"\"Create properly formatted body used for a Job CREATE request. See: https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs \"\"\" jobs_metadata = { \"name\" : self . job_name , \"annotations\" : { # See: https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation # noqa \"run.googleapis.com/launch-stage\" : \"BETA\" }, } # env and command here containers = [ self . _add_container_settings ({ \"image\" : self . image })] body = { \"apiVersion\" : \"run.googleapis.com/v1\" , \"kind\" : \"Job\" , \"metadata\" : jobs_metadata , \"spec\" : { # JobSpec \"template\" : { # ExecutionTemplateSpec \"spec\" : { # ExecutionSpec \"template\" : { # TaskTemplateSpec \"spec\" : { \"containers\" : containers } # TaskSpec } }, } }, } return body def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 ) def _watch_job_execution ( self , client , job_execution : Execution , timeout : int , poll_interval : int = 5 ): \"\"\" Update job_execution status until it is no longer running or timeout is reached. \"\"\" t0 = time . time () while job_execution . is_running (): job_execution = Execution . get ( client = client , namespace = job_execution . namespace , execution_name = job_execution . name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) return job_execution def _wait_for_job_creation ( self , client : Resource , timeout : int , poll_interval : int = 5 ): \"\"\"Give created job time to register.\"\"\" job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name ) t0 = time . time () while not job . is_ready (): ready_condition = ( job . ready_condition if job . ready_condition else \"waiting for condition update\" ) self . logger . info ( f \"Job is not yet ready... Current condition: { ready_condition } \" ) job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) def _get_client ( self ) -> Resource : \"\"\"Get the base client needed for interacting with GCP APIs.\"\"\" # region needed for 'v1' API api_endpoint = f \"https:// { self . region } -run.googleapis.com\" gcp_creds = self . credentials . get_credentials_from_service_account () options = ClientOptions ( api_endpoint = api_endpoint ) return discovery . build ( \"run\" , \"v1\" , client_options = options , credentials = gcp_creds ) . namespaces () # CONTAINER SETTINGS def _add_container_settings ( self , base_settings : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\" Add settings related to containers for Cloud Run Jobs to a dictionary. Includes environment variables, entrypoint command, entrypoint arguments, and cpu and memory limits. See: https://cloud.google.com/run/docs/reference/rest/v1/Container and https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements \"\"\" # noqa container_settings = base_settings . copy () container_settings . update ( self . _add_env ()) container_settings . update ( self . _add_resources ()) container_settings . update ( self . _add_command ()) container_settings . update ( self . _add_args ()) return container_settings def _add_args ( self ) -> dict : \"\"\"Set the arguments that will be passed to the entrypoint for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"args\" : self . args } if self . args else {} def _add_command ( self ) -> dict : \"\"\"Set the command that a container will run for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"command\" : self . command } def _add_resources ( self ) -> dict : \"\"\"Set specified resources limits for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements See also: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ \"\"\" # noqa resources = { \"limits\" : {}, \"requests\" : {}} if self . cpu is not None : resources [ \"limits\" ][ \"cpu\" ] = self . cpu resources [ \"requests\" ][ \"cpu\" ] = self . cpu if self . memory_string is not None : resources [ \"limits\" ][ \"memory\" ] = self . memory_string resources [ \"requests\" ][ \"memory\" ] = self . memory_string return { \"resources\" : resources } if resources [ \"requests\" ] else {} def _add_env ( self ) -> dict : \"\"\"Add environment variables for a Cloud Run Job. Method `self._base_environment()` gets necessary Prefect environment variables from the config. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#envvar for how environment variables are specified for Cloud Run Jobs. \"\"\" # noqa env = { ** self . _base_environment (), ** self . env } cloud_run_env = [{ \"name\" : k , \"value\" : v } for k , v in env . items ()] return { \"env\" : cloud_run_env } args : List [ str ] pydantic-field Arguments to be passed to your Cloud Run Job's entrypoint command. cpu : int pydantic-field The amount of compute allocated to the Cloud Run Job. The int must be valid based on the rules specified at https://cloud.google.com/run/docs/configuring/cpu#setting-jobs . image : str pydantic-field required The image to use for a new Cloud Run Job. This value must refer to an image within either Google Container Registry or Google Artifact Registry. job_name property readonly Create a unique and valid job name. keep_job : bool pydantic-field Keep the completed Cloud Run Job on Google Cloud Platform. memory : int pydantic-field The amount of memory allocated to the Cloud Run Job. memory_string property readonly Returns the string expected for memory resources argument. memory_unit : Literal [ 'G' , 'Gi' , 'M' , 'Mi' ] pydantic-field The unit of memory. See https://cloud.google.com/run/docs/configuring/memory-limits#setting for additional details. region : str pydantic-field required The region where the Cloud Run Job resides. timeout : int pydantic-field The length of time that Prefect will wait for a Cloud Run Job to complete before raising an exception. preview Generate a preview of the job definition that will be sent to GCP. Source code in prefect_gcp/cloud_run.py def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 ) run async Run the configured job on a Google Cloud Run Job. Source code in prefect_gcp/cloud_run.py @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result CloudRunJobResult pydantic-model Result from a Cloud Run Job. Source code in prefect_gcp/cloud_run.py class CloudRunJobResult ( InfrastructureResult ): \"\"\"Result from a Cloud Run Job.\"\"\" Execution pydantic-model Utility class to call GCP executions API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Execution ( BaseModel ): \"\"\" Utility class to call GCP `executions` API and interact with the returned objects. \"\"\" name : str namespace : str metadata : dict spec : dict status : dict log_uri : str def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], ) condition_after_completion Returns Execution condition if Execution has completed. Source code in prefect_gcp/cloud_run.py def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition get classmethod Make a get request to the GCP executions API and return an Execution instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], ) is_running Returns True if Execution is not completed. Source code in prefect_gcp/cloud_run.py def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None succeeded Whether or not the Execution completed is a successful state. Source code in prefect_gcp/cloud_run.py def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False Job pydantic-model Utility class to call GCP jobs API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Job ( BaseModel ): \"\"\" Utility class to call GCP `jobs` API and interact with the returned objects. \"\"\" metadata : dict spec : dict status : dict name : str ready_condition : dict execution_status : dict def _is_missing_container ( self ): \"\"\" Check if Job status is not ready because the specified container cannot be found. \"\"\" if ( self . ready_condition . get ( \"status\" ) == \"False\" and self . ready_condition . get ( \"reason\" ) == \"ContainerMissing\" ): return True return False def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\" def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None ) @staticmethod def _get_ready_condition ( job : dict ) -> dict : \"\"\"Utility to access JSON field containing ready condition.\"\"\" if job [ \"status\" ] . get ( \"conditions\" ): for condition in job [ \"status\" ][ \"conditions\" ]: if condition [ \"type\" ] == \"Ready\" : return condition return {} @staticmethod def _get_execution_status ( job : dict ): \"\"\"Utility to access JSON field containing execution status.\"\"\" if job [ \"status\" ] . get ( \"latestCreatedExecution\" ): return job [ \"status\" ][ \"latestCreatedExecution\" ] return {} @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), ) @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response create staticmethod Make a create request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response delete staticmethod Make a delete request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response get classmethod Make a get request to the GCP jobs API and return a Job instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), ) has_execution_in_progress See if job has a run in progress. Source code in prefect_gcp/cloud_run.py def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None ) is_ready Whether a job is finished registering and ready to be executed Source code in prefect_gcp/cloud_run.py def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\" run staticmethod Make a run request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"Cloud Run"},{"location":"cloud_run/#prefect_gcp.cloud_run","text":"Integrations with Google Cloud Run Job. Note this module is experimental. The intefaces within may change without notice. Examples: Run a job using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials ) . run () Run a job that runs the command echo hello world using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials command = [ \"echo\" , \"hello world\" ] ) . run ()","title":"cloud_run"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob","text":"Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. Source code in prefect_gcp/cloud_run.py class CloudRunJob ( Infrastructure ): \"\"\" <span class=\"badge-api experimental\"/> Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. \"\"\" _block_type_slug = \"cloud-run-job\" _block_type_name = \"GCP Cloud Run Job\" _description = \"Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.\" # noqa _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa type : Literal [ \"cloud-run-job\" ] = Field ( \"cloud-run-job\" , description = \"The slug for this task type.\" ) image : str = Field ( ... , title = \"Image Name\" , description = ( \"The image to use for a new Cloud Run Job. This value must \" \"refer to an image within either Google Container Registry \" \"or Google Artifact Registry.\" ), ) region : str = Field ( ... , description = \"The region where the Cloud Run Job resides.\" ) credentials : GcpCredentials # cannot be Field; else it shows as Json # Job settings cpu : Optional [ int ] = Field ( default = None , title = \"CPU\" , description = ( \"The amount of compute allocated to the Cloud Run Job. \" \"The int must be valid based on the rules specified at \" \"https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .\" ), ) memory : Optional [ int ] = Field ( default = None , title = \"Memory\" , description = \"The amount of memory allocated to the Cloud Run Job.\" , ) memory_unit : Optional [ Literal [ \"G\" , \"Gi\" , \"M\" , \"Mi\" ]] = Field ( default = None , title = \"Memory Units\" , description = ( \"The unit of memory. See \" \"https://cloud.google.com/run/docs/configuring/memory-limits#setting \" \"for additional details.\" ), ) args : Optional [ List [ str ]] = Field ( default = None , description = ( \"Arguments to be passed to your Cloud Run Job's entrypoint command.\" ), ) env : Dict [ str , str ] = Field ( default_factory = dict , description = \"Environment variables to be passed to your Cloud Run Job.\" , ) # Cleanup behavior keep_job : Optional [ bool ] = Field ( default = False , title = \"Keep Job After Completion\" , description = \"Keep the completed Cloud Run Job on Google Cloud Platform.\" , ) timeout : Optional [ int ] = Field ( default = None , title = \"Job Timeout\" , description = ( \"The length of time that Prefect will wait for a Cloud Run Job to complete \" \"before raising an exception.\" ), ) # For private use _job_name : str = None _execution : Optional [ Execution ] = None @property def job_name ( self ): \"\"\"Create a unique and valid job name.\"\"\" if self . _job_name is None : # get `repo` from `gcr.io/<project_name>/repo/other` components = self . image . split ( \"/\" ) image_name = components [ 2 ] # only alphanumeric and '-' allowed for a job name modified_image_name = image_name . replace ( \":\" , \"-\" ) . replace ( \".\" , \"-\" ) # make 50 char limit for final job name, which will be '<name>-<uuid>' if len ( modified_image_name ) > 17 : modified_image_name = modified_image_name [: 17 ] name = f \" { modified_image_name } - { uuid4 () . hex } \" self . _job_name = name return self . _job_name @property def memory_string ( self ): \"\"\"Returns the string expected for memory resources argument.\"\"\" if self . memory and self . memory_unit : return str ( self . memory ) + self . memory_unit return None @validator ( \"image\" ) def _remove_image_spaces ( cls , value ): \"\"\"Deal with spaces in image names.\"\"\" if value is not None : return value . strip () @validator ( \"cpu\" ) def _convert_cpu_to_k8s_quantity ( cls , value ): \"\"\"Set CPU integer to the format expected by API. See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ See also: https://cloud.google.com/run/docs/configuring/cpu#setting-jobs \"\"\" # noqa return str ( value * 1000 ) + \"m\" @root_validator def _check_valid_memory ( cls , values ): \"\"\"Make sure memory conforms to expected values for API. See: https://cloud.google.com/run/docs/configuring/memory-limits#setting \"\"\" # noqa if ( values . get ( \"memory\" ) is not None and values . get ( \"memory_units\" ) is None ) or ( values . get ( \"memory_units\" ) is not None and values . get ( \"memory\" ) is None ): raise ValueError ( \"A memory value and unit must both be supplied to specify a memory\" \" value other than the default memory value.\" ) return values def _create_job_error ( self , exc ): \"\"\"Provides a nicer error for 404s when trying to create a Cloud Run Job.\"\"\" # TODO consider lookup table instead of the if/else, # also check for documented errors if exc . status_code == 404 : raise RuntimeError ( f \"Failed to find resources at { exc . uri } . Confirm that region\" f \" ' { self . region } ' is the correct region for your Cloud Run Job and\" f \" that { self . credentials . project } is the correct GCP project. If\" f \" your project ID is not correct, you are using a Credentials block\" f \" with permissions for the wrong project.\" ) from exc raise exc def _job_run_submission_error ( self , exc ): \"\"\"Provides a nicer error for 404s when submitting job runs.\"\"\" if exc . status_code == 404 : pat1 = r \"The requested URL [^ ]+ was not found on this server\" # pat2 = ( # r\"Resource '[^ ]+' of kind 'JOB' in region '[\\w\\-0-9]+' \" # r\"in project '[\\w\\-0-9]+' does not exist\" # ) if re . findall ( pat1 , str ( exc )): raise RuntimeError ( f \"Failed to find resources at { exc . uri } . \" f \"Confirm that region ' { self . region } ' is \" f \"the correct region for your Cloud Run Job \" f \"and that ' { self . credentials . project } ' is the \" f \"correct GCP project. If your project ID is not \" f \"correct, you are using a Credentials \" f \"block with permissions for the wrong project.\" ) from exc else : raise exc raise exc @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result def _create_job_and_wait_for_registration ( self , client : Resource ) -> None : \"\"\"Create a new job wait for it to finish registering.\"\"\" try : self . logger . info ( f \"Creating Cloud Run Job { self . job_name } \" ) Job . create ( client = client , namespace = self . credentials . project , body = self . _jobs_body (), ) except googleapiclient . errors . HttpError as exc : self . _create_job_error ( exc ) try : self . _wait_for_job_creation ( client = client , timeout = self . timeout ) except Exception : self . logger . exception ( \"Encountered an exception while waiting for job run creation\" ) if not self . keep_job : self . logger . info ( f \"Deleting Cloud Run Job { self . job_name } from Google Cloud Run.\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete\" f \" Cloud Run Job { self . job_name !r} \" ) raise def _begin_job_execution ( self , client : Resource ) -> Execution : \"\"\"Submit a job run for execution and return the execution object.\"\"\" try : self . logger . info ( f \"Submitting Cloud Run Job { self . job_name !r} for execution.\" ) submission = Job . run ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) job_execution = Execution . get ( client = client , namespace = submission [ \"metadata\" ][ \"namespace\" ], execution_name = submission [ \"metadata\" ][ \"name\" ], ) command = ( \" \" . join ( self . command ) if self . command else \"default container command\" ) self . logger . info ( f \"Cloud Run Job { self . job_name !r} : Running command { command !r} \" ) except Exception as exc : self . _job_run_submission_error ( exc ) return job_execution def _watch_job_execution_and_get_result ( self , client : Resource , execution : Execution , poll_interval : int ) -> CloudRunJobResult : \"\"\"Wait for execution to complete and then return result.\"\"\" try : job_execution = self . _watch_job_execution ( client = client , job_execution = execution , timeout = self . timeout , poll_interval = poll_interval , ) except Exception : self . logger . exception ( \"Received an unexpected exception while monitoring Cloud Run Job \" f \" { self . job_name !r} \" ) raise if job_execution . succeeded (): status_code = 0 self . logger . info ( f \"Job Run { self . job_name } completed successfully\" ) else : status_code = 1 error_msg = job_execution . condition_after_completion ()[ \"message\" ] self . logger . error ( f \"Job Run { self . job_name } did not complete successfully. { error_msg } \" ) self . logger . info ( f \"Job Run logs can be found on GCP at: { job_execution . log_uri } \" ) if not self . keep_job : self . logger . info ( f \"Deleting completed Cloud Run Job { self . job_name !r} from Google Cloud\" \" Run...\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete Cloud\" f \" Run Job { self . job_name } \" ) return CloudRunJobResult ( identifier = self . job_name , status_code = status_code ) def _jobs_body ( self ) -> dict : \"\"\"Create properly formatted body used for a Job CREATE request. See: https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs \"\"\" jobs_metadata = { \"name\" : self . job_name , \"annotations\" : { # See: https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation # noqa \"run.googleapis.com/launch-stage\" : \"BETA\" }, } # env and command here containers = [ self . _add_container_settings ({ \"image\" : self . image })] body = { \"apiVersion\" : \"run.googleapis.com/v1\" , \"kind\" : \"Job\" , \"metadata\" : jobs_metadata , \"spec\" : { # JobSpec \"template\" : { # ExecutionTemplateSpec \"spec\" : { # ExecutionSpec \"template\" : { # TaskTemplateSpec \"spec\" : { \"containers\" : containers } # TaskSpec } }, } }, } return body def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 ) def _watch_job_execution ( self , client , job_execution : Execution , timeout : int , poll_interval : int = 5 ): \"\"\" Update job_execution status until it is no longer running or timeout is reached. \"\"\" t0 = time . time () while job_execution . is_running (): job_execution = Execution . get ( client = client , namespace = job_execution . namespace , execution_name = job_execution . name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) return job_execution def _wait_for_job_creation ( self , client : Resource , timeout : int , poll_interval : int = 5 ): \"\"\"Give created job time to register.\"\"\" job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name ) t0 = time . time () while not job . is_ready (): ready_condition = ( job . ready_condition if job . ready_condition else \"waiting for condition update\" ) self . logger . info ( f \"Job is not yet ready... Current condition: { ready_condition } \" ) job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) def _get_client ( self ) -> Resource : \"\"\"Get the base client needed for interacting with GCP APIs.\"\"\" # region needed for 'v1' API api_endpoint = f \"https:// { self . region } -run.googleapis.com\" gcp_creds = self . credentials . get_credentials_from_service_account () options = ClientOptions ( api_endpoint = api_endpoint ) return discovery . build ( \"run\" , \"v1\" , client_options = options , credentials = gcp_creds ) . namespaces () # CONTAINER SETTINGS def _add_container_settings ( self , base_settings : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\" Add settings related to containers for Cloud Run Jobs to a dictionary. Includes environment variables, entrypoint command, entrypoint arguments, and cpu and memory limits. See: https://cloud.google.com/run/docs/reference/rest/v1/Container and https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements \"\"\" # noqa container_settings = base_settings . copy () container_settings . update ( self . _add_env ()) container_settings . update ( self . _add_resources ()) container_settings . update ( self . _add_command ()) container_settings . update ( self . _add_args ()) return container_settings def _add_args ( self ) -> dict : \"\"\"Set the arguments that will be passed to the entrypoint for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"args\" : self . args } if self . args else {} def _add_command ( self ) -> dict : \"\"\"Set the command that a container will run for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"command\" : self . command } def _add_resources ( self ) -> dict : \"\"\"Set specified resources limits for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements See also: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ \"\"\" # noqa resources = { \"limits\" : {}, \"requests\" : {}} if self . cpu is not None : resources [ \"limits\" ][ \"cpu\" ] = self . cpu resources [ \"requests\" ][ \"cpu\" ] = self . cpu if self . memory_string is not None : resources [ \"limits\" ][ \"memory\" ] = self . memory_string resources [ \"requests\" ][ \"memory\" ] = self . memory_string return { \"resources\" : resources } if resources [ \"requests\" ] else {} def _add_env ( self ) -> dict : \"\"\"Add environment variables for a Cloud Run Job. Method `self._base_environment()` gets necessary Prefect environment variables from the config. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#envvar for how environment variables are specified for Cloud Run Jobs. \"\"\" # noqa env = { ** self . _base_environment (), ** self . env } cloud_run_env = [{ \"name\" : k , \"value\" : v } for k , v in env . items ()] return { \"env\" : cloud_run_env }","title":"CloudRunJob"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.args","text":"Arguments to be passed to your Cloud Run Job's entrypoint command.","title":"args"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.cpu","text":"The amount of compute allocated to the Cloud Run Job. The int must be valid based on the rules specified at https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .","title":"cpu"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.image","text":"The image to use for a new Cloud Run Job. This value must refer to an image within either Google Container Registry or Google Artifact Registry.","title":"image"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.job_name","text":"Create a unique and valid job name.","title":"job_name"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.keep_job","text":"Keep the completed Cloud Run Job on Google Cloud Platform.","title":"keep_job"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory","text":"The amount of memory allocated to the Cloud Run Job.","title":"memory"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory_string","text":"Returns the string expected for memory resources argument.","title":"memory_string"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory_unit","text":"The unit of memory. See https://cloud.google.com/run/docs/configuring/memory-limits#setting for additional details.","title":"memory_unit"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.region","text":"The region where the Cloud Run Job resides.","title":"region"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.timeout","text":"The length of time that Prefect will wait for a Cloud Run Job to complete before raising an exception.","title":"timeout"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.preview","text":"Generate a preview of the job definition that will be sent to GCP. Source code in prefect_gcp/cloud_run.py def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 )","title":"preview()"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.run","text":"Run the configured job on a Google Cloud Run Job. Source code in prefect_gcp/cloud_run.py @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result","title":"run()"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJobResult","text":"Result from a Cloud Run Job. Source code in prefect_gcp/cloud_run.py class CloudRunJobResult ( InfrastructureResult ): \"\"\"Result from a Cloud Run Job.\"\"\"","title":"CloudRunJobResult"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution","text":"Utility class to call GCP executions API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Execution ( BaseModel ): \"\"\" Utility class to call GCP `executions` API and interact with the returned objects. \"\"\" name : str namespace : str metadata : dict spec : dict status : dict log_uri : str def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], )","title":"Execution"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.condition_after_completion","text":"Returns Execution condition if Execution has completed. Source code in prefect_gcp/cloud_run.py def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition","title":"condition_after_completion()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.get","text":"Make a get request to the GCP executions API and return an Execution instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], )","title":"get()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.is_running","text":"Returns True if Execution is not completed. Source code in prefect_gcp/cloud_run.py def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None","title":"is_running()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.succeeded","text":"Whether or not the Execution completed is a successful state. Source code in prefect_gcp/cloud_run.py def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False","title":"succeeded()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job","text":"Utility class to call GCP jobs API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Job ( BaseModel ): \"\"\" Utility class to call GCP `jobs` API and interact with the returned objects. \"\"\" metadata : dict spec : dict status : dict name : str ready_condition : dict execution_status : dict def _is_missing_container ( self ): \"\"\" Check if Job status is not ready because the specified container cannot be found. \"\"\" if ( self . ready_condition . get ( \"status\" ) == \"False\" and self . ready_condition . get ( \"reason\" ) == \"ContainerMissing\" ): return True return False def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\" def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None ) @staticmethod def _get_ready_condition ( job : dict ) -> dict : \"\"\"Utility to access JSON field containing ready condition.\"\"\" if job [ \"status\" ] . get ( \"conditions\" ): for condition in job [ \"status\" ][ \"conditions\" ]: if condition [ \"type\" ] == \"Ready\" : return condition return {} @staticmethod def _get_execution_status ( job : dict ): \"\"\"Utility to access JSON field containing execution status.\"\"\" if job [ \"status\" ] . get ( \"latestCreatedExecution\" ): return job [ \"status\" ][ \"latestCreatedExecution\" ] return {} @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), ) @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"Job"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.create","text":"Make a create request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response","title":"create()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.delete","text":"Make a delete request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"delete()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.get","text":"Make a get request to the GCP jobs API and return a Job instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), )","title":"get()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.has_execution_in_progress","text":"See if job has a run in progress. Source code in prefect_gcp/cloud_run.py def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None )","title":"has_execution_in_progress()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.is_ready","text":"Whether a job is finished registering and ready to be executed Source code in prefect_gcp/cloud_run.py def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\"","title":"is_ready()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.run","text":"Make a run request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"run()"},{"location":"cloud_storage/","text":"prefect_gcp.cloud_storage Tasks for interacting with GCP Cloud Storage. GcsBucket pydantic-model Block used to store data using GCP Cloud Storage Buckets. Attributes: Name Type Description bucket str Name of the bucket. gcp_credentials GcpCredentials The credentials to authenticate with GCP. bucket_folder str A default path to a folder within the GCS bucket to use for reading and writing objects. Examples: Load stored GCP Cloud Storage Bucket: from prefect_gcp import GcsBucket gcp_cloud_storage_bucket_block = GcsBucket . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/cloud_storage.py class GcsBucket ( WritableDeploymentStorage , WritableFileSystem ): \"\"\" Block used to store data using GCP Cloud Storage Buckets. Attributes: bucket: Name of the bucket. gcp_credentials: The credentials to authenticate with GCP. bucket_folder: A default path to a folder within the GCS bucket to use for reading and writing objects. Example: Load stored GCP Cloud Storage Bucket: ```python from prefect_gcp import GcsBucket gcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCS Bucket\" bucket : str = Field ( ... , description = \"Name of the bucket.\" ) gcp_credentials : GcpCredentials = Field ( default_factory = GcpCredentials , description = \"The credentials to authenticate with GCP.\" , ) bucket_folder : str = Field ( default = \"\" , description = ( \"A default path to a folder within the GCS bucket to use \" \"for reading and writing objects.\" ), ) @validator ( \"bucket_folder\" , pre = True , always = True ) def _bucket_folder_suffix ( cls , value ): \"\"\" Ensures that the bucket folder is suffixed with a forward slash. \"\"\" if value != \"\" and not value . endswith ( \"/\" ): value = f \" { value } /\" return value def _resolve_path ( self , path : str ) -> str : \"\"\" A helper function used in write_path to join `self.bucket_folder` and `path`. Args: path: Name of the key, e.g. \"file1\". Each object in your bucket has a unique key (or key name). Returns: The joined path. \"\"\" path = path or str ( uuid4 ()) # If bucket_folder provided, it means we won't write to the root dir of # the bucket. So we need to add it on the front of the path. path = os . path . join ( self . bucket_folder , path ) if self . bucket_folder else path return path @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , ) @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path bucket : str pydantic-field required Name of the bucket. bucket_folder : str pydantic-field A default path to a folder within the GCS bucket to use for reading and writing objects. gcp_credentials : GcpCredentials pydantic-field The credentials to authenticate with GCP. get_directory async Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Parameters: Name Type Description Default from_path Optional[str] Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. None local_path Optional[str] Local path to download GCS bucket contents to. Defaults to the current working directory. None Source code in prefect_gcp/cloud_storage.py @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , ) put_directory async Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Parameters: Name Type Description Default local_path Optional[str] Path to local directory to upload from. None to_path Optional[str] Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. None ignore_file Optional[str] Path to file containing gitignore style expressions for filepaths to ignore. None Returns: Type Description int The number of files uploaded. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count read_path async Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Parameters: Name Type Description Default path str Entire path to (and including) the key. required Returns: Type Description bytes A bytes or string representation of the blob object. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents write_path async Writes to an GCS bucket. Parameters: Name Type Description Default path str The key name. Each object in your bucket has a unique key (or key name). required content bytes What you are uploading to GCS Bucket. required Returns: Type Description str The path that the contents were written to. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path cloud_storage_copy_blob async Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Parameters: Name Type Description Default source_bucket str Source bucket name. required dest_bucket str Destination bucket name. required source_blob str Source blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required dest_blob Optional[str] Destination blob name; if not provided, defaults to source_blob. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **copy_kwargs Dict[str, Any] Additional keyword arguments to pass to Bucket.copy_blob . {} Returns: Type Description str Destination blob name. Examples: Copies blob from one bucket to another. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow () def example_cloud_storage_copy_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_copy_blob ( \"source_bucket\" , \"dest_bucket\" , \"source_blob\" , gcp_credentials ) return blob example_cloud_storage_copy_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_copy_blob ( source_bucket : str , dest_bucket : str , source_blob : str , gcp_credentials : GcpCredentials , dest_blob : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** copy_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Args: source_bucket: Source bucket name. dest_bucket: Destination bucket name. source_blob: Source blob name. gcp_credentials: Credentials to use for authentication with GCP. dest_blob: Destination blob name; if not provided, defaults to source_blob. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **copy_kwargs: Additional keyword arguments to pass to `Bucket.copy_blob`. Returns: Destination blob name. Example: Copies blob from one bucket to another. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow() def example_cloud_storage_copy_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_copy_blob( \"source_bucket\", \"dest_bucket\", \"source_blob\", gcp_credentials ) return blob example_cloud_storage_copy_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Copying blob named %s from the %s bucket to the %s bucket\" , source_blob , source_bucket , dest_bucket , ) source_bucket_obj = await _get_bucket ( source_bucket , gcp_credentials , project = project ) dest_bucket_obj = await _get_bucket ( dest_bucket , gcp_credentials , project = project ) if dest_blob is None : dest_blob = source_blob source_blob_obj = source_bucket_obj . blob ( source_blob ) await run_sync_in_worker_thread ( source_bucket_obj . copy_blob , blob = source_blob_obj , destination_bucket = dest_bucket_obj , new_name = dest_blob , timeout = timeout , ** copy_kwargs , ) return dest_blob cloud_storage_create_bucket async Creates a bucket. Parameters: Name Type Description Default bucket str Name of the bucket. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None location Optional[str] Location of the bucket. None **create_kwargs Dict[str, Any] Additional keyword arguments to pass to client.create_bucket . {} Returns: Type Description str The bucket name. Examples: Creates a bucket named \"prefect\". from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow () def example_cloud_storage_create_bucket_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) bucket = cloud_storage_create_bucket ( \"prefect\" , gcp_credentials ) example_cloud_storage_create_bucket_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_create_bucket ( bucket : str , gcp_credentials : GcpCredentials , project : Optional [ str ] = None , location : Optional [ str ] = None , ** create_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Creates a bucket. Args: bucket: Name of the bucket. gcp_credentials: Credentials to use for authentication with GCP. project: Name of the project to use; overrides the gcp_credentials project if provided. location: Location of the bucket. **create_kwargs: Additional keyword arguments to pass to `client.create_bucket`. Returns: The bucket name. Example: Creates a bucket named \"prefect\". ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow() def example_cloud_storage_create_bucket_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials) example_cloud_storage_create_bucket_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s bucket\" , bucket ) client = gcp_credentials . get_cloud_storage_client ( project = project ) await run_sync_in_worker_thread ( client . create_bucket , bucket , location = location , ** create_kwargs ) return bucket cloud_storage_download_blob_as_bytes async Downloads a blob as bytes. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_as_bytes . {} Returns: Type Description bytes A bytes or string representation of the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = cloud_storage_download_blob_as_bytes ( \"bucket\" , \"blob\" , gcp_credentials ) return contents example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_as_bytes ( bucket : str , blob : str , gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> bytes : \"\"\" Downloads a blob as bytes. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_as_bytes`. Returns: A bytes or string representation of the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") contents = cloud_storage_download_blob_as_bytes( \"bucket\", \"blob\", gcp_credentials) return contents example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) contents = await run_sync_in_worker_thread ( blob_obj . download_as_bytes , timeout = timeout , ** download_kwargs ) return contents cloud_storage_download_blob_to_file async Downloads a blob to a file path. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required path Union[str, pathlib.Path] Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Union[str, pathlib.Path] The path to the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) path = cloud_storage_download_blob_to_file ( \"bucket\" , \"blob\" , \"file_path\" , gcp_credentials ) return path example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_to_file ( bucket : str , blob : str , path : Union [ str , Path ], gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> Union [ str , Path ]: \"\"\" Downloads a blob to a file path. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. path: Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The path to the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") path = cloud_storage_download_blob_to_file( \"bucket\", \"blob\", \"file_path\", gcp_credentials) return path example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket to %s \" , blob , bucket , path ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if os . path . isdir ( path ): if isinstance ( path , Path ): path = path . joinpath ( blob ) # keep as Path if Path is passed else : path = os . path . join ( path , blob ) # keep as str if a str is passed await run_sync_in_worker_thread ( blob_obj . download_to_filename , path , timeout = timeout , ** download_kwargs ) return path cloud_storage_upload_blob_from_file async Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Parameters: Name Type Description Default file Union[str, pathlib.Path, _io.BytesIO] Path to data or file like object to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_file or Blob.upload_from_filename . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow () def example_cloud_storage_upload_blob_from_file_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_file ( \"/path/somewhere\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_file_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_file ( file : Union [ str , Path , BytesIO ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Args: file: Path to data or file like object to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file` or `Blob.upload_from_filename`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow() def example_cloud_storage_upload_blob_from_file_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_file( \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if isinstance ( file , BytesIO ): await run_sync_in_worker_thread ( blob_obj . upload_from_file , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) else : await run_sync_in_worker_thread ( blob_obj . upload_from_filename , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob cloud_storage_upload_blob_from_string async Uploads a blob from a string or bytes representation of data. Parameters: Name Type Description Default data Union[str, bytes] String or bytes representation of data to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_string . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow () def example_cloud_storage_upload_blob_from_string_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_string ( \"data\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_string_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_string ( data : Union [ str , bytes ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from a string or bytes representation of data. Args: data: String or bytes representation of data to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_string`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow() def example_cloud_storage_upload_blob_from_string_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_string( \"data\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_string_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) await run_sync_in_worker_thread ( blob_obj . upload_from_string , data , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob","title":"Cloud Storage"},{"location":"cloud_storage/#prefect_gcp.cloud_storage","text":"Tasks for interacting with GCP Cloud Storage.","title":"cloud_storage"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket","text":"Block used to store data using GCP Cloud Storage Buckets. Attributes: Name Type Description bucket str Name of the bucket. gcp_credentials GcpCredentials The credentials to authenticate with GCP. bucket_folder str A default path to a folder within the GCS bucket to use for reading and writing objects. Examples: Load stored GCP Cloud Storage Bucket: from prefect_gcp import GcsBucket gcp_cloud_storage_bucket_block = GcsBucket . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/cloud_storage.py class GcsBucket ( WritableDeploymentStorage , WritableFileSystem ): \"\"\" Block used to store data using GCP Cloud Storage Buckets. Attributes: bucket: Name of the bucket. gcp_credentials: The credentials to authenticate with GCP. bucket_folder: A default path to a folder within the GCS bucket to use for reading and writing objects. Example: Load stored GCP Cloud Storage Bucket: ```python from prefect_gcp import GcsBucket gcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCS Bucket\" bucket : str = Field ( ... , description = \"Name of the bucket.\" ) gcp_credentials : GcpCredentials = Field ( default_factory = GcpCredentials , description = \"The credentials to authenticate with GCP.\" , ) bucket_folder : str = Field ( default = \"\" , description = ( \"A default path to a folder within the GCS bucket to use \" \"for reading and writing objects.\" ), ) @validator ( \"bucket_folder\" , pre = True , always = True ) def _bucket_folder_suffix ( cls , value ): \"\"\" Ensures that the bucket folder is suffixed with a forward slash. \"\"\" if value != \"\" and not value . endswith ( \"/\" ): value = f \" { value } /\" return value def _resolve_path ( self , path : str ) -> str : \"\"\" A helper function used in write_path to join `self.bucket_folder` and `path`. Args: path: Name of the key, e.g. \"file1\". Each object in your bucket has a unique key (or key name). Returns: The joined path. \"\"\" path = path or str ( uuid4 ()) # If bucket_folder provided, it means we won't write to the root dir of # the bucket. So we need to add it on the front of the path. path = os . path . join ( self . bucket_folder , path ) if self . bucket_folder else path return path @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , ) @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path","title":"GcsBucket"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.bucket","text":"Name of the bucket.","title":"bucket"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.bucket_folder","text":"A default path to a folder within the GCS bucket to use for reading and writing objects.","title":"bucket_folder"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.gcp_credentials","text":"The credentials to authenticate with GCP.","title":"gcp_credentials"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.get_directory","text":"Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Parameters: Name Type Description Default from_path Optional[str] Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. None local_path Optional[str] Local path to download GCS bucket contents to. Defaults to the current working directory. None Source code in prefect_gcp/cloud_storage.py @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , )","title":"get_directory()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.put_directory","text":"Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Parameters: Name Type Description Default local_path Optional[str] Path to local directory to upload from. None to_path Optional[str] Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. None ignore_file Optional[str] Path to file containing gitignore style expressions for filepaths to ignore. None Returns: Type Description int The number of files uploaded. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count","title":"put_directory()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.read_path","text":"Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Parameters: Name Type Description Default path str Entire path to (and including) the key. required Returns: Type Description bytes A bytes or string representation of the blob object. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents","title":"read_path()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.write_path","text":"Writes to an GCS bucket. Parameters: Name Type Description Default path str The key name. Each object in your bucket has a unique key (or key name). required content bytes What you are uploading to GCS Bucket. required Returns: Type Description str The path that the contents were written to. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path","title":"write_path()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_copy_blob","text":"Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Parameters: Name Type Description Default source_bucket str Source bucket name. required dest_bucket str Destination bucket name. required source_blob str Source blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required dest_blob Optional[str] Destination blob name; if not provided, defaults to source_blob. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **copy_kwargs Dict[str, Any] Additional keyword arguments to pass to Bucket.copy_blob . {} Returns: Type Description str Destination blob name. Examples: Copies blob from one bucket to another. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow () def example_cloud_storage_copy_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_copy_blob ( \"source_bucket\" , \"dest_bucket\" , \"source_blob\" , gcp_credentials ) return blob example_cloud_storage_copy_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_copy_blob ( source_bucket : str , dest_bucket : str , source_blob : str , gcp_credentials : GcpCredentials , dest_blob : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** copy_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Args: source_bucket: Source bucket name. dest_bucket: Destination bucket name. source_blob: Source blob name. gcp_credentials: Credentials to use for authentication with GCP. dest_blob: Destination blob name; if not provided, defaults to source_blob. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **copy_kwargs: Additional keyword arguments to pass to `Bucket.copy_blob`. Returns: Destination blob name. Example: Copies blob from one bucket to another. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow() def example_cloud_storage_copy_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_copy_blob( \"source_bucket\", \"dest_bucket\", \"source_blob\", gcp_credentials ) return blob example_cloud_storage_copy_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Copying blob named %s from the %s bucket to the %s bucket\" , source_blob , source_bucket , dest_bucket , ) source_bucket_obj = await _get_bucket ( source_bucket , gcp_credentials , project = project ) dest_bucket_obj = await _get_bucket ( dest_bucket , gcp_credentials , project = project ) if dest_blob is None : dest_blob = source_blob source_blob_obj = source_bucket_obj . blob ( source_blob ) await run_sync_in_worker_thread ( source_bucket_obj . copy_blob , blob = source_blob_obj , destination_bucket = dest_bucket_obj , new_name = dest_blob , timeout = timeout , ** copy_kwargs , ) return dest_blob","title":"cloud_storage_copy_blob()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_create_bucket","text":"Creates a bucket. Parameters: Name Type Description Default bucket str Name of the bucket. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None location Optional[str] Location of the bucket. None **create_kwargs Dict[str, Any] Additional keyword arguments to pass to client.create_bucket . {} Returns: Type Description str The bucket name. Examples: Creates a bucket named \"prefect\". from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow () def example_cloud_storage_create_bucket_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) bucket = cloud_storage_create_bucket ( \"prefect\" , gcp_credentials ) example_cloud_storage_create_bucket_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_create_bucket ( bucket : str , gcp_credentials : GcpCredentials , project : Optional [ str ] = None , location : Optional [ str ] = None , ** create_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Creates a bucket. Args: bucket: Name of the bucket. gcp_credentials: Credentials to use for authentication with GCP. project: Name of the project to use; overrides the gcp_credentials project if provided. location: Location of the bucket. **create_kwargs: Additional keyword arguments to pass to `client.create_bucket`. Returns: The bucket name. Example: Creates a bucket named \"prefect\". ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow() def example_cloud_storage_create_bucket_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials) example_cloud_storage_create_bucket_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s bucket\" , bucket ) client = gcp_credentials . get_cloud_storage_client ( project = project ) await run_sync_in_worker_thread ( client . create_bucket , bucket , location = location , ** create_kwargs ) return bucket","title":"cloud_storage_create_bucket()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_as_bytes","text":"Downloads a blob as bytes. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_as_bytes . {} Returns: Type Description bytes A bytes or string representation of the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = cloud_storage_download_blob_as_bytes ( \"bucket\" , \"blob\" , gcp_credentials ) return contents example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_as_bytes ( bucket : str , blob : str , gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> bytes : \"\"\" Downloads a blob as bytes. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_as_bytes`. Returns: A bytes or string representation of the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") contents = cloud_storage_download_blob_as_bytes( \"bucket\", \"blob\", gcp_credentials) return contents example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) contents = await run_sync_in_worker_thread ( blob_obj . download_as_bytes , timeout = timeout , ** download_kwargs ) return contents","title":"cloud_storage_download_blob_as_bytes()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_to_file","text":"Downloads a blob to a file path. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required path Union[str, pathlib.Path] Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Union[str, pathlib.Path] The path to the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) path = cloud_storage_download_blob_to_file ( \"bucket\" , \"blob\" , \"file_path\" , gcp_credentials ) return path example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_to_file ( bucket : str , blob : str , path : Union [ str , Path ], gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> Union [ str , Path ]: \"\"\" Downloads a blob to a file path. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. path: Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The path to the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") path = cloud_storage_download_blob_to_file( \"bucket\", \"blob\", \"file_path\", gcp_credentials) return path example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket to %s \" , blob , bucket , path ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if os . path . isdir ( path ): if isinstance ( path , Path ): path = path . joinpath ( blob ) # keep as Path if Path is passed else : path = os . path . join ( path , blob ) # keep as str if a str is passed await run_sync_in_worker_thread ( blob_obj . download_to_filename , path , timeout = timeout , ** download_kwargs ) return path","title":"cloud_storage_download_blob_to_file()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_file","text":"Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Parameters: Name Type Description Default file Union[str, pathlib.Path, _io.BytesIO] Path to data or file like object to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_file or Blob.upload_from_filename . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow () def example_cloud_storage_upload_blob_from_file_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_file ( \"/path/somewhere\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_file_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_file ( file : Union [ str , Path , BytesIO ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Args: file: Path to data or file like object to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file` or `Blob.upload_from_filename`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow() def example_cloud_storage_upload_blob_from_file_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_file( \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if isinstance ( file , BytesIO ): await run_sync_in_worker_thread ( blob_obj . upload_from_file , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) else : await run_sync_in_worker_thread ( blob_obj . upload_from_filename , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob","title":"cloud_storage_upload_blob_from_file()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_string","text":"Uploads a blob from a string or bytes representation of data. Parameters: Name Type Description Default data Union[str, bytes] String or bytes representation of data to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_string . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow () def example_cloud_storage_upload_blob_from_string_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_string ( \"data\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_string_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_string ( data : Union [ str , bytes ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from a string or bytes representation of data. Args: data: String or bytes representation of data to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_string`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow() def example_cloud_storage_upload_blob_from_string_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_string( \"data\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_string_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) await run_sync_in_worker_thread ( blob_obj . upload_from_string , data , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob","title":"cloud_storage_upload_blob_from_string()"},{"location":"credentials/","text":"prefect_gcp.credentials Module handling GCP credentials. GcpCredentials pydantic-model Block used to manage authentication with GCP. GCP authentication is handled via the google.oauth2 module or through the CLI. Specify either one of service account_file or service_account_info ; if both are not specified, the client will try to detect the service account info stored in the env from the command, gcloud auth application-default login . Refer to the Authentication docs for more info about the possible credential configurations. Attributes: Name Type Description service_account_file Optional[pathlib.Path] Path to the service account JSON keyfile. service_account_info Union[Dict[str, str], pydantic.types.Json] The contents of the keyfile as a dict or JSON string. Examples: Load GCP credentials stored in a GCP Credentials Block: from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/credentials.py class GcpCredentials ( Block ): \"\"\" Block used to manage authentication with GCP. GCP authentication is handled via the `google.oauth2` module or through the CLI. Specify either one of service `account_file` or `service_account_info`; if both are not specified, the client will try to detect the service account info stored in the env from the command, `gcloud auth application-default login`. Refer to the [Authentication docs](https://cloud.google.com/docs/authentication/production) for more info about the possible credential configurations. Attributes: service_account_file: Path to the service account JSON keyfile. service_account_info: The contents of the keyfile as a dict or JSON string. Example: Load GCP credentials stored in a `GCP Credentials` Block: ```python from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCP Credentials\" service_account_file : Optional [ Path ] = None service_account_info : Optional [ Union [ Dict [ str , str ], Json ]] = None project : Optional [ str ] = None @root_validator def _provide_one_service_account_source ( cls , values ): \"\"\" Ensure that only a service account file or service account info ias provided. \"\"\" if ( values . get ( \"service_account_info\" ) is not None and values . get ( \"service_account_file\" ) is not None ): raise ValueError ( \"Only one of service_account_info or service_account_file \" \"can be specified at once\" ) return values @validator ( \"service_account_file\" ) def _check_service_account_file ( cls , file ): \"\"\"Get full path of provided file and make sure that it exists.\"\"\" if not file : return file service_account_file = Path ( file ) . expanduser () if not service_account_file . exists (): raise ValueError ( \"The provided path to the service account is invalid\" ) return service_account_file def block_initialization ( self ): if self . project is None and ( self . service_account_file or self . service_account_info ): credentials = self . get_credentials_from_service_account () self . project = credentials . project def get_credentials_from_service_account ( self ) -> Union [ Credentials , None ]: \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : return None return credentials async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" }) client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client get_access_token async Source code in prefect_gcp/credentials.py async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token get_bigquery_client Gets an authenticated BigQuery client. Parameters: Name Type Description Default project str Name of the project to use; overrides the base class's project if provided. None location str Location to use. None Returns: Type Description BigQueryClient An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_bigquery_client () example_get_client_flow () Gets a GCP BigQuery client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_bigquery_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client get_cloud_storage_client Gets an authenticated Cloud Storage client. Parameters: Name Type Description Default project Optional[str] Name of the project to use; overrides the base class's project if provided. None Returns: Type Description StorageClient An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_cloud_storage_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_cloud_storage_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client get_credentials_from_service_account Helper method to serialize credentials by using either service_account_file or service_account_info. Source code in prefect_gcp/credentials.py def get_credentials_from_service_account ( self ) -> Union [ Credentials , None ]: \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : return None return credentials get_secret_manager_client Gets an authenticated Secret Manager Service client. Returns: Type Description SecretManagerServiceClient An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_secret_manager_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" }) client = GcpCredentials ( service_account_info = service_account_info ) . get_secret_manager_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" }) client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client","title":"Credentials"},{"location":"credentials/#prefect_gcp.credentials","text":"Module handling GCP credentials.","title":"credentials"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials","text":"Block used to manage authentication with GCP. GCP authentication is handled via the google.oauth2 module or through the CLI. Specify either one of service account_file or service_account_info ; if both are not specified, the client will try to detect the service account info stored in the env from the command, gcloud auth application-default login . Refer to the Authentication docs for more info about the possible credential configurations. Attributes: Name Type Description service_account_file Optional[pathlib.Path] Path to the service account JSON keyfile. service_account_info Union[Dict[str, str], pydantic.types.Json] The contents of the keyfile as a dict or JSON string. Examples: Load GCP credentials stored in a GCP Credentials Block: from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/credentials.py class GcpCredentials ( Block ): \"\"\" Block used to manage authentication with GCP. GCP authentication is handled via the `google.oauth2` module or through the CLI. Specify either one of service `account_file` or `service_account_info`; if both are not specified, the client will try to detect the service account info stored in the env from the command, `gcloud auth application-default login`. Refer to the [Authentication docs](https://cloud.google.com/docs/authentication/production) for more info about the possible credential configurations. Attributes: service_account_file: Path to the service account JSON keyfile. service_account_info: The contents of the keyfile as a dict or JSON string. Example: Load GCP credentials stored in a `GCP Credentials` Block: ```python from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCP Credentials\" service_account_file : Optional [ Path ] = None service_account_info : Optional [ Union [ Dict [ str , str ], Json ]] = None project : Optional [ str ] = None @root_validator def _provide_one_service_account_source ( cls , values ): \"\"\" Ensure that only a service account file or service account info ias provided. \"\"\" if ( values . get ( \"service_account_info\" ) is not None and values . get ( \"service_account_file\" ) is not None ): raise ValueError ( \"Only one of service_account_info or service_account_file \" \"can be specified at once\" ) return values @validator ( \"service_account_file\" ) def _check_service_account_file ( cls , file ): \"\"\"Get full path of provided file and make sure that it exists.\"\"\" if not file : return file service_account_file = Path ( file ) . expanduser () if not service_account_file . exists (): raise ValueError ( \"The provided path to the service account is invalid\" ) return service_account_file def block_initialization ( self ): if self . project is None and ( self . service_account_file or self . service_account_info ): credentials = self . get_credentials_from_service_account () self . project = credentials . project def get_credentials_from_service_account ( self ) -> Union [ Credentials , None ]: \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : return None return credentials async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" }) client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client","title":"GcpCredentials"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_access_token","text":"Source code in prefect_gcp/credentials.py async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token","title":"get_access_token()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_bigquery_client","text":"Gets an authenticated BigQuery client. Parameters: Name Type Description Default project str Name of the project to use; overrides the base class's project if provided. None location str Location to use. None Returns: Type Description BigQueryClient An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_bigquery_client () example_get_client_flow () Gets a GCP BigQuery client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_bigquery_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client","title":"get_bigquery_client()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_cloud_storage_client","text":"Gets an authenticated Cloud Storage client. Parameters: Name Type Description Default project Optional[str] Name of the project to use; overrides the base class's project if provided. None Returns: Type Description StorageClient An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_cloud_storage_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_cloud_storage_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client","title":"get_cloud_storage_client()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_credentials_from_service_account","text":"Helper method to serialize credentials by using either service_account_file or service_account_info. Source code in prefect_gcp/credentials.py def get_credentials_from_service_account ( self ) -> Union [ Credentials , None ]: \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : return None return credentials","title":"get_credentials_from_service_account()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_secret_manager_client","text":"Gets an authenticated Secret Manager Service client. Returns: Type Description SecretManagerServiceClient An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_secret_manager_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" }) client = GcpCredentials ( service_account_info = service_account_info ) . get_secret_manager_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" }) client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client","title":"get_secret_manager_client()"},{"location":"secret_manager/","text":"prefect_gcp.secret_manager create_secret async Creates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the created secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow () def example_cloud_storage_create_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = create_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_create_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def create_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Creates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the created secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow() def example_cloud_storage_create_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = create_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_create_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } \" secret_settings = { \"replication\" : { \"automatic\" : {}}} partial_create = partial ( client . create_secret , parent = parent , secret_id = secret_name , secret = secret_settings , timeout = timeout , ) response = await to_thread . run_sync ( partial_create ) return response . name delete_secret async Deletes the specified secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to delete. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow () def example_cloud_storage_delete_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = delete_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_delete_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes the specified secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to delete. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow() def example_cloud_storage_delete_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = delete_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_delete_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Deleting %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /\" partial_delete = partial ( client . delete_secret , name = name , timeout = timeout ) await to_thread . run_sync ( partial_delete ) return name delete_secret_version async Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required version_id int Version number of the secret to use; \"latest\" can NOT be used. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret version. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow () def example_cloud_storage_delete_secret_version_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = delete_secret_version ( \"secret_name\" , 1 , gcp_credentials ) return secret_value example_cloud_storage_delete_secret_version_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret_version ( secret_name : str , version_id : int , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. version_id: Version number of the secret to use; \"latest\" can NOT be used. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret version. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow() def example_cloud_storage_delete_secret_version_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials) return secret_value example_cloud_storage_delete_secret_version_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project if version_id == \"latest\" : raise ValueError ( \"The version_id cannot be 'latest'\" ) name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_destroy = partial ( client . destroy_secret_version , name = name , timeout = timeout ) await to_thread . run_sync ( partial_destroy ) return name read_secret async Reads the value of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str Contents of the specified secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow () def example_cloud_storage_read_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = read_secret ( \"secret_name\" , gcp_credentials , version_id = 1 ) return secret_value example_cloud_storage_read_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def read_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , version_id : Union [ str , int ] = \"latest\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Reads the value of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: Contents of the specified secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow() def example_cloud_storage_read_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1) return secret_value example_cloud_storage_read_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_access = partial ( client . access_secret_version , name = name , timeout = timeout ) response = await to_thread . run_sync ( partial_access ) secret = response . payload . data . decode ( \"UTF-8\" ) return secret update_secret async Updates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required secret_value Union[str, bytes] Desired value of the secret. Can be either str or bytes . required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the updated secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow () def example_cloud_storage_update_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = update_secret ( \"secret_name\" , \"secret_value\" , gcp_credentials ) return secret_path example_cloud_storage_update_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def update_secret ( secret_name : str , secret_value : Union [ str , bytes ], gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Updates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. secret_value: Desired value of the secret. Can be either `str` or `bytes`. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the updated secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow() def example_cloud_storage_update_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials) return secret_path example_cloud_storage_update_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Updating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } /secrets/ { secret_name } \" if isinstance ( secret_value , str ): secret_value = secret_value . encode ( \"UTF-8\" ) partial_add = partial ( client . add_secret_version , parent = parent , payload = { \"data\" : secret_value }, timeout = timeout , ) response = await to_thread . run_sync ( partial_add ) return response . name","title":"Secret Manager"},{"location":"secret_manager/#prefect_gcp.secret_manager","text":"","title":"secret_manager"},{"location":"secret_manager/#prefect_gcp.secret_manager.create_secret","text":"Creates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the created secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow () def example_cloud_storage_create_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = create_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_create_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def create_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Creates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the created secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow() def example_cloud_storage_create_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = create_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_create_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } \" secret_settings = { \"replication\" : { \"automatic\" : {}}} partial_create = partial ( client . create_secret , parent = parent , secret_id = secret_name , secret = secret_settings , timeout = timeout , ) response = await to_thread . run_sync ( partial_create ) return response . name","title":"create_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret","text":"Deletes the specified secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to delete. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow () def example_cloud_storage_delete_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = delete_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_delete_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes the specified secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to delete. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow() def example_cloud_storage_delete_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = delete_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_delete_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Deleting %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /\" partial_delete = partial ( client . delete_secret , name = name , timeout = timeout ) await to_thread . run_sync ( partial_delete ) return name","title":"delete_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret_version","text":"Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required version_id int Version number of the secret to use; \"latest\" can NOT be used. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret version. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow () def example_cloud_storage_delete_secret_version_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = delete_secret_version ( \"secret_name\" , 1 , gcp_credentials ) return secret_value example_cloud_storage_delete_secret_version_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret_version ( secret_name : str , version_id : int , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. version_id: Version number of the secret to use; \"latest\" can NOT be used. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret version. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow() def example_cloud_storage_delete_secret_version_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials) return secret_value example_cloud_storage_delete_secret_version_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project if version_id == \"latest\" : raise ValueError ( \"The version_id cannot be 'latest'\" ) name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_destroy = partial ( client . destroy_secret_version , name = name , timeout = timeout ) await to_thread . run_sync ( partial_destroy ) return name","title":"delete_secret_version()"},{"location":"secret_manager/#prefect_gcp.secret_manager.read_secret","text":"Reads the value of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str Contents of the specified secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow () def example_cloud_storage_read_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = read_secret ( \"secret_name\" , gcp_credentials , version_id = 1 ) return secret_value example_cloud_storage_read_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def read_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , version_id : Union [ str , int ] = \"latest\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Reads the value of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: Contents of the specified secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow() def example_cloud_storage_read_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1) return secret_value example_cloud_storage_read_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_access = partial ( client . access_secret_version , name = name , timeout = timeout ) response = await to_thread . run_sync ( partial_access ) secret = response . payload . data . decode ( \"UTF-8\" ) return secret","title":"read_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.update_secret","text":"Updates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required secret_value Union[str, bytes] Desired value of the secret. Can be either str or bytes . required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the updated secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow () def example_cloud_storage_update_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = update_secret ( \"secret_name\" , \"secret_value\" , gcp_credentials ) return secret_path example_cloud_storage_update_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def update_secret ( secret_name : str , secret_value : Union [ str , bytes ], gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Updates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. secret_value: Desired value of the secret. Can be either `str` or `bytes`. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the updated secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow() def example_cloud_storage_update_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials) return secret_path example_cloud_storage_update_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Updating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } /secrets/ { secret_name } \" if isinstance ( secret_value , str ): secret_value = secret_value . encode ( \"UTF-8\" ) partial_add = partial ( client . add_secret_version , parent = parent , payload = { \"data\" : secret_value }, timeout = timeout , ) response = await to_thread . run_sync ( partial_add ) return response . name","title":"update_secret()"}]}
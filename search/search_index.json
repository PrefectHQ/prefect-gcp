{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"prefect-gcp Welcome! prefect-gcp is a collection of prebuilt Prefect tasks that can be used to quickly construct Prefect flows. Getting Started Python setup Requires an installation of Python 3.7+. We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation . Installation To use prefect-gcp and Cloud Run: pip install prefect-gcp To use Cloud Storage: pip install \"prefect-gcp[cloud_storage]\" To use BigQuery: pip install \"prefect-gcp[bigquery]\" To use Secret Manager: pip install \"prefect-gcp[secret_manager]\" To use Vertex AI: pip install \"prefect-gcp[aiplatform]\" A list of available blocks in prefect-gcp and their setup instructions can be found here . Write and run a flow Download blob from bucket from prefect import flow from prefect_gcp.cloud_storage import GcsBucket @flow def donwload_flow (): gcs_bucket = GcsBucket . load ( \"my-bucket\" ) path = gcs_bucket . download_object_to_path ( \"my_folder/notes.txt\" , \"notes.txt\" ) return path download_flow () Deploy command on Cloud Run Save the following as prefect_gcp_flow.py : from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_run import CloudRunJob @flow def cloud_run_job_flow (): cloud_run_job = CloudRunJob ( image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , credentials = GcpCredentials . load ( \"MY_BLOCK_NAME\" ), region = \"us-central1\" , command = [ \"echo\" , \"hello world\" ], ) return cloud_run_job . run () Deploy prefect_gcp_flow.py : from prefect.deployments import Deployment from prefect_gcp_flow import cloud_run_job_flow deployment = Deployment . build_from_flow ( flow = cloud_run_job_flow , name = \"cloud_run_job_deployment\" , version = 1 , work_queue_name = \"demo\" , ) deployment . apply () Run the deployment either on the UI or through the CLI: prefect deployment run cloud-run-job-flow/cloud_run_job_deployment Visit Prefect Deployments for more information about deployments. Get Google auth credentials from GcpCredentials To instantiate a Google Cloud client, like bigquery.Client , GcpCredentials is not a valid input. Instead, use the get_credentials_from_service_account method. import google.cloud.bigquery from prefect import flow from prefect_gcp import GcpCredentials @flow def create_bigquery_client (): gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) google_auth_credentials = gcp_credentials_block . get_credentials_from_service_account () bigquery_client = bigquery . Client ( credentials = google_auth_credentials ) Or simply call get_bigquery_client from GcpCredentials . from prefect import flow from prefect_gcp import GcpCredentials @flow def create_bigquery_client (): gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) bigquery_client = gcp_credentials_block . get_bigquery_client () Deploy command on Vertex AI as a flow Save the following as prefect_gcp_flow.py : from prefect import flow from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob @flow def vertex_ai_job_flow (): gcp_credentials = GcpCredentials . load ( \"MY_BLOCK\" ) job = VertexAICustomTrainingJob ( command = [ \"echo\" , \"hello world\" ], region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . run () vertex_ai_job_flow () Deploy prefect_gcp_flow.py : from prefect.deployments import Deployment from prefect_gcp_flow import vertex_ai_job_flow deployment = Deployment . build_from_flow ( flow = vertex_ai_job_flow , name = \"vertex-ai-job-deployment\" , version = 1 , work_queue_name = \"demo\" , ) deployment . apply () Run the deployment either on the UI or through the CLI: prefect deployment run vertex-ai-job-flow/vertex-ai-job-deployment Visit Prefect Deployments for more information about deployments. Use with_options to customize options on any existing task or flow from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes custom_download = cloud_storage_download_blob_as_bytes . with_options ( name = \"My custom task name\" , retries = 2 , retry_delay_seconds = 10 , ) @flow def example_with_options_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = custom_download ( \"bucket\" , \"blob\" , gcp_credentials ) return contents () example_with_options_flow () For more tips on how to use tasks and flows in a Collection, check out Using Collections ! Blocks Catalog Below is a list of Blocks available for registration in prefect-gcp . To register blocks in this module to view and edit them on Prefect Cloud: prefect block register -m prefect_gcp Note, to use the load method on Blocks, you must already have a block document saved through code or saved through the UI . Credentials Module GcpCredentials To load the GcpCredentials: from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow def my_flow (): my_block = GcpCredentials . load ( \"MY_BLOCK_NAME\" ) my_flow () Bigquery Module BigQueryWarehouse To load the BigQueryWarehouse: from prefect import flow from prefect_gcp.bigquery import BigQueryWarehouse @flow def my_flow (): my_block = BigQueryWarehouse . load ( \"MY_BLOCK_NAME\" ) my_flow () Aiplatform Module VertexAICustomTrainingJob To load the VertexAICustomTrainingJob: from prefect import flow from prefect_gcp.aiplatform import VertexAICustomTrainingJob @flow def my_flow (): my_block = VertexAICustomTrainingJob . load ( \"MY_BLOCK_NAME\" ) my_flow () Cloud Storage Module GcsBucket To load the GcsBucket: from prefect import flow from prefect_gcp.cloud_storage import GcsBucket @flow def my_flow (): my_block = GcsBucket . load ( \"MY_BLOCK_NAME\" ) my_flow () Cloud Run Module CloudRunJob To load the CloudRunJob: from prefect import flow from prefect_gcp.cloud_run import CloudRunJob @flow def my_flow (): my_block = CloudRunJob . load ( \"MY_BLOCK_NAME\" ) my_flow () Secret Manager Module GcpSecret To load the GcpSecret: from prefect import flow from prefect_gcp.secret_manager import GcpSecret @flow def my_flow (): my_block = GcpSecret . load ( \"MY_BLOCK_NAME\" ) my_flow () Resources If you encounter any bugs while using prefect-gcp , feel free to open an issue in the prefect-gcp repository. If you have any questions or issues while using prefect-gcp , you can find help in either the Prefect Discourse forum or the Prefect Slack community . Feel free to \u2b50\ufe0f or watch prefect-gcp for updates too! Development If you'd like to install a version of prefect-gcp for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-gcp.git cd prefect-gcp/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Home"},{"location":"#prefect-gcp","text":"","title":"prefect-gcp"},{"location":"#welcome","text":"prefect-gcp is a collection of prebuilt Prefect tasks that can be used to quickly construct Prefect flows.","title":"Welcome!"},{"location":"#getting-started","text":"","title":"Getting Started"},{"location":"#python-setup","text":"Requires an installation of Python 3.7+. We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv. These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation .","title":"Python setup"},{"location":"#installation","text":"To use prefect-gcp and Cloud Run: pip install prefect-gcp To use Cloud Storage: pip install \"prefect-gcp[cloud_storage]\" To use BigQuery: pip install \"prefect-gcp[bigquery]\" To use Secret Manager: pip install \"prefect-gcp[secret_manager]\" To use Vertex AI: pip install \"prefect-gcp[aiplatform]\" A list of available blocks in prefect-gcp and their setup instructions can be found here .","title":"Installation"},{"location":"#write-and-run-a-flow","text":"","title":"Write and run a flow"},{"location":"#download-blob-from-bucket","text":"from prefect import flow from prefect_gcp.cloud_storage import GcsBucket @flow def donwload_flow (): gcs_bucket = GcsBucket . load ( \"my-bucket\" ) path = gcs_bucket . download_object_to_path ( \"my_folder/notes.txt\" , \"notes.txt\" ) return path download_flow ()","title":"Download blob from bucket"},{"location":"#deploy-command-on-cloud-run","text":"Save the following as prefect_gcp_flow.py : from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_run import CloudRunJob @flow def cloud_run_job_flow (): cloud_run_job = CloudRunJob ( image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , credentials = GcpCredentials . load ( \"MY_BLOCK_NAME\" ), region = \"us-central1\" , command = [ \"echo\" , \"hello world\" ], ) return cloud_run_job . run () Deploy prefect_gcp_flow.py : from prefect.deployments import Deployment from prefect_gcp_flow import cloud_run_job_flow deployment = Deployment . build_from_flow ( flow = cloud_run_job_flow , name = \"cloud_run_job_deployment\" , version = 1 , work_queue_name = \"demo\" , ) deployment . apply () Run the deployment either on the UI or through the CLI: prefect deployment run cloud-run-job-flow/cloud_run_job_deployment Visit Prefect Deployments for more information about deployments.","title":"Deploy command on Cloud Run"},{"location":"#get-google-auth-credentials-from-gcpcredentials","text":"To instantiate a Google Cloud client, like bigquery.Client , GcpCredentials is not a valid input. Instead, use the get_credentials_from_service_account method. import google.cloud.bigquery from prefect import flow from prefect_gcp import GcpCredentials @flow def create_bigquery_client (): gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) google_auth_credentials = gcp_credentials_block . get_credentials_from_service_account () bigquery_client = bigquery . Client ( credentials = google_auth_credentials ) Or simply call get_bigquery_client from GcpCredentials . from prefect import flow from prefect_gcp import GcpCredentials @flow def create_bigquery_client (): gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) bigquery_client = gcp_credentials_block . get_bigquery_client ()","title":"Get Google auth credentials from GcpCredentials"},{"location":"#deploy-command-on-vertex-ai-as-a-flow","text":"Save the following as prefect_gcp_flow.py : from prefect import flow from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob @flow def vertex_ai_job_flow (): gcp_credentials = GcpCredentials . load ( \"MY_BLOCK\" ) job = VertexAICustomTrainingJob ( command = [ \"echo\" , \"hello world\" ], region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . run () vertex_ai_job_flow () Deploy prefect_gcp_flow.py : from prefect.deployments import Deployment from prefect_gcp_flow import vertex_ai_job_flow deployment = Deployment . build_from_flow ( flow = vertex_ai_job_flow , name = \"vertex-ai-job-deployment\" , version = 1 , work_queue_name = \"demo\" , ) deployment . apply () Run the deployment either on the UI or through the CLI: prefect deployment run vertex-ai-job-flow/vertex-ai-job-deployment Visit Prefect Deployments for more information about deployments.","title":"Deploy command on Vertex AI as a flow"},{"location":"#use-with_options-to-customize-options-on-any-existing-task-or-flow","text":"from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes custom_download = cloud_storage_download_blob_as_bytes . with_options ( name = \"My custom task name\" , retries = 2 , retry_delay_seconds = 10 , ) @flow def example_with_options_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = custom_download ( \"bucket\" , \"blob\" , gcp_credentials ) return contents () example_with_options_flow () For more tips on how to use tasks and flows in a Collection, check out Using Collections !","title":"Use with_options to customize options on any existing task or flow"},{"location":"#blocks-catalog","text":"Below is a list of Blocks available for registration in prefect-gcp . To register blocks in this module to view and edit them on Prefect Cloud: prefect block register -m prefect_gcp Note, to use the load method on Blocks, you must already have a block document saved through code or saved through the UI .","title":"Blocks Catalog"},{"location":"#credentials-module","text":"GcpCredentials To load the GcpCredentials: from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow def my_flow (): my_block = GcpCredentials . load ( \"MY_BLOCK_NAME\" ) my_flow ()","title":"Credentials Module"},{"location":"#bigquery-module","text":"BigQueryWarehouse To load the BigQueryWarehouse: from prefect import flow from prefect_gcp.bigquery import BigQueryWarehouse @flow def my_flow (): my_block = BigQueryWarehouse . load ( \"MY_BLOCK_NAME\" ) my_flow ()","title":"Bigquery Module"},{"location":"#aiplatform-module","text":"VertexAICustomTrainingJob To load the VertexAICustomTrainingJob: from prefect import flow from prefect_gcp.aiplatform import VertexAICustomTrainingJob @flow def my_flow (): my_block = VertexAICustomTrainingJob . load ( \"MY_BLOCK_NAME\" ) my_flow ()","title":"Aiplatform Module"},{"location":"#cloud-storage-module","text":"GcsBucket To load the GcsBucket: from prefect import flow from prefect_gcp.cloud_storage import GcsBucket @flow def my_flow (): my_block = GcsBucket . load ( \"MY_BLOCK_NAME\" ) my_flow ()","title":"Cloud Storage Module"},{"location":"#cloud-run-module","text":"CloudRunJob To load the CloudRunJob: from prefect import flow from prefect_gcp.cloud_run import CloudRunJob @flow def my_flow (): my_block = CloudRunJob . load ( \"MY_BLOCK_NAME\" ) my_flow ()","title":"Cloud Run Module"},{"location":"#secret-manager-module","text":"GcpSecret To load the GcpSecret: from prefect import flow from prefect_gcp.secret_manager import GcpSecret @flow def my_flow (): my_block = GcpSecret . load ( \"MY_BLOCK_NAME\" ) my_flow ()","title":"Secret Manager Module"},{"location":"#resources","text":"If you encounter any bugs while using prefect-gcp , feel free to open an issue in the prefect-gcp repository. If you have any questions or issues while using prefect-gcp , you can find help in either the Prefect Discourse forum or the Prefect Slack community . Feel free to \u2b50\ufe0f or watch prefect-gcp for updates too!","title":"Resources"},{"location":"#development","text":"If you'd like to install a version of prefect-gcp for development, clone the repository and perform an editable install with pip : git clone https://github.com/PrefectHQ/prefect-gcp.git cd prefect-gcp/ pip install -e \".[dev]\" # Install linting pre-commit hooks pre-commit install","title":"Development"},{"location":"aiplatform/","text":"prefect_gcp.aiplatform Integrations with Google AI Platform. Note this module is experimental. The intefaces within may change without notice. Examples: Run a job using Vertex AI Custom Training: from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob gcp_credentials = GcpCredentials . load ( \"BLOCK_NAME\" ) job = VertexAICustomTrainingJob ( region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . run () Run a job that runs the command echo hello world using Google Cloud Run Jobs: from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob gcp_credentials = GcpCredentials . load ( \"BLOCK_NAME\" ) job = VertexAICustomTrainingJob ( command = [ \"echo\" , \"hello world\" ], region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . run () Preview job specs: from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob gcp_credentials = GcpCredentials . load ( \"BLOCK_NAME\" ) job = VertexAICustomTrainingJob ( command = [ \"echo\" , \"hello world\" ], region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . preview () Classes VertexAICustomTrainingJob ( Infrastructure ) pydantic-model Infrastructure block used to run Vertex AI custom training jobs. Source code in prefect_gcp/aiplatform.py class VertexAICustomTrainingJob ( Infrastructure ): \"\"\" Infrastructure block used to run Vertex AI custom training jobs. \"\"\" _block_type_name = \"Vertex AI Custom Training Job\" _block_type_slug = \"vertex-ai-custom-training-job\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa type : Literal [ \"vertex-ai-custom-training-job\" ] = Field ( \"vertex-ai-custom-training-job\" , description = \"The slug for this task type.\" ) gcp_credentials : GcpCredentials = Field ( default_factory = GcpCredentials , description = ( \"GCP credentials to use when running the configured \" \"Vertex AI training job.\" ), ) region : str = Field ( default =... , description = \"The region where the Vertex AI custom training job resides.\" , ) image : str = Field ( default =... , title = \"Image Name\" , description = ( \"The image to use for a new Vertex AI custom training job. This value must \" \"refer to an image within either Google Container Registry \" \"or Google Artifact Registry, like `gcr.io/<project_name>/<repo>/`.\" ), ) env : Dict [ str , str ] = Field ( default_factory = dict , title = \"Environment Variables\" , description = \"Environment variables to be passed to your Cloud Run Job.\" , ) machine_type : str = Field ( default = \"n1-standard-4\" , description = \"The machine type to use for the run, which controls the available \" \"CPU and memory.\" , ) accelerator_type : Optional [ str ] = Field ( default = None , description = \"The type of accelerator to attach to the machine.\" ) maximum_run_time : datetime . timedelta = Field ( default = datetime . timedelta ( days = 7 ), description = \"The maximum job running time.\" ) network : Optional [ str ] = Field ( default = None , description = \"The full name of the Compute Engine network\" \"to which the Job should be peered. Private services access must \" \"already be configured for the network. If left unspecified, the job \" \"is not peered with any network.\" , ) reserved_ip_ranges : Optional [ List [ str ]] = Field ( default = None , description = \"A list of names for the reserved ip ranges under the VPC \" \"network that can be used for this job. If set, we will deploy the job \" \"within the provided ip ranges. Otherwise, the job will be deployed to \" \"any ip ranges under the provided VPC network.\" , ) service_account : Optional [ str ] = Field ( default = None , description = ( \"Specifies the service account to use \" \"as the run-as account in Vertex AI. The agent submitting jobs must have \" \"act-as permission on this run-as account. If unspecified, the AI \" \"Platform Custom Code Service Agent for the CustomJob's project is \" \"used. Takes precedence over the service account found in gcp_credentials, \" \"and required if a service account cannot be detected in gcp_credentials.\" ), ) job_watch_poll_interval : float = Field ( default = 5.0 , description = ( \"The amount of time to wait between GCP API calls while monitoring the \" \"state of a Vertex AI Job.\" ), ) @property def job_name ( self ): \"\"\" The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name \"\"\" # noqa try : repo_name = self . image . split ( \"/\" )[ 2 ] # `gcr.io/<project_name>/<repo>/`\" except IndexError : raise ValueError ( \"The provided image must be from either Google Container Registry \" \"or Google Artifact Registry\" ) unique_suffix = uuid4 () . hex job_name = f \" { repo_name } - { unique_suffix } \" return job_name def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" job_spec = self . _build_job_spec () custom_job = CustomJob ( display_name = self . job_name , job_spec = job_spec ) return str ( custom_job ) # outputs a json string def _build_job_spec ( self ) -> \"CustomJobSpec\" : \"\"\" Builds a job spec by gathering details. \"\"\" # gather worker pool spec env_list = [{ \"name\" : name , \"value\" : value } for name , value in self . env . items ()] container_spec = ContainerSpec ( image_uri = self . image , command = self . command , args = [], env = env_list ) machine_spec = MachineSpec ( machine_type = self . machine_type , accelerator_type = self . accelerator_type ) worker_pool_spec = WorkerPoolSpec ( container_spec = container_spec , machine_spec = machine_spec , replica_count = 1 ) # look for service account service_account = ( self . service_account or self . gcp_credentials . _service_account_email ) if service_account is None : raise ValueError ( \"Could not detect a service_account through gcp_credentials; \" \"please provide a service_account in VertexAICustomTrainingJob\" ) # build custom job specs timeout = Duration () . FromTimedelta ( td = self . maximum_run_time ) scheduling = Scheduling ( timeout = timeout ) job_spec = CustomJobSpec ( worker_pool_specs = [ worker_pool_spec ], service_account = service_account , scheduling = scheduling , network = self . network , reserved_ip_ranges = self . reserved_ip_ranges , ) return job_spec async def _create_and_begin_job ( self , job_spec : \"CustomJobSpec\" , job_service_client : \"JobServiceClient\" ) -> \"CustomJob\" : \"\"\" Builds a custom job and begins running it. \"\"\" # create custom job custom_job = CustomJob ( display_name = self . job_name , job_spec = job_spec ) # run job self . logger . info ( f \" { self . _log_prefix } : Job { self . job_name !r} starting to run \" f \"the command { ' ' . join ( self . command ) !r} in region \" f \" { self . region !r} using image { self . image !r} \" ) project = self . gcp_credentials . project resource_name = f \"projects/ { project } /locations/ { self . region } \" custom_job_run = await run_sync_in_worker_thread ( job_service_client . create_custom_job , parent = resource_name , custom_job = custom_job , ) self . logger . info ( f \" { self . _log_prefix } : Job { self . job_name !r} has successfully started; \" f \"the full job name is { custom_job_run . name !r} \" ) return custom_job_run async def _watch_job_run ( self , full_job_name : str , # different from self.job_name job_service_client : \"JobServiceClient\" , current_state : \"JobState\" , until_states : Tuple [ \"JobState\" ], timeout : int = None , ) -> \"CustomJob\" : \"\"\" Polls job run to see if status changed. \"\"\" state = JobState . JOB_STATE_UNSPECIFIED last_state = current_state t0 = time . time () while state not in until_states : job_run = await run_sync_in_worker_thread ( job_service_client . get_custom_job , name = full_job_name , ) state = job_run . state if state != last_state : state_label = ( state . name . replace ( \"_\" , \" \" ) . lower () . replace ( \"state\" , \"state is now:\" ) ) # results in \"New job state is now: succeeded\" self . logger . info ( f \" { self . _log_prefix } : { self . job_name } has new { state_label } \" ) last_state = state else : # Intermittently, the job will not be described. We want to respect the # watch timeout though. self . logger . debug ( f \" { self . _log_prefix } : Job not found.\" ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while watching job for states \" \" {until_states!r} \" ) time . sleep ( self . job_watch_poll_interval ) return job_run @sync_compatible async def run ( self , task_status : Optional [ \"TaskStatus\" ] = None ) -> VertexAICustomTrainingJobResult : \"\"\" Run the configured task on VertexAI. Args: task_status: An optional `TaskStatus` to update when the container starts. Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) job_spec = self . _build_job_spec () with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : job_run = await self . _create_and_begin_job ( job_spec , job_service_client ) if task_status : task_status . started ( self . job_name ) final_job_run = await self . _watch_job_run ( full_job_name = job_run . name , job_service_client = job_service_client , current_state = job_run . state , until_states = ( JobState . JOB_STATE_SUCCEEDED , JobState . JOB_STATE_FAILED , JobState . JOB_STATE_CANCELLED , JobState . JOB_STATE_EXPIRED , ), timeout = self . maximum_run_time . total_seconds (), ) error_msg = final_job_run . error . message if error_msg : raise RuntimeError ( f \" { self . _log_prefix } : { error_msg } \" ) status_code = 0 if final_job_run . state == JobState . JOB_STATE_SUCCEEDED else 1 return VertexAICustomTrainingJobResult ( identifier = final_job_run . display_name , status_code = status_code ) @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a job running Cloud Run. Args: identifier: The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\". Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : await run_sync_in_worker_thread ( self . _kill_job , job_service_client = job_service_client , full_job_name = identifier , ) self . logger . info ( f \"Requested to cancel { identifier } ...\" ) def _kill_job ( self , job_service_client : \"JobServiceClient\" , full_job_name : str ) -> None : \"\"\" Thin wrapper around Job.delete, wrapping a try/except since Job is an independent class that doesn't have knowledge of CloudRunJob and its associated logic. \"\"\" cancel_custom_job_request = CancelCustomJobRequest ( name = full_job_name ) try : job_service_client . cancel_custom_job ( request = cancel_custom_job_request , ) except Exception as exc : if \"does not exist\" in str ( exc ): raise InfrastructureNotFound ( f \"Cannot stop Vertex AI job; the job name { full_job_name !r} \" \"could not be found.\" ) from exc raise @property def _log_prefix ( self ) -> str : \"\"\" Internal property for generating a prefix for logs where `name` may be null \"\"\" if self . name is not None : return f \"VertexAICustomTrainingJob { self . name !r} \" else : return \"VertexAICustomTrainingJob\" Attributes accelerator_type : str pydantic-field The type of accelerator to attach to the machine. gcp_credentials : GcpCredentials pydantic-field GCP credentials to use when running the configured Vertex AI training job. image : str pydantic-field required The image to use for a new Vertex AI custom training job. This value must refer to an image within either Google Container Registry or Google Artifact Registry, like gcr.io/<project_name>/<repo>/ . job_name property readonly The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name job_watch_poll_interval : float pydantic-field The amount of time to wait between GCP API calls while monitoring the state of a Vertex AI Job. machine_type : str pydantic-field The machine type to use for the run, which controls the available CPU and memory. maximum_run_time : timedelta pydantic-field The maximum job running time. network : str pydantic-field The full name of the Compute Engine networkto which the Job should be peered. Private services access must already be configured for the network. If left unspecified, the job is not peered with any network. region : str pydantic-field required The region where the Vertex AI custom training job resides. reserved_ip_ranges : List [ str ] pydantic-field A list of names for the reserved ip ranges under the VPC network that can be used for this job. If set, we will deploy the job within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network. service_account : str pydantic-field Specifies the service account to use as the run-as account in Vertex AI. The agent submitting jobs must have act-as permission on this run-as account. If unspecified, the AI Platform Custom Code Service Agent for the CustomJob's project is used. Takes precedence over the service account found in gcp_credentials, and required if a service account cannot be detected in gcp_credentials. Methods kill async Kill a job running Cloud Run. Parameters: Name Type Description Default identifier str The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\". required Returns: Type Description None The VertexAICustomTrainingJobResult . Source code in prefect_gcp/aiplatform.py @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a job running Cloud Run. Args: identifier: The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\". Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : await run_sync_in_worker_thread ( self . _kill_job , job_service_client = job_service_client , full_job_name = identifier , ) self . logger . info ( f \"Requested to cancel { identifier } ...\" ) preview Generate a preview of the job definition that will be sent to GCP. Source code in prefect_gcp/aiplatform.py def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" job_spec = self . _build_job_spec () custom_job = CustomJob ( display_name = self . job_name , job_spec = job_spec ) return str ( custom_job ) # outputs a json string run async Run the configured task on VertexAI. Parameters: Name Type Description Default task_status Optional[TaskStatus] An optional TaskStatus to update when the container starts. None Returns: Type Description VertexAICustomTrainingJobResult The VertexAICustomTrainingJobResult . Source code in prefect_gcp/aiplatform.py @sync_compatible async def run ( self , task_status : Optional [ \"TaskStatus\" ] = None ) -> VertexAICustomTrainingJobResult : \"\"\" Run the configured task on VertexAI. Args: task_status: An optional `TaskStatus` to update when the container starts. Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) job_spec = self . _build_job_spec () with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : job_run = await self . _create_and_begin_job ( job_spec , job_service_client ) if task_status : task_status . started ( self . job_name ) final_job_run = await self . _watch_job_run ( full_job_name = job_run . name , job_service_client = job_service_client , current_state = job_run . state , until_states = ( JobState . JOB_STATE_SUCCEEDED , JobState . JOB_STATE_FAILED , JobState . JOB_STATE_CANCELLED , JobState . JOB_STATE_EXPIRED , ), timeout = self . maximum_run_time . total_seconds (), ) error_msg = final_job_run . error . message if error_msg : raise RuntimeError ( f \" { self . _log_prefix } : { error_msg } \" ) status_code = 0 if final_job_run . state == JobState . JOB_STATE_SUCCEEDED else 1 return VertexAICustomTrainingJobResult ( identifier = final_job_run . display_name , status_code = status_code ) VertexAICustomTrainingJobResult ( InfrastructureResult ) pydantic-model Result from a Vertex AI custom training job. Source code in prefect_gcp/aiplatform.py class VertexAICustomTrainingJobResult ( InfrastructureResult ): \"\"\"Result from a Vertex AI custom training job.\"\"\"","title":"AI Platform"},{"location":"aiplatform/#prefect_gcp.aiplatform","text":"Integrations with Google AI Platform. Note this module is experimental. The intefaces within may change without notice. Examples: Run a job using Vertex AI Custom Training: from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob gcp_credentials = GcpCredentials . load ( \"BLOCK_NAME\" ) job = VertexAICustomTrainingJob ( region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . run () Run a job that runs the command echo hello world using Google Cloud Run Jobs: from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob gcp_credentials = GcpCredentials . load ( \"BLOCK_NAME\" ) job = VertexAICustomTrainingJob ( command = [ \"echo\" , \"hello world\" ], region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . run () Preview job specs: from prefect_gcp.credentials import GcpCredentials from prefect_gcp.aiplatform import VertexAICustomTrainingJob gcp_credentials = GcpCredentials . load ( \"BLOCK_NAME\" ) job = VertexAICustomTrainingJob ( command = [ \"echo\" , \"hello world\" ], region = \"us-east1\" , image = \"us-docker.pkg.dev/cloudrun/container/job:latest\" , gcp_credentials = gcp_credentials , ) job . preview ()","title":"aiplatform"},{"location":"aiplatform/#prefect_gcp.aiplatform-classes","text":"","title":"Classes"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob","text":"Infrastructure block used to run Vertex AI custom training jobs. Source code in prefect_gcp/aiplatform.py class VertexAICustomTrainingJob ( Infrastructure ): \"\"\" Infrastructure block used to run Vertex AI custom training jobs. \"\"\" _block_type_name = \"Vertex AI Custom Training Job\" _block_type_slug = \"vertex-ai-custom-training-job\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa type : Literal [ \"vertex-ai-custom-training-job\" ] = Field ( \"vertex-ai-custom-training-job\" , description = \"The slug for this task type.\" ) gcp_credentials : GcpCredentials = Field ( default_factory = GcpCredentials , description = ( \"GCP credentials to use when running the configured \" \"Vertex AI training job.\" ), ) region : str = Field ( default =... , description = \"The region where the Vertex AI custom training job resides.\" , ) image : str = Field ( default =... , title = \"Image Name\" , description = ( \"The image to use for a new Vertex AI custom training job. This value must \" \"refer to an image within either Google Container Registry \" \"or Google Artifact Registry, like `gcr.io/<project_name>/<repo>/`.\" ), ) env : Dict [ str , str ] = Field ( default_factory = dict , title = \"Environment Variables\" , description = \"Environment variables to be passed to your Cloud Run Job.\" , ) machine_type : str = Field ( default = \"n1-standard-4\" , description = \"The machine type to use for the run, which controls the available \" \"CPU and memory.\" , ) accelerator_type : Optional [ str ] = Field ( default = None , description = \"The type of accelerator to attach to the machine.\" ) maximum_run_time : datetime . timedelta = Field ( default = datetime . timedelta ( days = 7 ), description = \"The maximum job running time.\" ) network : Optional [ str ] = Field ( default = None , description = \"The full name of the Compute Engine network\" \"to which the Job should be peered. Private services access must \" \"already be configured for the network. If left unspecified, the job \" \"is not peered with any network.\" , ) reserved_ip_ranges : Optional [ List [ str ]] = Field ( default = None , description = \"A list of names for the reserved ip ranges under the VPC \" \"network that can be used for this job. If set, we will deploy the job \" \"within the provided ip ranges. Otherwise, the job will be deployed to \" \"any ip ranges under the provided VPC network.\" , ) service_account : Optional [ str ] = Field ( default = None , description = ( \"Specifies the service account to use \" \"as the run-as account in Vertex AI. The agent submitting jobs must have \" \"act-as permission on this run-as account. If unspecified, the AI \" \"Platform Custom Code Service Agent for the CustomJob's project is \" \"used. Takes precedence over the service account found in gcp_credentials, \" \"and required if a service account cannot be detected in gcp_credentials.\" ), ) job_watch_poll_interval : float = Field ( default = 5.0 , description = ( \"The amount of time to wait between GCP API calls while monitoring the \" \"state of a Vertex AI Job.\" ), ) @property def job_name ( self ): \"\"\" The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name \"\"\" # noqa try : repo_name = self . image . split ( \"/\" )[ 2 ] # `gcr.io/<project_name>/<repo>/`\" except IndexError : raise ValueError ( \"The provided image must be from either Google Container Registry \" \"or Google Artifact Registry\" ) unique_suffix = uuid4 () . hex job_name = f \" { repo_name } - { unique_suffix } \" return job_name def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" job_spec = self . _build_job_spec () custom_job = CustomJob ( display_name = self . job_name , job_spec = job_spec ) return str ( custom_job ) # outputs a json string def _build_job_spec ( self ) -> \"CustomJobSpec\" : \"\"\" Builds a job spec by gathering details. \"\"\" # gather worker pool spec env_list = [{ \"name\" : name , \"value\" : value } for name , value in self . env . items ()] container_spec = ContainerSpec ( image_uri = self . image , command = self . command , args = [], env = env_list ) machine_spec = MachineSpec ( machine_type = self . machine_type , accelerator_type = self . accelerator_type ) worker_pool_spec = WorkerPoolSpec ( container_spec = container_spec , machine_spec = machine_spec , replica_count = 1 ) # look for service account service_account = ( self . service_account or self . gcp_credentials . _service_account_email ) if service_account is None : raise ValueError ( \"Could not detect a service_account through gcp_credentials; \" \"please provide a service_account in VertexAICustomTrainingJob\" ) # build custom job specs timeout = Duration () . FromTimedelta ( td = self . maximum_run_time ) scheduling = Scheduling ( timeout = timeout ) job_spec = CustomJobSpec ( worker_pool_specs = [ worker_pool_spec ], service_account = service_account , scheduling = scheduling , network = self . network , reserved_ip_ranges = self . reserved_ip_ranges , ) return job_spec async def _create_and_begin_job ( self , job_spec : \"CustomJobSpec\" , job_service_client : \"JobServiceClient\" ) -> \"CustomJob\" : \"\"\" Builds a custom job and begins running it. \"\"\" # create custom job custom_job = CustomJob ( display_name = self . job_name , job_spec = job_spec ) # run job self . logger . info ( f \" { self . _log_prefix } : Job { self . job_name !r} starting to run \" f \"the command { ' ' . join ( self . command ) !r} in region \" f \" { self . region !r} using image { self . image !r} \" ) project = self . gcp_credentials . project resource_name = f \"projects/ { project } /locations/ { self . region } \" custom_job_run = await run_sync_in_worker_thread ( job_service_client . create_custom_job , parent = resource_name , custom_job = custom_job , ) self . logger . info ( f \" { self . _log_prefix } : Job { self . job_name !r} has successfully started; \" f \"the full job name is { custom_job_run . name !r} \" ) return custom_job_run async def _watch_job_run ( self , full_job_name : str , # different from self.job_name job_service_client : \"JobServiceClient\" , current_state : \"JobState\" , until_states : Tuple [ \"JobState\" ], timeout : int = None , ) -> \"CustomJob\" : \"\"\" Polls job run to see if status changed. \"\"\" state = JobState . JOB_STATE_UNSPECIFIED last_state = current_state t0 = time . time () while state not in until_states : job_run = await run_sync_in_worker_thread ( job_service_client . get_custom_job , name = full_job_name , ) state = job_run . state if state != last_state : state_label = ( state . name . replace ( \"_\" , \" \" ) . lower () . replace ( \"state\" , \"state is now:\" ) ) # results in \"New job state is now: succeeded\" self . logger . info ( f \" { self . _log_prefix } : { self . job_name } has new { state_label } \" ) last_state = state else : # Intermittently, the job will not be described. We want to respect the # watch timeout though. self . logger . debug ( f \" { self . _log_prefix } : Job not found.\" ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while watching job for states \" \" {until_states!r} \" ) time . sleep ( self . job_watch_poll_interval ) return job_run @sync_compatible async def run ( self , task_status : Optional [ \"TaskStatus\" ] = None ) -> VertexAICustomTrainingJobResult : \"\"\" Run the configured task on VertexAI. Args: task_status: An optional `TaskStatus` to update when the container starts. Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) job_spec = self . _build_job_spec () with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : job_run = await self . _create_and_begin_job ( job_spec , job_service_client ) if task_status : task_status . started ( self . job_name ) final_job_run = await self . _watch_job_run ( full_job_name = job_run . name , job_service_client = job_service_client , current_state = job_run . state , until_states = ( JobState . JOB_STATE_SUCCEEDED , JobState . JOB_STATE_FAILED , JobState . JOB_STATE_CANCELLED , JobState . JOB_STATE_EXPIRED , ), timeout = self . maximum_run_time . total_seconds (), ) error_msg = final_job_run . error . message if error_msg : raise RuntimeError ( f \" { self . _log_prefix } : { error_msg } \" ) status_code = 0 if final_job_run . state == JobState . JOB_STATE_SUCCEEDED else 1 return VertexAICustomTrainingJobResult ( identifier = final_job_run . display_name , status_code = status_code ) @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a job running Cloud Run. Args: identifier: The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\". Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : await run_sync_in_worker_thread ( self . _kill_job , job_service_client = job_service_client , full_job_name = identifier , ) self . logger . info ( f \"Requested to cancel { identifier } ...\" ) def _kill_job ( self , job_service_client : \"JobServiceClient\" , full_job_name : str ) -> None : \"\"\" Thin wrapper around Job.delete, wrapping a try/except since Job is an independent class that doesn't have knowledge of CloudRunJob and its associated logic. \"\"\" cancel_custom_job_request = CancelCustomJobRequest ( name = full_job_name ) try : job_service_client . cancel_custom_job ( request = cancel_custom_job_request , ) except Exception as exc : if \"does not exist\" in str ( exc ): raise InfrastructureNotFound ( f \"Cannot stop Vertex AI job; the job name { full_job_name !r} \" \"could not be found.\" ) from exc raise @property def _log_prefix ( self ) -> str : \"\"\" Internal property for generating a prefix for logs where `name` may be null \"\"\" if self . name is not None : return f \"VertexAICustomTrainingJob { self . name !r} \" else : return \"VertexAICustomTrainingJob\"","title":"VertexAICustomTrainingJob"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob-attributes","text":"","title":"Attributes"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.accelerator_type","text":"The type of accelerator to attach to the machine.","title":"accelerator_type"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.gcp_credentials","text":"GCP credentials to use when running the configured Vertex AI training job.","title":"gcp_credentials"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.image","text":"The image to use for a new Vertex AI custom training job. This value must refer to an image within either Google Container Registry or Google Artifact Registry, like gcr.io/<project_name>/<repo>/ .","title":"image"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.job_name","text":"The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name","title":"job_name"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.job_watch_poll_interval","text":"The amount of time to wait between GCP API calls while monitoring the state of a Vertex AI Job.","title":"job_watch_poll_interval"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.machine_type","text":"The machine type to use for the run, which controls the available CPU and memory.","title":"machine_type"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.maximum_run_time","text":"The maximum job running time.","title":"maximum_run_time"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.network","text":"The full name of the Compute Engine networkto which the Job should be peered. Private services access must already be configured for the network. If left unspecified, the job is not peered with any network.","title":"network"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.region","text":"The region where the Vertex AI custom training job resides.","title":"region"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.reserved_ip_ranges","text":"A list of names for the reserved ip ranges under the VPC network that can be used for this job. If set, we will deploy the job within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network.","title":"reserved_ip_ranges"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.service_account","text":"Specifies the service account to use as the run-as account in Vertex AI. The agent submitting jobs must have act-as permission on this run-as account. If unspecified, the AI Platform Custom Code Service Agent for the CustomJob's project is used. Takes precedence over the service account found in gcp_credentials, and required if a service account cannot be detected in gcp_credentials.","title":"service_account"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob-methods","text":"","title":"Methods"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.kill","text":"Kill a job running Cloud Run. Parameters: Name Type Description Default identifier str The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\". required Returns: Type Description None The VertexAICustomTrainingJobResult . Source code in prefect_gcp/aiplatform.py @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a job running Cloud Run. Args: identifier: The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\". Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : await run_sync_in_worker_thread ( self . _kill_job , job_service_client = job_service_client , full_job_name = identifier , ) self . logger . info ( f \"Requested to cancel { identifier } ...\" )","title":"kill()"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.preview","text":"Generate a preview of the job definition that will be sent to GCP. Source code in prefect_gcp/aiplatform.py def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" job_spec = self . _build_job_spec () custom_job = CustomJob ( display_name = self . job_name , job_spec = job_spec ) return str ( custom_job ) # outputs a json string","title":"preview()"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.run","text":"Run the configured task on VertexAI. Parameters: Name Type Description Default task_status Optional[TaskStatus] An optional TaskStatus to update when the container starts. None Returns: Type Description VertexAICustomTrainingJobResult The VertexAICustomTrainingJobResult . Source code in prefect_gcp/aiplatform.py @sync_compatible async def run ( self , task_status : Optional [ \"TaskStatus\" ] = None ) -> VertexAICustomTrainingJobResult : \"\"\" Run the configured task on VertexAI. Args: task_status: An optional `TaskStatus` to update when the container starts. Returns: The `VertexAICustomTrainingJobResult`. \"\"\" client_options = ClientOptions ( api_endpoint = f \" { self . region } -aiplatform.googleapis.com\" ) job_spec = self . _build_job_spec () with self . gcp_credentials . get_job_service_client ( client_options = client_options ) as job_service_client : job_run = await self . _create_and_begin_job ( job_spec , job_service_client ) if task_status : task_status . started ( self . job_name ) final_job_run = await self . _watch_job_run ( full_job_name = job_run . name , job_service_client = job_service_client , current_state = job_run . state , until_states = ( JobState . JOB_STATE_SUCCEEDED , JobState . JOB_STATE_FAILED , JobState . JOB_STATE_CANCELLED , JobState . JOB_STATE_EXPIRED , ), timeout = self . maximum_run_time . total_seconds (), ) error_msg = final_job_run . error . message if error_msg : raise RuntimeError ( f \" { self . _log_prefix } : { error_msg } \" ) status_code = 0 if final_job_run . state == JobState . JOB_STATE_SUCCEEDED else 1 return VertexAICustomTrainingJobResult ( identifier = final_job_run . display_name , status_code = status_code )","title":"run()"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJobResult","text":"Result from a Vertex AI custom training job. Source code in prefect_gcp/aiplatform.py class VertexAICustomTrainingJobResult ( InfrastructureResult ): \"\"\"Result from a Vertex AI custom training job.\"\"\"","title":"VertexAICustomTrainingJobResult"},{"location":"bigquery/","text":"prefect_gcp.bigquery Tasks for interacting with GCP BigQuery Classes BigQueryWarehouse ( DatabaseBlock ) pydantic-model A block for querying a database with BigQuery. Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called. It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited. It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost. Attributes: Name Type Description gcp_credentials GcpCredentials The credentials to use to authenticate. fetch_size int The number of rows to fetch at a time when calling fetch_many. Note, this parameter is executed on the client side and is not passed to the database. To limit on the server side, add the LIMIT clause, or the dialect's equivalent clause, like TOP , to the query. Source code in prefect_gcp/bigquery.py class BigQueryWarehouse ( DatabaseBlock ): \"\"\" A block for querying a database with BigQuery. Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called. It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited. It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost. Attributes: gcp_credentials: The credentials to use to authenticate. fetch_size: The number of rows to fetch at a time when calling fetch_many. Note, this parameter is executed on the client side and is not passed to the database. To limit on the server side, add the `LIMIT` clause, or the dialect's equivalent clause, like `TOP`, to the query. \"\"\" # noqa _block_type_name = \"BigQuery Warehouse\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa gcp_credentials : GcpCredentials fetch_size : int = Field ( default = 1 , description = \"The number of rows to fetch at a time.\" ) _connection : Optional [ Connection ] = None _unique_cursors : Dict [ str , Cursor ] = None def _start_connection ( self ): \"\"\" Starts a connection. \"\"\" with self . gcp_credentials . get_bigquery_client () as client : self . _connection = Connection ( client = client ) def block_initialization ( self ) -> None : super () . block_initialization () if self . _connection is None : self . _start_connection () if self . _unique_cursors is None : self . _unique_cursors = {} def get_connection ( self ) -> Connection : \"\"\" Get the opened connection to BigQuery. \"\"\" return self . _connection def _get_cursor ( self , inputs : Dict [ str , Any ]) -> Tuple [ bool , Cursor ]: \"\"\" Get a BigQuery cursor. Args: inputs: The inputs to generate a unique hash, used to decide whether a new cursor should be used. Returns: Whether a cursor is new and a BigQuery cursor. \"\"\" input_hash = hash_objects ( inputs ) assert input_hash is not None , ( \"We were not able to hash your inputs, \" \"which resulted in an unexpected data return; \" \"please open an issue with a reproducible example.\" ) if input_hash not in self . _unique_cursors . keys (): new_cursor = self . _connection . cursor () self . _unique_cursors [ input_hash ] = new_cursor return True , new_cursor else : existing_cursor = self . _unique_cursors [ input_hash ] return False , existing_cursor def reset_cursors ( self ) -> None : \"\"\" Tries to close all opened cursors. \"\"\" input_hashes = tuple ( self . _unique_cursors . keys ()) for input_hash in input_hashes : cursor = self . _unique_cursors . pop ( input_hash ) try : cursor . close () except Exception as exc : self . logger . warning ( f \"Failed to close cursor for input hash { input_hash !r} : { exc } \" ) @sync_compatible async def fetch_one ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> Row : \"\"\" Fetch a single result from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A tuple containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching one new row at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_one(operation, parameters=parameters) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchone ) return result @sync_compatible async def fetch_many ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , size : Optional [ int ] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch a limited number of results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. size: The number of results to return; if None or 0, uses the value of `fetch_size` configured on the block. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching two new rows at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 6; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_many( operation, parameters=parameters, size=2 ) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) size = size or self . fetch_size result = await run_sync_in_worker_thread ( cursor . fetchmany , size = size ) return result @sync_compatible async def fetch_all ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch all results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching all rows: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } result = warehouse.fetch_all(operation, parameters=parameters) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchall ) return result @sync_compatible async def execute ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> None : \"\"\" Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operation upon calling. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Examples: Execute operation with parameters: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' CREATE TABLE mydataset.trips AS ( SELECT bikeid, start_time, duration_minutes FROM bigquery-public-data.austin_bikeshare.bikeshare_trips LIMIT %(limit)s ); ''' warehouse.execute(operation, parameters={\"limit\": 5}) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . execute , ** inputs ) @sync_compatible async def execute_many ( self , operation : str , seq_of_parameters : List [ Dict [ str , Any ]], ) -> None : \"\"\" Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operations upon calling. Args: operation: The SQL query or other operation to be executed. seq_of_parameters: The sequence of parameters for the operation. Examples: Create mytable in mydataset and insert two rows into it: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"bigquery\") as warehouse: create_operation = ''' CREATE TABLE IF NOT EXISTS mydataset.mytable ( col1 STRING, col2 INTEGER, col3 BOOLEAN ) ''' warehouse.execute(create_operation) insert_operation = ''' INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s) ''' seq_of_parameters = [ (\"a\", 1, True), (\"b\", 2, False), ] warehouse.execute_many( insert_operation, seq_of_parameters=seq_of_parameters ) ``` \"\"\" inputs = dict ( operation = operation , seq_of_parameters = seq_of_parameters , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . executemany , ** inputs ) def close ( self ): \"\"\" Closes connection and its cursors. \"\"\" try : self . reset_cursors () finally : if self . _connection is not None : self . _connection . close () self . _connection = None def __enter__ ( self ): \"\"\" Start a connection upon entry. \"\"\" return self def __exit__ ( self , * args ): \"\"\" Closes connection and its cursors upon exit. \"\"\" self . close () def __getstate__ ( self ): \"\"\" \"\"\" data = self . __dict__ . copy () data . update ({ k : None for k in { \"_connection\" , \"_unique_cursors\" }}) return data def __setstate__ ( self , data : dict ): \"\"\" \"\"\" self . __dict__ . update ( data ) self . _unique_cursors = {} self . _start_connection () Attributes fetch_size : int pydantic-field The number of rows to fetch at a time. Methods close Closes connection and its cursors. Source code in prefect_gcp/bigquery.py def close ( self ): \"\"\" Closes connection and its cursors. \"\"\" try : self . reset_cursors () finally : if self . _connection is not None : self . _connection . close () self . _connection = None execute async Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operation upon calling. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Examples: Execute operation with parameters: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' CREATE TABLE mydataset.trips AS ( SELECT bikeid, start_time, duration_minutes FROM bigquery-public-data.austin_bikeshare.bikeshare_trips LIMIT %(limit)s ); ''' warehouse . execute ( operation , parameters = { \"limit\" : 5 }) Source code in prefect_gcp/bigquery.py @sync_compatible async def execute ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> None : \"\"\" Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operation upon calling. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Examples: Execute operation with parameters: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' CREATE TABLE mydataset.trips AS ( SELECT bikeid, start_time, duration_minutes FROM bigquery-public-data.austin_bikeshare.bikeshare_trips LIMIT %(limit)s ); ''' warehouse.execute(operation, parameters={\"limit\": 5}) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . execute , ** inputs ) execute_many async Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operations upon calling. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required seq_of_parameters List[Dict[str, Any]] The sequence of parameters for the operation. required Examples: Create mytable in mydataset and insert two rows into it: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"bigquery\" ) as warehouse : create_operation = ''' CREATE TABLE IF NOT EXISTS mydataset.mytable ( col1 STRING, col2 INTEGER, col3 BOOLEAN ) ''' warehouse . execute ( create_operation ) insert_operation = ''' INSERT INTO mydataset.mytable (col1, col2, col3) VALUES ( %s , %s , %s ) ''' seq_of_parameters = [ ( \"a\" , 1 , True ), ( \"b\" , 2 , False ), ] warehouse . execute_many ( insert_operation , seq_of_parameters = seq_of_parameters ) Source code in prefect_gcp/bigquery.py @sync_compatible async def execute_many ( self , operation : str , seq_of_parameters : List [ Dict [ str , Any ]], ) -> None : \"\"\" Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operations upon calling. Args: operation: The SQL query or other operation to be executed. seq_of_parameters: The sequence of parameters for the operation. Examples: Create mytable in mydataset and insert two rows into it: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"bigquery\") as warehouse: create_operation = ''' CREATE TABLE IF NOT EXISTS mydataset.mytable ( col1 STRING, col2 INTEGER, col3 BOOLEAN ) ''' warehouse.execute(create_operation) insert_operation = ''' INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s) ''' seq_of_parameters = [ (\"a\", 1, True), (\"b\", 2, False), ] warehouse.execute_many( insert_operation, seq_of_parameters=seq_of_parameters ) ``` \"\"\" inputs = dict ( operation = operation , seq_of_parameters = seq_of_parameters , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . executemany , ** inputs ) fetch_all async Fetch all results from the database. Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Returns: Type Description List[google.cloud.bigquery.table.Row] A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching all rows: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\" : \"romeoandjuliet\" , \"min_word_count\" : 250 , } result = warehouse . fetch_all ( operation , parameters = parameters ) Source code in prefect_gcp/bigquery.py @sync_compatible async def fetch_all ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch all results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching all rows: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } result = warehouse.fetch_all(operation, parameters=parameters) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchall ) return result fetch_many async Fetch a limited number of results from the database. Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None size Optional[int] The number of results to return; if None or 0, uses the value of fetch_size configured on the block. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Returns: Type Description List[google.cloud.bigquery.table.Row] A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching two new rows at a time: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 6; ''' parameters = { \"corpus\" : \"romeoandjuliet\" , \"min_word_count\" : 250 , } for _ in range ( 0 , 3 ): result = warehouse . fetch_many ( operation , parameters = parameters , size = 2 ) print ( result ) Source code in prefect_gcp/bigquery.py @sync_compatible async def fetch_many ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , size : Optional [ int ] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch a limited number of results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. size: The number of results to return; if None or 0, uses the value of `fetch_size` configured on the block. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching two new rows at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 6; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_many( operation, parameters=parameters, size=2 ) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) size = size or self . fetch_size result = await run_sync_in_worker_thread ( cursor . fetchmany , size = size ) return result fetch_one async Fetch a single result from the database. Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Returns: Type Description Row A tuple containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching one new row at a time: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\" : \"romeoandjuliet\" , \"min_word_count\" : 250 , } for _ in range ( 0 , 3 ): result = warehouse . fetch_one ( operation , parameters = parameters ) print ( result ) Source code in prefect_gcp/bigquery.py @sync_compatible async def fetch_one ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> Row : \"\"\" Fetch a single result from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A tuple containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching one new row at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_one(operation, parameters=parameters) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchone ) return result get_connection Get the opened connection to BigQuery. Source code in prefect_gcp/bigquery.py def get_connection ( self ) -> Connection : \"\"\" Get the opened connection to BigQuery. \"\"\" return self . _connection reset_cursors Tries to close all opened cursors. Source code in prefect_gcp/bigquery.py def reset_cursors ( self ) -> None : \"\"\" Tries to close all opened cursors. \"\"\" input_hashes = tuple ( self . _unique_cursors . keys ()) for input_hash in input_hashes : cursor = self . _unique_cursors . pop ( input_hash ) try : cursor . close () except Exception as exc : self . logger . warning ( f \"Failed to close cursor for input hash { input_hash !r} : { exc } \" ) Functions bigquery_create_table async Creates table in BigQuery. Parameters: Name Type Description Default dataset str Name of a dataset in that the table will be created. required table str Name of a table to create. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required clustering_fields List[str] List of fields to cluster the table by. None time_partitioning TimePartitioning bigquery.TimePartitioning object specifying a partitioning of the newly created table None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str The location of the dataset that will be written to. 'US' external_config Optional[google.cloud.bigquery.external_config.ExternalConfig] The external data source . # noqa None Returns: Type Description str Table name. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) schema = [ SchemaField ( \"number\" , field_type = \"INTEGER\" , mode = \"REQUIRED\" ), SchemaField ( \"text\" , field_type = \"STRING\" , mode = \"REQUIRED\" ), SchemaField ( \"bool\" , field_type = \"BOOLEAN\" ) ] result = bigquery_create_table ( dataset = \"dataset\" , table = \"test_table\" , schema = schema , gcp_credentials = gcp_credentials ) return result example_bigquery_create_table_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_create_table ( dataset : str , table : str , gcp_credentials : GcpCredentials , schema : Optional [ List [ SchemaField ]] = None , clustering_fields : List [ str ] = None , time_partitioning : TimePartitioning = None , project : Optional [ str ] = None , location : str = \"US\" , external_config : Optional [ ExternalConfig ] = None , ) -> str : \"\"\" Creates table in BigQuery. Args: dataset: Name of a dataset in that the table will be created. table: Name of a table to create. schema: Schema to use when creating the table. gcp_credentials: Credentials to use for authentication with GCP. clustering_fields: List of fields to cluster the table by. time_partitioning: `bigquery.TimePartitioning` object specifying a partitioning of the newly created table project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: The location of the dataset that will be written to. external_config: The [external data source](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_table#nested_external_data_configuration). # noqa Returns: Table name. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow(): gcp_credentials = GcpCredentials(project=\"project\") schema = [ SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"), SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"), SchemaField(\"bool\", field_type=\"BOOLEAN\") ] result = bigquery_create_table( dataset=\"dataset\", table=\"test_table\", schema=schema, gcp_credentials=gcp_credentials ) return result example_bigquery_create_table_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s . %s \" , dataset , table ) if not external_config and not schema : raise ValueError ( \"Either a schema or an external config must be provided.\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) try : partial_get_dataset = partial ( client . get_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_get_dataset ) except NotFound : logger . debug ( \"Dataset %s not found, creating\" , dataset ) partial_create_dataset = partial ( client . create_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_create_dataset ) table_ref = dataset_ref . table ( table ) try : partial_get_table = partial ( client . get_table , table_ref ) await to_thread . run_sync ( partial_get_table ) logger . info ( \" %s . %s already exists\" , dataset , table ) except NotFound : logger . debug ( \"Table %s not found, creating\" , table ) table_obj = Table ( table_ref , schema = schema ) # external data configuration if external_config : table_obj . external_data_configuration = external_config # cluster for optimal data sorting/access if clustering_fields : table_obj . clustering_fields = clustering_fields # partitioning if time_partitioning : table_obj . time_partitioning = time_partitioning partial_create_table = partial ( client . create_table , table_obj ) await to_thread . run_sync ( partial_create_table ) return table bigquery_insert_stream async Insert records in a Google BigQuery table via the streaming API . Parameters: Name Type Description Default dataset str Name of a dataset where the records will be written to. required table str Name of a table to write to. required records List[dict] The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description List List of inserted rows. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) records = [ { \"number\" : 1 , \"text\" : \"abc\" , \"bool\" : True }, { \"number\" : 2 , \"text\" : \"def\" , \"bool\" : False }, ] result = bigquery_insert_stream ( dataset = \"integrations\" , table = \"test_table\" , records = records , gcp_credentials = gcp_credentials ) return result example_bigquery_insert_stream_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_insert_stream ( dataset : str , table : str , records : List [ dict ], gcp_credentials : GcpCredentials , project : Optional [ str ] = None , location : str = \"US\" , ) -> List : \"\"\" Insert records in a Google BigQuery table via the [streaming API](https://cloud.google.com/bigquery/streaming-data-into-bigquery). Args: dataset: Name of a dataset where the records will be written to. table: Name of a table to write to. records: The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. gcp_credentials: Credentials to use for authentication with GCP. project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: List of inserted rows. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow(): gcp_credentials = GcpCredentials(project=\"project\") records = [ {\"number\": 1, \"text\": \"abc\", \"bool\": True}, {\"number\": 2, \"text\": \"def\", \"bool\": False}, ] result = bigquery_insert_stream( dataset=\"integrations\", table=\"test_table\", records=records, gcp_credentials=gcp_credentials ) return result example_bigquery_insert_stream_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Inserting into %s . %s as a stream\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) partial_insert = partial ( client . insert_rows_json , table = table_ref , json_rows = records ) response = await to_thread . run_sync ( partial_insert ) errors = [] output = [] for row in response : output . append ( row ) if \"errors\" in row : errors . append ( row [ \"errors\" ]) if errors : raise ValueError ( errors ) return output bigquery_load_cloud_storage async Run method for this Task. Invoked by calling this Task within a Flow context, after initialization. Parameters: Name Type Description Default uri str GCS path to load data from. required dataset str The id of a destination dataset to write the records to. required table str The name of a destination table to write the records to. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] The schema to use when creating the table. None job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_uri . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_cloud_storage ( dataset = \"dataset\" , table = \"test_table\" , uri = \"uri\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_cloud_storage_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_cloud_storage ( dataset : str , table : str , uri : str , gcp_credentials : GcpCredentials , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Run method for this Task. Invoked by _calling_ this Task within a Flow context, after initialization. Args: uri: GCS path to load data from. dataset: The id of a destination dataset to write the records to. table: The name of a destination table to write the records to. gcp_credentials: Credentials to use for authentication with GCP. schema: The schema to use when creating the table. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: The response from `load_table_from_uri`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_cloud_storage( dataset=\"dataset\", table=\"test_table\", uri=\"uri\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_cloud_storage_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from cloud storage\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True job_config = LoadJobConfig ( ** job_config ) if schema : job_config . schema = schema result = None try : partial_load = partial ( _result_sync , client . load_table_from_uri , uri , table_ref , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except Exception as exception : logger . exception ( exception ) if result is not None and result . errors is not None : for error in result . errors : logger . exception ( error ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result bigquery_load_file async Loads file into BigQuery. Parameters: Name Type Description Default dataset str ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. required table str Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. required path Union[str, pathlib.Path] A string or path-like object of the file to be loaded. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None job_config Optional[dict] An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None rewind bool if True, seek to the beginning of the file handle before reading the file. False size Optional[int] Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_file . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_file ( dataset = \"dataset\" , table = \"test_table\" , path = \"path\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_file_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_file ( dataset : str , table : str , path : Union [ str , Path ], gcp_credentials : GcpCredentials , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , rewind : bool = False , size : Optional [ int ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Loads file into BigQuery. Args: dataset: ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. table: Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. path: A string or path-like object of the file to be loaded. gcp_credentials: Credentials to use for authentication with GCP. schema: Schema to use when creating the table. job_config: An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). rewind: if True, seek to the beginning of the file handle before reading the file. size: Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: location of the dataset that will be written to. Returns: The response from `load_table_from_file`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_file( dataset=\"dataset\", table=\"test_table\", path=\"path\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from file\" , dataset , table ) if not os . path . exists ( path ): raise ValueError ( f \" { path } does not exist\" ) elif not os . path . isfile ( path ): raise ValueError ( f \" { path } is not a file\" ) client = gcp_credentials . get_bigquery_client ( project = project ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True # TODO: test if autodetect is needed when schema is passed job_config = LoadJobConfig ( ** job_config ) if schema : # TODO: test if schema can be passed directly in job_config job_config . schema = schema try : with open ( path , \"rb\" ) as file_obj : partial_load = partial ( _result_sync , client . load_table_from_file , file_obj , table_ref , rewind = rewind , size = size , location = location , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except IOError : logger . exception ( f \"Could not open and read from { path } \" ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result bigquery_query async Runs a BigQuery query. Parameters: Name Type Description Default query str String of the query to execute. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required query_params Optional[List[tuple]] List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the Google documentation for more details on how both the query and the query parameters should be formatted. None dry_run_max_bytes Optional[int] If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a ValueError if the maximum is exceeded. None dataset Optional[str] Name of a destination dataset to write the query results to, if you don't want them returned; if provided, table must also be provided. None table Optional[str] Name of a destination table to write the query results to, if you don't want them returned; if provided, dataset must also be provided. None to_dataframe bool If provided, returns the results of the query as a pandas dataframe instead of a list of bigquery.table.Row objects. False job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be queried. 'US' Returns: Type Description List[Row] A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Examples: Queries the public names database, returning 10 results. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" , project = \"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ ( \"corpus\" , \"STRING\" , \"romeoandjuliet\" ), ( \"min_word_count\" , \"INT64\" , 250 ) ] result = bigquery_query ( query , gcp_credentials , query_params = query_params ) return result example_bigquery_query_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_query ( query : str , gcp_credentials : GcpCredentials , query_params : Optional [ List [ tuple ]] = None , # 3-tuples dry_run_max_bytes : Optional [ int ] = None , dataset : Optional [ str ] = None , table : Optional [ str ] = None , to_dataframe : bool = False , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> List [ \"Row\" ]: \"\"\" Runs a BigQuery query. Args: query: String of the query to execute. gcp_credentials: Credentials to use for authentication with GCP. query_params: List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the [Google documentation](https://cloud.google.com/bigquery/docs/parameterized-queries#bigquery-query-params-python) for more details on how both the query and the query parameters should be formatted. dry_run_max_bytes: If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a `ValueError` if the maximum is exceeded. dataset: Name of a destination dataset to write the query results to, if you don't want them returned; if provided, `table` must also be provided. table: Name of a destination table to write the query results to, if you don't want them returned; if provided, `dataset` must also be provided. to_dataframe: If provided, returns the results of the query as a pandas dataframe instead of a list of `bigquery.table.Row` objects. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be queried. Returns: A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Example: Queries the public names database, returning 10 results. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\", project=\"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ (\"corpus\", \"STRING\", \"romeoandjuliet\"), (\"min_word_count\", \"INT64\", 250) ] result = bigquery_query( query, gcp_credentials, query_params=query_params ) return result example_bigquery_query_flow() ``` \"\"\" # noqa logger = get_run_logger () logger . info ( \"Running BigQuery query\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) # setup job config job_config = QueryJobConfig ( ** job_config or {}) if query_params is not None : job_config . query_parameters = [ ScalarQueryParameter ( * qp ) for qp in query_params ] # perform dry_run if requested if dry_run_max_bytes is not None : saved_info = dict ( dry_run = job_config . dry_run , use_query_cache = job_config . use_query_cache ) job_config . dry_run = True job_config . use_query_cache = False partial_query = partial ( client . query , query , job_config = job_config ) response = await to_thread . run_sync ( partial_query ) total_bytes_processed = response . total_bytes_processed if total_bytes_processed > dry_run_max_bytes : raise RuntimeError ( f \"Query will process { total_bytes_processed } bytes which is above \" f \"the set maximum of { dry_run_max_bytes } for this task.\" ) job_config . dry_run = saved_info [ \"dry_run\" ] job_config . use_query_cache = saved_info [ \"use_query_cache\" ] # if writing to a destination table if dataset is not None : table_ref = client . dataset ( dataset ) . table ( table ) job_config . destination = table_ref partial_query = partial ( _result_sync , client . query , query , job_config = job_config , ) result = await to_thread . run_sync ( partial_query ) if to_dataframe : return result . to_dataframe () else : return list ( result )","title":"BigQuery"},{"location":"bigquery/#prefect_gcp.bigquery","text":"Tasks for interacting with GCP BigQuery","title":"bigquery"},{"location":"bigquery/#prefect_gcp.bigquery-classes","text":"","title":"Classes"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse","text":"A block for querying a database with BigQuery. Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called. It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited. It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost. Attributes: Name Type Description gcp_credentials GcpCredentials The credentials to use to authenticate. fetch_size int The number of rows to fetch at a time when calling fetch_many. Note, this parameter is executed on the client side and is not passed to the database. To limit on the server side, add the LIMIT clause, or the dialect's equivalent clause, like TOP , to the query. Source code in prefect_gcp/bigquery.py class BigQueryWarehouse ( DatabaseBlock ): \"\"\" A block for querying a database with BigQuery. Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called. It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited. It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost. Attributes: gcp_credentials: The credentials to use to authenticate. fetch_size: The number of rows to fetch at a time when calling fetch_many. Note, this parameter is executed on the client side and is not passed to the database. To limit on the server side, add the `LIMIT` clause, or the dialect's equivalent clause, like `TOP`, to the query. \"\"\" # noqa _block_type_name = \"BigQuery Warehouse\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa gcp_credentials : GcpCredentials fetch_size : int = Field ( default = 1 , description = \"The number of rows to fetch at a time.\" ) _connection : Optional [ Connection ] = None _unique_cursors : Dict [ str , Cursor ] = None def _start_connection ( self ): \"\"\" Starts a connection. \"\"\" with self . gcp_credentials . get_bigquery_client () as client : self . _connection = Connection ( client = client ) def block_initialization ( self ) -> None : super () . block_initialization () if self . _connection is None : self . _start_connection () if self . _unique_cursors is None : self . _unique_cursors = {} def get_connection ( self ) -> Connection : \"\"\" Get the opened connection to BigQuery. \"\"\" return self . _connection def _get_cursor ( self , inputs : Dict [ str , Any ]) -> Tuple [ bool , Cursor ]: \"\"\" Get a BigQuery cursor. Args: inputs: The inputs to generate a unique hash, used to decide whether a new cursor should be used. Returns: Whether a cursor is new and a BigQuery cursor. \"\"\" input_hash = hash_objects ( inputs ) assert input_hash is not None , ( \"We were not able to hash your inputs, \" \"which resulted in an unexpected data return; \" \"please open an issue with a reproducible example.\" ) if input_hash not in self . _unique_cursors . keys (): new_cursor = self . _connection . cursor () self . _unique_cursors [ input_hash ] = new_cursor return True , new_cursor else : existing_cursor = self . _unique_cursors [ input_hash ] return False , existing_cursor def reset_cursors ( self ) -> None : \"\"\" Tries to close all opened cursors. \"\"\" input_hashes = tuple ( self . _unique_cursors . keys ()) for input_hash in input_hashes : cursor = self . _unique_cursors . pop ( input_hash ) try : cursor . close () except Exception as exc : self . logger . warning ( f \"Failed to close cursor for input hash { input_hash !r} : { exc } \" ) @sync_compatible async def fetch_one ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> Row : \"\"\" Fetch a single result from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A tuple containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching one new row at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_one(operation, parameters=parameters) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchone ) return result @sync_compatible async def fetch_many ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , size : Optional [ int ] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch a limited number of results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. size: The number of results to return; if None or 0, uses the value of `fetch_size` configured on the block. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching two new rows at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 6; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_many( operation, parameters=parameters, size=2 ) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) size = size or self . fetch_size result = await run_sync_in_worker_thread ( cursor . fetchmany , size = size ) return result @sync_compatible async def fetch_all ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch all results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching all rows: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } result = warehouse.fetch_all(operation, parameters=parameters) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchall ) return result @sync_compatible async def execute ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> None : \"\"\" Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operation upon calling. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Examples: Execute operation with parameters: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' CREATE TABLE mydataset.trips AS ( SELECT bikeid, start_time, duration_minutes FROM bigquery-public-data.austin_bikeshare.bikeshare_trips LIMIT %(limit)s ); ''' warehouse.execute(operation, parameters={\"limit\": 5}) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . execute , ** inputs ) @sync_compatible async def execute_many ( self , operation : str , seq_of_parameters : List [ Dict [ str , Any ]], ) -> None : \"\"\" Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operations upon calling. Args: operation: The SQL query or other operation to be executed. seq_of_parameters: The sequence of parameters for the operation. Examples: Create mytable in mydataset and insert two rows into it: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"bigquery\") as warehouse: create_operation = ''' CREATE TABLE IF NOT EXISTS mydataset.mytable ( col1 STRING, col2 INTEGER, col3 BOOLEAN ) ''' warehouse.execute(create_operation) insert_operation = ''' INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s) ''' seq_of_parameters = [ (\"a\", 1, True), (\"b\", 2, False), ] warehouse.execute_many( insert_operation, seq_of_parameters=seq_of_parameters ) ``` \"\"\" inputs = dict ( operation = operation , seq_of_parameters = seq_of_parameters , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . executemany , ** inputs ) def close ( self ): \"\"\" Closes connection and its cursors. \"\"\" try : self . reset_cursors () finally : if self . _connection is not None : self . _connection . close () self . _connection = None def __enter__ ( self ): \"\"\" Start a connection upon entry. \"\"\" return self def __exit__ ( self , * args ): \"\"\" Closes connection and its cursors upon exit. \"\"\" self . close () def __getstate__ ( self ): \"\"\" \"\"\" data = self . __dict__ . copy () data . update ({ k : None for k in { \"_connection\" , \"_unique_cursors\" }}) return data def __setstate__ ( self , data : dict ): \"\"\" \"\"\" self . __dict__ . update ( data ) self . _unique_cursors = {} self . _start_connection ()","title":"BigQueryWarehouse"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse-attributes","text":"","title":"Attributes"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_size","text":"The number of rows to fetch at a time.","title":"fetch_size"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse-methods","text":"","title":"Methods"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.close","text":"Closes connection and its cursors. Source code in prefect_gcp/bigquery.py def close ( self ): \"\"\" Closes connection and its cursors. \"\"\" try : self . reset_cursors () finally : if self . _connection is not None : self . _connection . close () self . _connection = None","title":"close()"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.execute","text":"Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operation upon calling. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Examples: Execute operation with parameters: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' CREATE TABLE mydataset.trips AS ( SELECT bikeid, start_time, duration_minutes FROM bigquery-public-data.austin_bikeshare.bikeshare_trips LIMIT %(limit)s ); ''' warehouse . execute ( operation , parameters = { \"limit\" : 5 }) Source code in prefect_gcp/bigquery.py @sync_compatible async def execute ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> None : \"\"\" Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operation upon calling. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Examples: Execute operation with parameters: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' CREATE TABLE mydataset.trips AS ( SELECT bikeid, start_time, duration_minutes FROM bigquery-public-data.austin_bikeshare.bikeshare_trips LIMIT %(limit)s ); ''' warehouse.execute(operation, parameters={\"limit\": 5}) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . execute , ** inputs )","title":"execute()"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.execute_many","text":"Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operations upon calling. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required seq_of_parameters List[Dict[str, Any]] The sequence of parameters for the operation. required Examples: Create mytable in mydataset and insert two rows into it: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"bigquery\" ) as warehouse : create_operation = ''' CREATE TABLE IF NOT EXISTS mydataset.mytable ( col1 STRING, col2 INTEGER, col3 BOOLEAN ) ''' warehouse . execute ( create_operation ) insert_operation = ''' INSERT INTO mydataset.mytable (col1, col2, col3) VALUES ( %s , %s , %s ) ''' seq_of_parameters = [ ( \"a\" , 1 , True ), ( \"b\" , 2 , False ), ] warehouse . execute_many ( insert_operation , seq_of_parameters = seq_of_parameters ) Source code in prefect_gcp/bigquery.py @sync_compatible async def execute_many ( self , operation : str , seq_of_parameters : List [ Dict [ str , Any ]], ) -> None : \"\"\" Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE. Unlike the fetch methods, this method will always execute the operations upon calling. Args: operation: The SQL query or other operation to be executed. seq_of_parameters: The sequence of parameters for the operation. Examples: Create mytable in mydataset and insert two rows into it: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"bigquery\") as warehouse: create_operation = ''' CREATE TABLE IF NOT EXISTS mydataset.mytable ( col1 STRING, col2 INTEGER, col3 BOOLEAN ) ''' warehouse.execute(create_operation) insert_operation = ''' INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s) ''' seq_of_parameters = [ (\"a\", 1, True), (\"b\", 2, False), ] warehouse.execute_many( insert_operation, seq_of_parameters=seq_of_parameters ) ``` \"\"\" inputs = dict ( operation = operation , seq_of_parameters = seq_of_parameters , ) cursor = self . _get_cursor ( inputs )[ 1 ] await run_sync_in_worker_thread ( cursor . executemany , ** inputs )","title":"execute_many()"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_all","text":"Fetch all results from the database. Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Returns: Type Description List[google.cloud.bigquery.table.Row] A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching all rows: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\" : \"romeoandjuliet\" , \"min_word_count\" : 250 , } result = warehouse . fetch_all ( operation , parameters = parameters ) Source code in prefect_gcp/bigquery.py @sync_compatible async def fetch_all ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch all results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching all rows: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } result = warehouse.fetch_all(operation, parameters=parameters) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchall ) return result","title":"fetch_all()"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_many","text":"Fetch a limited number of results from the database. Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None size Optional[int] The number of results to return; if None or 0, uses the value of fetch_size configured on the block. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Returns: Type Description List[google.cloud.bigquery.table.Row] A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching two new rows at a time: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 6; ''' parameters = { \"corpus\" : \"romeoandjuliet\" , \"min_word_count\" : 250 , } for _ in range ( 0 , 3 ): result = warehouse . fetch_many ( operation , parameters = parameters , size = 2 ) print ( result ) Source code in prefect_gcp/bigquery.py @sync_compatible async def fetch_many ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , size : Optional [ int ] = None , ** execution_options : Dict [ str , Any ], ) -> List [ Row ]: \"\"\" Fetch a limited number of results from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. size: The number of results to return; if None or 0, uses the value of `fetch_size` configured on the block. **execution_options: Additional options to pass to `connection.execute`. Returns: A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching two new rows at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 6; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_many( operation, parameters=parameters, size=2 ) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) size = size or self . fetch_size result = await run_sync_in_worker_thread ( cursor . fetchmany , size = size ) return result","title":"fetch_many()"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_one","text":"Fetch a single result from the database. Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Parameters: Name Type Description Default operation str The SQL query or other operation to be executed. required parameters Optional[Dict[str, Any]] The parameters for the operation. None **execution_options Dict[str, Any] Additional options to pass to connection.execute . {} Returns: Type Description Row A tuple containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching one new row at a time: from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse . load ( \"BLOCK_NAME\" ) as warehouse : operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\" : \"romeoandjuliet\" , \"min_word_count\" : 250 , } for _ in range ( 0 , 3 ): result = warehouse . fetch_one ( operation , parameters = parameters ) print ( result ) Source code in prefect_gcp/bigquery.py @sync_compatible async def fetch_one ( self , operation : str , parameters : Optional [ Dict [ str , Any ]] = None , ** execution_options : Dict [ str , Any ], ) -> Row : \"\"\" Fetch a single result from the database. Repeated calls using the same inputs to *any* of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called. Args: operation: The SQL query or other operation to be executed. parameters: The parameters for the operation. **execution_options: Additional options to pass to `connection.execute`. Returns: A tuple containing the data returned by the database, where each row is a tuple and each column is a value in the tuple. Examples: Execute operation with parameters, fetching one new row at a time: ```python from prefect_gcp.bigquery import BigQueryWarehouse with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse: operation = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = %(corpus)s AND word_count >= %(min_word_count)s ORDER BY word_count DESC LIMIT 3; ''' parameters = { \"corpus\": \"romeoandjuliet\", \"min_word_count\": 250, } for _ in range(0, 3): result = warehouse.fetch_one(operation, parameters=parameters) print(result) ``` \"\"\" inputs = dict ( operation = operation , parameters = parameters , ** execution_options , ) new , cursor = self . _get_cursor ( inputs ) if new : await run_sync_in_worker_thread ( cursor . execute , ** inputs ) result = await run_sync_in_worker_thread ( cursor . fetchone ) return result","title":"fetch_one()"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.get_connection","text":"Get the opened connection to BigQuery. Source code in prefect_gcp/bigquery.py def get_connection ( self ) -> Connection : \"\"\" Get the opened connection to BigQuery. \"\"\" return self . _connection","title":"get_connection()"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.reset_cursors","text":"Tries to close all opened cursors. Source code in prefect_gcp/bigquery.py def reset_cursors ( self ) -> None : \"\"\" Tries to close all opened cursors. \"\"\" input_hashes = tuple ( self . _unique_cursors . keys ()) for input_hash in input_hashes : cursor = self . _unique_cursors . pop ( input_hash ) try : cursor . close () except Exception as exc : self . logger . warning ( f \"Failed to close cursor for input hash { input_hash !r} : { exc } \" )","title":"reset_cursors()"},{"location":"bigquery/#prefect_gcp.bigquery-functions","text":"","title":"Functions"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_create_table","text":"Creates table in BigQuery. Parameters: Name Type Description Default dataset str Name of a dataset in that the table will be created. required table str Name of a table to create. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required clustering_fields List[str] List of fields to cluster the table by. None time_partitioning TimePartitioning bigquery.TimePartitioning object specifying a partitioning of the newly created table None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str The location of the dataset that will be written to. 'US' external_config Optional[google.cloud.bigquery.external_config.ExternalConfig] The external data source . # noqa None Returns: Type Description str Table name. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) schema = [ SchemaField ( \"number\" , field_type = \"INTEGER\" , mode = \"REQUIRED\" ), SchemaField ( \"text\" , field_type = \"STRING\" , mode = \"REQUIRED\" ), SchemaField ( \"bool\" , field_type = \"BOOLEAN\" ) ] result = bigquery_create_table ( dataset = \"dataset\" , table = \"test_table\" , schema = schema , gcp_credentials = gcp_credentials ) return result example_bigquery_create_table_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_create_table ( dataset : str , table : str , gcp_credentials : GcpCredentials , schema : Optional [ List [ SchemaField ]] = None , clustering_fields : List [ str ] = None , time_partitioning : TimePartitioning = None , project : Optional [ str ] = None , location : str = \"US\" , external_config : Optional [ ExternalConfig ] = None , ) -> str : \"\"\" Creates table in BigQuery. Args: dataset: Name of a dataset in that the table will be created. table: Name of a table to create. schema: Schema to use when creating the table. gcp_credentials: Credentials to use for authentication with GCP. clustering_fields: List of fields to cluster the table by. time_partitioning: `bigquery.TimePartitioning` object specifying a partitioning of the newly created table project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: The location of the dataset that will be written to. external_config: The [external data source](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_table#nested_external_data_configuration). # noqa Returns: Table name. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_create_table from google.cloud.bigquery import SchemaField @flow def example_bigquery_create_table_flow(): gcp_credentials = GcpCredentials(project=\"project\") schema = [ SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"), SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"), SchemaField(\"bool\", field_type=\"BOOLEAN\") ] result = bigquery_create_table( dataset=\"dataset\", table=\"test_table\", schema=schema, gcp_credentials=gcp_credentials ) return result example_bigquery_create_table_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s . %s \" , dataset , table ) if not external_config and not schema : raise ValueError ( \"Either a schema or an external config must be provided.\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) try : partial_get_dataset = partial ( client . get_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_get_dataset ) except NotFound : logger . debug ( \"Dataset %s not found, creating\" , dataset ) partial_create_dataset = partial ( client . create_dataset , dataset ) dataset_ref = await to_thread . run_sync ( partial_create_dataset ) table_ref = dataset_ref . table ( table ) try : partial_get_table = partial ( client . get_table , table_ref ) await to_thread . run_sync ( partial_get_table ) logger . info ( \" %s . %s already exists\" , dataset , table ) except NotFound : logger . debug ( \"Table %s not found, creating\" , table ) table_obj = Table ( table_ref , schema = schema ) # external data configuration if external_config : table_obj . external_data_configuration = external_config # cluster for optimal data sorting/access if clustering_fields : table_obj . clustering_fields = clustering_fields # partitioning if time_partitioning : table_obj . time_partitioning = time_partitioning partial_create_table = partial ( client . create_table , table_obj ) await to_thread . run_sync ( partial_create_table ) return table","title":"bigquery_create_table()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_insert_stream","text":"Insert records in a Google BigQuery table via the streaming API . Parameters: Name Type Description Default dataset str Name of a dataset where the records will be written to. required table str Name of a table to write to. required records List[dict] The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description List List of inserted rows. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) records = [ { \"number\" : 1 , \"text\" : \"abc\" , \"bool\" : True }, { \"number\" : 2 , \"text\" : \"def\" , \"bool\" : False }, ] result = bigquery_insert_stream ( dataset = \"integrations\" , table = \"test_table\" , records = records , gcp_credentials = gcp_credentials ) return result example_bigquery_insert_stream_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_insert_stream ( dataset : str , table : str , records : List [ dict ], gcp_credentials : GcpCredentials , project : Optional [ str ] = None , location : str = \"US\" , ) -> List : \"\"\" Insert records in a Google BigQuery table via the [streaming API](https://cloud.google.com/bigquery/streaming-data-into-bigquery). Args: dataset: Name of a dataset where the records will be written to. table: Name of a table to write to. records: The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table. gcp_credentials: Credentials to use for authentication with GCP. project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: List of inserted rows. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_insert_stream from google.cloud.bigquery import SchemaField @flow def example_bigquery_insert_stream_flow(): gcp_credentials = GcpCredentials(project=\"project\") records = [ {\"number\": 1, \"text\": \"abc\", \"bool\": True}, {\"number\": 2, \"text\": \"def\", \"bool\": False}, ] result = bigquery_insert_stream( dataset=\"integrations\", table=\"test_table\", records=records, gcp_credentials=gcp_credentials ) return result example_bigquery_insert_stream_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Inserting into %s . %s as a stream\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) partial_insert = partial ( client . insert_rows_json , table = table_ref , json_rows = records ) response = await to_thread . run_sync ( partial_insert ) errors = [] output = [] for row in response : output . append ( row ) if \"errors\" in row : errors . append ( row [ \"errors\" ]) if errors : raise ValueError ( errors ) return output","title":"bigquery_insert_stream()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_cloud_storage","text":"Run method for this Task. Invoked by calling this Task within a Flow context, after initialization. Parameters: Name Type Description Default uri str GCS path to load data from. required dataset str The id of a destination dataset to write the records to. required table str The name of a destination table to write the records to. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] The schema to use when creating the table. None job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_uri . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_cloud_storage ( dataset = \"dataset\" , table = \"test_table\" , uri = \"uri\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_cloud_storage_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_cloud_storage ( dataset : str , table : str , uri : str , gcp_credentials : GcpCredentials , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Run method for this Task. Invoked by _calling_ this Task within a Flow context, after initialization. Args: uri: GCS path to load data from. dataset: The id of a destination dataset to write the records to. table: The name of a destination table to write the records to. gcp_credentials: Credentials to use for authentication with GCP. schema: The schema to use when creating the table. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be written to. Returns: The response from `load_table_from_uri`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_cloud_storage @flow def example_bigquery_load_cloud_storage_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_cloud_storage( dataset=\"dataset\", table=\"test_table\", uri=\"uri\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_cloud_storage_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from cloud storage\" , dataset , table ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True job_config = LoadJobConfig ( ** job_config ) if schema : job_config . schema = schema result = None try : partial_load = partial ( _result_sync , client . load_table_from_uri , uri , table_ref , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except Exception as exception : logger . exception ( exception ) if result is not None and result . errors is not None : for error in result . errors : logger . exception ( error ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result","title":"bigquery_load_cloud_storage()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_file","text":"Loads file into BigQuery. Parameters: Name Type Description Default dataset str ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. required table str Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. required path Union[str, pathlib.Path] A string or path-like object of the file to be loaded. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required schema Optional[List[google.cloud.bigquery.schema.SchemaField]] Schema to use when creating the table. None job_config Optional[dict] An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None rewind bool if True, seek to the beginning of the file handle before reading the file. False size Optional[int] Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. None project Optional[str] Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str location of the dataset that will be written to. 'US' Returns: Type Description LoadJob The response from load_table_from_file . Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) result = bigquery_load_file ( dataset = \"dataset\" , table = \"test_table\" , path = \"path\" , gcp_credentials = gcp_credentials ) return result example_bigquery_load_file_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_load_file ( dataset : str , table : str , path : Union [ str , Path ], gcp_credentials : GcpCredentials , schema : Optional [ List [ SchemaField ]] = None , job_config : Optional [ dict ] = None , rewind : bool = False , size : Optional [ int ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> LoadJob : \"\"\" Loads file into BigQuery. Args: dataset: ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization. table: Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization. path: A string or path-like object of the file to be loaded. gcp_credentials: Credentials to use for authentication with GCP. schema: Schema to use when creating the table. job_config: An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). rewind: if True, seek to the beginning of the file handle before reading the file. size: Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used. project: Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: location of the dataset that will be written to. Returns: The response from `load_table_from_file`. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_load_file from google.cloud.bigquery import SchemaField @flow def example_bigquery_load_file_flow(): gcp_credentials = GcpCredentials(project=\"project\") result = bigquery_load_file( dataset=\"dataset\", table=\"test_table\", path=\"path\", gcp_credentials=gcp_credentials ) return result example_bigquery_load_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Loading into %s . %s from file\" , dataset , table ) if not os . path . exists ( path ): raise ValueError ( f \" { path } does not exist\" ) elif not os . path . isfile ( path ): raise ValueError ( f \" { path } is not a file\" ) client = gcp_credentials . get_bigquery_client ( project = project ) table_ref = client . dataset ( dataset ) . table ( table ) job_config = job_config or {} if \"autodetect\" not in job_config : job_config [ \"autodetect\" ] = True # TODO: test if autodetect is needed when schema is passed job_config = LoadJobConfig ( ** job_config ) if schema : # TODO: test if schema can be passed directly in job_config job_config . schema = schema try : with open ( path , \"rb\" ) as file_obj : partial_load = partial ( _result_sync , client . load_table_from_file , file_obj , table_ref , rewind = rewind , size = size , location = location , job_config = job_config , ) result = await to_thread . run_sync ( partial_load ) except IOError : logger . exception ( f \"Could not open and read from { path } \" ) raise if result is not None : # remove unpickleable attributes result . _client = None result . _completion_lock = None return result","title":"bigquery_load_file()"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_query","text":"Runs a BigQuery query. Parameters: Name Type Description Default query str String of the query to execute. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required query_params Optional[List[tuple]] List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the Google documentation for more details on how both the query and the query parameters should be formatted. None dry_run_max_bytes Optional[int] If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a ValueError if the maximum is exceeded. None dataset Optional[str] Name of a destination dataset to write the query results to, if you don't want them returned; if provided, table must also be provided. None table Optional[str] Name of a destination table to write the query results to, if you don't want them returned; if provided, dataset must also be provided. None to_dataframe bool If provided, returns the results of the query as a pandas dataframe instead of a list of bigquery.table.Row objects. False job_config Optional[dict] Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). None project Optional[str] The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. None location str Location of the dataset that will be queried. 'US' Returns: Type Description List[Row] A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Examples: Queries the public names database, returning 10 results. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" , project = \"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ ( \"corpus\" , \"STRING\" , \"romeoandjuliet\" ), ( \"min_word_count\" , \"INT64\" , 250 ) ] result = bigquery_query ( query , gcp_credentials , query_params = query_params ) return result example_bigquery_query_flow () Source code in prefect_gcp/bigquery.py @task async def bigquery_query ( query : str , gcp_credentials : GcpCredentials , query_params : Optional [ List [ tuple ]] = None , # 3-tuples dry_run_max_bytes : Optional [ int ] = None , dataset : Optional [ str ] = None , table : Optional [ str ] = None , to_dataframe : bool = False , job_config : Optional [ dict ] = None , project : Optional [ str ] = None , location : str = \"US\" , ) -> List [ \"Row\" ]: \"\"\" Runs a BigQuery query. Args: query: String of the query to execute. gcp_credentials: Credentials to use for authentication with GCP. query_params: List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported. See the [Google documentation](https://cloud.google.com/bigquery/docs/parameterized-queries#bigquery-query-params-python) for more details on how both the query and the query parameters should be formatted. dry_run_max_bytes: If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a `ValueError` if the maximum is exceeded. dataset: Name of a destination dataset to write the query results to, if you don't want them returned; if provided, `table` must also be provided. table: Name of a destination table to write the query results to, if you don't want them returned; if provided, `dataset` must also be provided. to_dataframe: If provided, returns the results of the query as a pandas dataframe instead of a list of `bigquery.table.Row` objects. job_config: Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected). project: The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials. location: Location of the dataset that will be queried. Returns: A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria. Example: Queries the public names database, returning 10 results. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.bigquery import bigquery_query @flow def example_bigquery_query_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\", project=\"project\" ) query = ''' SELECT word, word_count FROM `bigquery-public-data.samples.shakespeare` WHERE corpus = @corpus AND word_count >= @min_word_count ORDER BY word_count DESC; ''' query_params = [ (\"corpus\", \"STRING\", \"romeoandjuliet\"), (\"min_word_count\", \"INT64\", 250) ] result = bigquery_query( query, gcp_credentials, query_params=query_params ) return result example_bigquery_query_flow() ``` \"\"\" # noqa logger = get_run_logger () logger . info ( \"Running BigQuery query\" ) client = gcp_credentials . get_bigquery_client ( project = project , location = location ) # setup job config job_config = QueryJobConfig ( ** job_config or {}) if query_params is not None : job_config . query_parameters = [ ScalarQueryParameter ( * qp ) for qp in query_params ] # perform dry_run if requested if dry_run_max_bytes is not None : saved_info = dict ( dry_run = job_config . dry_run , use_query_cache = job_config . use_query_cache ) job_config . dry_run = True job_config . use_query_cache = False partial_query = partial ( client . query , query , job_config = job_config ) response = await to_thread . run_sync ( partial_query ) total_bytes_processed = response . total_bytes_processed if total_bytes_processed > dry_run_max_bytes : raise RuntimeError ( f \"Query will process { total_bytes_processed } bytes which is above \" f \"the set maximum of { dry_run_max_bytes } for this task.\" ) job_config . dry_run = saved_info [ \"dry_run\" ] job_config . use_query_cache = saved_info [ \"use_query_cache\" ] # if writing to a destination table if dataset is not None : table_ref = client . dataset ( dataset ) . table ( table ) job_config . destination = table_ref partial_query = partial ( _result_sync , client . query , query , job_config = job_config , ) result = await to_thread . run_sync ( partial_query ) if to_dataframe : return result . to_dataframe () else : return list ( result )","title":"bigquery_query()"},{"location":"cloud_run/","text":"prefect_gcp.cloud_run Integrations with Google Cloud Run Job. Note this module is experimental. The intefaces within may change without notice. Examples: Run a job using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials ) . run () Run a job that runs the command echo hello world using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials command = [ \"echo\" , \"hello world\" ] ) . run () Classes CloudRunJob ( Infrastructure ) pydantic-model Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. Source code in prefect_gcp/cloud_run.py class CloudRunJob ( Infrastructure ): \"\"\" <span class=\"badge-api experimental\"/> Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. \"\"\" _block_type_slug = \"cloud-run-job\" _block_type_name = \"GCP Cloud Run Job\" _description = \"Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.\" # noqa _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa type : Literal [ \"cloud-run-job\" ] = Field ( \"cloud-run-job\" , description = \"The slug for this task type.\" ) image : str = Field ( ... , title = \"Image Name\" , description = ( \"The image to use for a new Cloud Run Job. This value must \" \"refer to an image within either Google Container Registry \" \"or Google Artifact Registry, like `gcr.io/<project_name>/<repo>/`.\" ), ) region : str = Field ( ... , description = \"The region where the Cloud Run Job resides.\" ) credentials : GcpCredentials # cannot be Field; else it shows as Json # Job settings cpu : Optional [ int ] = Field ( default = None , title = \"CPU\" , description = ( \"The amount of compute allocated to the Cloud Run Job. \" \"The int must be valid based on the rules specified at \" \"https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .\" ), ) memory : Optional [ int ] = Field ( default = None , title = \"Memory\" , description = \"The amount of memory allocated to the Cloud Run Job.\" , ) memory_unit : Optional [ Literal [ \"G\" , \"Gi\" , \"M\" , \"Mi\" ]] = Field ( default = None , title = \"Memory Units\" , description = ( \"The unit of memory. See \" \"https://cloud.google.com/run/docs/configuring/memory-limits#setting \" \"for additional details.\" ), ) args : Optional [ List [ str ]] = Field ( default = None , description = ( \"Arguments to be passed to your Cloud Run Job's entrypoint command.\" ), ) env : Dict [ str , str ] = Field ( default_factory = dict , description = \"Environment variables to be passed to your Cloud Run Job.\" , ) # Cleanup behavior keep_job : Optional [ bool ] = Field ( default = False , title = \"Keep Job After Completion\" , description = \"Keep the completed Cloud Run Job on Google Cloud Platform.\" , ) timeout : Optional [ int ] = Field ( default = None , title = \"Job Timeout\" , description = ( \"The length of time that Prefect will wait for a Cloud Run Job to complete \" \"before raising an exception.\" ), ) # For private use _job_name : str = None _execution : Optional [ Execution ] = None @property def job_name ( self ): \"\"\"Create a unique and valid job name.\"\"\" if self . _job_name is None : # get `repo` from `gcr.io/<project_name>/repo/other` components = self . image . split ( \"/\" ) image_name = components [ 2 ] # only alphanumeric and '-' allowed for a job name modified_image_name = image_name . replace ( \":\" , \"-\" ) . replace ( \".\" , \"-\" ) # make 50 char limit for final job name, which will be '<name>-<uuid>' if len ( modified_image_name ) > 17 : modified_image_name = modified_image_name [: 17 ] name = f \" { modified_image_name } - { uuid4 () . hex } \" self . _job_name = name return self . _job_name @property def memory_string ( self ): \"\"\"Returns the string expected for memory resources argument.\"\"\" if self . memory and self . memory_unit : return str ( self . memory ) + self . memory_unit return None @validator ( \"image\" ) def _remove_image_spaces ( cls , value ): \"\"\"Deal with spaces in image names.\"\"\" if value is not None : return value . strip () @root_validator def _check_valid_memory ( cls , values ): \"\"\"Make sure memory conforms to expected values for API. See: https://cloud.google.com/run/docs/configuring/memory-limits#setting \"\"\" # noqa if ( values . get ( \"memory\" ) is not None and values . get ( \"memory_unit\" ) is None ) or ( values . get ( \"memory_unit\" ) is not None and values . get ( \"memory\" ) is None ): raise ValueError ( \"A memory value and unit must both be supplied to specify a memory\" \" value other than the default memory value.\" ) return values def _create_job_error ( self , exc ): \"\"\"Provides a nicer error for 404s when trying to create a Cloud Run Job.\"\"\" # TODO consider lookup table instead of the if/else, # also check for documented errors if exc . status_code == 404 : raise RuntimeError ( f \"Failed to find resources at { exc . uri } . Confirm that region\" f \" ' { self . region } ' is the correct region for your Cloud Run Job and\" f \" that { self . credentials . project } is the correct GCP project. If\" f \" your project ID is not correct, you are using a Credentials block\" f \" with permissions for the wrong project.\" ) from exc raise exc def _job_run_submission_error ( self , exc ): \"\"\"Provides a nicer error for 404s when submitting job runs.\"\"\" if exc . status_code == 404 : pat1 = r \"The requested URL [^ ]+ was not found on this server\" # pat2 = ( # r\"Resource '[^ ]+' of kind 'JOB' in region '[\\w\\-0-9]+' \" # r\"in project '[\\w\\-0-9]+' does not exist\" # ) if re . findall ( pat1 , str ( exc )): raise RuntimeError ( f \"Failed to find resources at { exc . uri } . \" f \"Confirm that region ' { self . region } ' is \" f \"the correct region for your Cloud Run Job \" f \"and that ' { self . credentials . project } ' is the \" f \"correct GCP project. If your project ID is not \" f \"correct, you are using a Credentials \" f \"block with permissions for the wrong project.\" ) from exc else : raise exc raise exc def _cpu_as_k8s_quantity ( self ) -> str : \"\"\"Return the CPU integer in the format expected by GCP Cloud Run Jobs API. See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ See also: https://cloud.google.com/run/docs/configuring/cpu#setting-jobs \"\"\" # noqa return str ( self . cpu * 1000 ) + \"m\" @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a task running Cloud Run. Args: identifier: The Cloud Run Job name. This should match a value yielded by CloudRunJob.run. \"\"\" if grace_seconds != 30 : self . logger . warning ( f \"Kill grace period of { grace_seconds } s requested, but GCP does not \" \"support dynamic grace period configuration. See here for more info: \" \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\" # noqa ) with self . _get_client () as client : await run_sync_in_worker_thread ( self . _kill_job , client = client , namespace = self . credentials . project , job_name = identifier , ) def _kill_job ( self , client : Resource , namespace : str , job_name : str ) -> None : \"\"\" Thin wrapper around Job.delete, wrapping a try/except since Job is an independent class that doesn't have knowledge of CloudRunJob and its associated logic. \"\"\" try : Job . delete ( client = client , namespace = namespace , job_name = job_name ) except Exception as exc : if \"does not exist\" in str ( exc ): raise InfrastructureNotFound ( f \"Cannot stop Cloud Run Job; the job name { job_name !r} \" \"could not be found.\" ) from exc raise def _create_job_and_wait_for_registration ( self , client : Resource ) -> None : \"\"\"Create a new job wait for it to finish registering.\"\"\" try : self . logger . info ( f \"Creating Cloud Run Job { self . job_name } \" ) Job . create ( client = client , namespace = self . credentials . project , body = self . _jobs_body (), ) except googleapiclient . errors . HttpError as exc : self . _create_job_error ( exc ) try : self . _wait_for_job_creation ( client = client , timeout = self . timeout ) except Exception : self . logger . exception ( \"Encountered an exception while waiting for job run creation\" ) if not self . keep_job : self . logger . info ( f \"Deleting Cloud Run Job { self . job_name } from Google Cloud Run.\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete\" f \" Cloud Run Job { self . job_name !r} \" ) raise def _begin_job_execution ( self , client : Resource ) -> Execution : \"\"\"Submit a job run for execution and return the execution object.\"\"\" try : self . logger . info ( f \"Submitting Cloud Run Job { self . job_name !r} for execution.\" ) submission = Job . run ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) job_execution = Execution . get ( client = client , namespace = submission [ \"metadata\" ][ \"namespace\" ], execution_name = submission [ \"metadata\" ][ \"name\" ], ) command = ( \" \" . join ( self . command ) if self . command else \"default container command\" ) self . logger . info ( f \"Cloud Run Job { self . job_name !r} : Running command { command !r} \" ) except Exception as exc : self . _job_run_submission_error ( exc ) return job_execution def _watch_job_execution_and_get_result ( self , client : Resource , execution : Execution , poll_interval : int ) -> CloudRunJobResult : \"\"\"Wait for execution to complete and then return result.\"\"\" try : job_execution = self . _watch_job_execution ( client = client , job_execution = execution , timeout = self . timeout , poll_interval = poll_interval , ) except Exception : self . logger . exception ( \"Received an unexpected exception while monitoring Cloud Run Job \" f \" { self . job_name !r} \" ) raise if job_execution . succeeded (): status_code = 0 self . logger . info ( f \"Job Run { self . job_name } completed successfully\" ) else : status_code = 1 error_msg = job_execution . condition_after_completion ()[ \"message\" ] self . logger . error ( f \"Job Run { self . job_name } did not complete successfully. { error_msg } \" ) self . logger . info ( f \"Job Run logs can be found on GCP at: { job_execution . log_uri } \" ) if not self . keep_job : self . logger . info ( f \"Deleting completed Cloud Run Job { self . job_name !r} from Google Cloud\" \" Run...\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete Cloud\" f \" Run Job { self . job_name } \" ) return CloudRunJobResult ( identifier = self . job_name , status_code = status_code ) def _jobs_body ( self ) -> dict : \"\"\"Create properly formatted body used for a Job CREATE request. See: https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs \"\"\" jobs_metadata = { \"name\" : self . job_name , \"annotations\" : { # See: https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation # noqa \"run.googleapis.com/launch-stage\" : \"BETA\" }, } # env and command here containers = [ self . _add_container_settings ({ \"image\" : self . image })] body = { \"apiVersion\" : \"run.googleapis.com/v1\" , \"kind\" : \"Job\" , \"metadata\" : jobs_metadata , \"spec\" : { # JobSpec \"template\" : { # ExecutionTemplateSpec \"spec\" : { # ExecutionSpec \"template\" : { # TaskTemplateSpec \"spec\" : { \"containers\" : containers } # TaskSpec } }, } }, } return body def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 ) def _watch_job_execution ( self , client , job_execution : Execution , timeout : int , poll_interval : int = 5 ): \"\"\" Update job_execution status until it is no longer running or timeout is reached. \"\"\" t0 = time . time () while job_execution . is_running (): job_execution = Execution . get ( client = client , namespace = job_execution . namespace , execution_name = job_execution . name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) return job_execution def _wait_for_job_creation ( self , client : Resource , timeout : int , poll_interval : int = 5 ): \"\"\"Give created job time to register.\"\"\" job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name ) t0 = time . time () while not job . is_ready (): ready_condition = ( job . ready_condition if job . ready_condition else \"waiting for condition update\" ) self . logger . info ( f \"Job is not yet ready... Current condition: { ready_condition } \" ) job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) def _get_client ( self ) -> Resource : \"\"\"Get the base client needed for interacting with GCP APIs.\"\"\" # region needed for 'v1' API api_endpoint = f \"https:// { self . region } -run.googleapis.com\" gcp_creds = self . credentials . get_credentials_from_service_account () options = ClientOptions ( api_endpoint = api_endpoint ) return discovery . build ( \"run\" , \"v1\" , client_options = options , credentials = gcp_creds ) . namespaces () # CONTAINER SETTINGS def _add_container_settings ( self , base_settings : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\" Add settings related to containers for Cloud Run Jobs to a dictionary. Includes environment variables, entrypoint command, entrypoint arguments, and cpu and memory limits. See: https://cloud.google.com/run/docs/reference/rest/v1/Container and https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements \"\"\" # noqa container_settings = base_settings . copy () container_settings . update ( self . _add_env ()) container_settings . update ( self . _add_resources ()) container_settings . update ( self . _add_command ()) container_settings . update ( self . _add_args ()) return container_settings def _add_args ( self ) -> dict : \"\"\"Set the arguments that will be passed to the entrypoint for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"args\" : self . args } if self . args else {} def _add_command ( self ) -> dict : \"\"\"Set the command that a container will run for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"command\" : self . command } def _add_resources ( self ) -> dict : \"\"\"Set specified resources limits for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements See also: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ \"\"\" # noqa resources = { \"limits\" : {}, \"requests\" : {}} if self . cpu is not None : cpu = self . _cpu_as_k8s_quantity () resources [ \"limits\" ][ \"cpu\" ] = cpu resources [ \"requests\" ][ \"cpu\" ] = cpu if self . memory_string is not None : resources [ \"limits\" ][ \"memory\" ] = self . memory_string resources [ \"requests\" ][ \"memory\" ] = self . memory_string return { \"resources\" : resources } if resources [ \"requests\" ] else {} def _add_env ( self ) -> dict : \"\"\"Add environment variables for a Cloud Run Job. Method `self._base_environment()` gets necessary Prefect environment variables from the config. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#envvar for how environment variables are specified for Cloud Run Jobs. \"\"\" # noqa env = { ** self . _base_environment (), ** self . env } cloud_run_env = [{ \"name\" : k , \"value\" : v } for k , v in env . items ()] return { \"env\" : cloud_run_env } Attributes args : List [ str ] pydantic-field Arguments to be passed to your Cloud Run Job's entrypoint command. cpu : int pydantic-field The amount of compute allocated to the Cloud Run Job. The int must be valid based on the rules specified at https://cloud.google.com/run/docs/configuring/cpu#setting-jobs . image : str pydantic-field required The image to use for a new Cloud Run Job. This value must refer to an image within either Google Container Registry or Google Artifact Registry, like gcr.io/<project_name>/<repo>/ . job_name property readonly Create a unique and valid job name. keep_job : bool pydantic-field Keep the completed Cloud Run Job on Google Cloud Platform. memory : int pydantic-field The amount of memory allocated to the Cloud Run Job. memory_string property readonly Returns the string expected for memory resources argument. memory_unit : Literal [ 'G' , 'Gi' , 'M' , 'Mi' ] pydantic-field The unit of memory. See https://cloud.google.com/run/docs/configuring/memory-limits#setting for additional details. region : str pydantic-field required The region where the Cloud Run Job resides. timeout : int pydantic-field The length of time that Prefect will wait for a Cloud Run Job to complete before raising an exception. Methods kill async Kill a task running Cloud Run. Parameters: Name Type Description Default identifier str The Cloud Run Job name. This should match a value yielded by CloudRunJob.run. required Source code in prefect_gcp/cloud_run.py @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a task running Cloud Run. Args: identifier: The Cloud Run Job name. This should match a value yielded by CloudRunJob.run. \"\"\" if grace_seconds != 30 : self . logger . warning ( f \"Kill grace period of { grace_seconds } s requested, but GCP does not \" \"support dynamic grace period configuration. See here for more info: \" \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\" # noqa ) with self . _get_client () as client : await run_sync_in_worker_thread ( self . _kill_job , client = client , namespace = self . credentials . project , job_name = identifier , ) preview Generate a preview of the job definition that will be sent to GCP. Source code in prefect_gcp/cloud_run.py def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 ) run async Run the configured job on a Google Cloud Run Job. Source code in prefect_gcp/cloud_run.py @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result CloudRunJobResult ( InfrastructureResult ) pydantic-model Result from a Cloud Run Job. Source code in prefect_gcp/cloud_run.py class CloudRunJobResult ( InfrastructureResult ): \"\"\"Result from a Cloud Run Job.\"\"\" Execution ( BaseModel ) pydantic-model Utility class to call GCP executions API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Execution ( BaseModel ): \"\"\" Utility class to call GCP `executions` API and interact with the returned objects. \"\"\" name : str namespace : str metadata : dict spec : dict status : dict log_uri : str def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], ) Methods condition_after_completion Returns Execution condition if Execution has completed. Source code in prefect_gcp/cloud_run.py def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition get classmethod Make a get request to the GCP executions API and return an Execution instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], ) is_running Returns True if Execution is not completed. Source code in prefect_gcp/cloud_run.py def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None succeeded Whether or not the Execution completed is a successful state. Source code in prefect_gcp/cloud_run.py def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False Job ( BaseModel ) pydantic-model Utility class to call GCP jobs API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Job ( BaseModel ): \"\"\" Utility class to call GCP `jobs` API and interact with the returned objects. \"\"\" metadata : dict spec : dict status : dict name : str ready_condition : dict execution_status : dict def _is_missing_container ( self ): \"\"\" Check if Job status is not ready because the specified container cannot be found. \"\"\" if ( self . ready_condition . get ( \"status\" ) == \"False\" and self . ready_condition . get ( \"reason\" ) == \"ContainerMissing\" ): return True return False def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\" def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None ) @staticmethod def _get_ready_condition ( job : dict ) -> dict : \"\"\"Utility to access JSON field containing ready condition.\"\"\" if job [ \"status\" ] . get ( \"conditions\" ): for condition in job [ \"status\" ][ \"conditions\" ]: if condition [ \"type\" ] == \"Ready\" : return condition return {} @staticmethod def _get_execution_status ( job : dict ): \"\"\"Utility to access JSON field containing execution status.\"\"\" if job [ \"status\" ] . get ( \"latestCreatedExecution\" ): return job [ \"status\" ][ \"latestCreatedExecution\" ] return {} @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), ) @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response Methods create staticmethod Make a create request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response delete staticmethod Make a delete request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response get classmethod Make a get request to the GCP jobs API and return a Job instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), ) has_execution_in_progress See if job has a run in progress. Source code in prefect_gcp/cloud_run.py def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None ) is_ready Whether a job is finished registering and ready to be executed Source code in prefect_gcp/cloud_run.py def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\" run staticmethod Make a run request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"Cloud Run"},{"location":"cloud_run/#prefect_gcp.cloud_run","text":"Integrations with Google Cloud Run Job. Note this module is experimental. The intefaces within may change without notice. Examples: Run a job using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials ) . run () Run a job that runs the command echo hello world using Google Cloud Run Jobs: CloudRunJob ( image = \"gcr.io/my-project/my-image\" , region = \"us-east1\" , credentials = my_gcp_credentials command = [ \"echo\" , \"hello world\" ] ) . run ()","title":"cloud_run"},{"location":"cloud_run/#prefect_gcp.cloud_run-classes","text":"","title":"Classes"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob","text":"Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. Source code in prefect_gcp/cloud_run.py class CloudRunJob ( Infrastructure ): \"\"\" <span class=\"badge-api experimental\"/> Infrastructure block used to run GCP Cloud Run Jobs. Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project. Note this block is experimental. The interface may change without notice. \"\"\" _block_type_slug = \"cloud-run-job\" _block_type_name = \"GCP Cloud Run Job\" _description = \"Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.\" # noqa _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa type : Literal [ \"cloud-run-job\" ] = Field ( \"cloud-run-job\" , description = \"The slug for this task type.\" ) image : str = Field ( ... , title = \"Image Name\" , description = ( \"The image to use for a new Cloud Run Job. This value must \" \"refer to an image within either Google Container Registry \" \"or Google Artifact Registry, like `gcr.io/<project_name>/<repo>/`.\" ), ) region : str = Field ( ... , description = \"The region where the Cloud Run Job resides.\" ) credentials : GcpCredentials # cannot be Field; else it shows as Json # Job settings cpu : Optional [ int ] = Field ( default = None , title = \"CPU\" , description = ( \"The amount of compute allocated to the Cloud Run Job. \" \"The int must be valid based on the rules specified at \" \"https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .\" ), ) memory : Optional [ int ] = Field ( default = None , title = \"Memory\" , description = \"The amount of memory allocated to the Cloud Run Job.\" , ) memory_unit : Optional [ Literal [ \"G\" , \"Gi\" , \"M\" , \"Mi\" ]] = Field ( default = None , title = \"Memory Units\" , description = ( \"The unit of memory. See \" \"https://cloud.google.com/run/docs/configuring/memory-limits#setting \" \"for additional details.\" ), ) args : Optional [ List [ str ]] = Field ( default = None , description = ( \"Arguments to be passed to your Cloud Run Job's entrypoint command.\" ), ) env : Dict [ str , str ] = Field ( default_factory = dict , description = \"Environment variables to be passed to your Cloud Run Job.\" , ) # Cleanup behavior keep_job : Optional [ bool ] = Field ( default = False , title = \"Keep Job After Completion\" , description = \"Keep the completed Cloud Run Job on Google Cloud Platform.\" , ) timeout : Optional [ int ] = Field ( default = None , title = \"Job Timeout\" , description = ( \"The length of time that Prefect will wait for a Cloud Run Job to complete \" \"before raising an exception.\" ), ) # For private use _job_name : str = None _execution : Optional [ Execution ] = None @property def job_name ( self ): \"\"\"Create a unique and valid job name.\"\"\" if self . _job_name is None : # get `repo` from `gcr.io/<project_name>/repo/other` components = self . image . split ( \"/\" ) image_name = components [ 2 ] # only alphanumeric and '-' allowed for a job name modified_image_name = image_name . replace ( \":\" , \"-\" ) . replace ( \".\" , \"-\" ) # make 50 char limit for final job name, which will be '<name>-<uuid>' if len ( modified_image_name ) > 17 : modified_image_name = modified_image_name [: 17 ] name = f \" { modified_image_name } - { uuid4 () . hex } \" self . _job_name = name return self . _job_name @property def memory_string ( self ): \"\"\"Returns the string expected for memory resources argument.\"\"\" if self . memory and self . memory_unit : return str ( self . memory ) + self . memory_unit return None @validator ( \"image\" ) def _remove_image_spaces ( cls , value ): \"\"\"Deal with spaces in image names.\"\"\" if value is not None : return value . strip () @root_validator def _check_valid_memory ( cls , values ): \"\"\"Make sure memory conforms to expected values for API. See: https://cloud.google.com/run/docs/configuring/memory-limits#setting \"\"\" # noqa if ( values . get ( \"memory\" ) is not None and values . get ( \"memory_unit\" ) is None ) or ( values . get ( \"memory_unit\" ) is not None and values . get ( \"memory\" ) is None ): raise ValueError ( \"A memory value and unit must both be supplied to specify a memory\" \" value other than the default memory value.\" ) return values def _create_job_error ( self , exc ): \"\"\"Provides a nicer error for 404s when trying to create a Cloud Run Job.\"\"\" # TODO consider lookup table instead of the if/else, # also check for documented errors if exc . status_code == 404 : raise RuntimeError ( f \"Failed to find resources at { exc . uri } . Confirm that region\" f \" ' { self . region } ' is the correct region for your Cloud Run Job and\" f \" that { self . credentials . project } is the correct GCP project. If\" f \" your project ID is not correct, you are using a Credentials block\" f \" with permissions for the wrong project.\" ) from exc raise exc def _job_run_submission_error ( self , exc ): \"\"\"Provides a nicer error for 404s when submitting job runs.\"\"\" if exc . status_code == 404 : pat1 = r \"The requested URL [^ ]+ was not found on this server\" # pat2 = ( # r\"Resource '[^ ]+' of kind 'JOB' in region '[\\w\\-0-9]+' \" # r\"in project '[\\w\\-0-9]+' does not exist\" # ) if re . findall ( pat1 , str ( exc )): raise RuntimeError ( f \"Failed to find resources at { exc . uri } . \" f \"Confirm that region ' { self . region } ' is \" f \"the correct region for your Cloud Run Job \" f \"and that ' { self . credentials . project } ' is the \" f \"correct GCP project. If your project ID is not \" f \"correct, you are using a Credentials \" f \"block with permissions for the wrong project.\" ) from exc else : raise exc raise exc def _cpu_as_k8s_quantity ( self ) -> str : \"\"\"Return the CPU integer in the format expected by GCP Cloud Run Jobs API. See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ See also: https://cloud.google.com/run/docs/configuring/cpu#setting-jobs \"\"\" # noqa return str ( self . cpu * 1000 ) + \"m\" @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a task running Cloud Run. Args: identifier: The Cloud Run Job name. This should match a value yielded by CloudRunJob.run. \"\"\" if grace_seconds != 30 : self . logger . warning ( f \"Kill grace period of { grace_seconds } s requested, but GCP does not \" \"support dynamic grace period configuration. See here for more info: \" \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\" # noqa ) with self . _get_client () as client : await run_sync_in_worker_thread ( self . _kill_job , client = client , namespace = self . credentials . project , job_name = identifier , ) def _kill_job ( self , client : Resource , namespace : str , job_name : str ) -> None : \"\"\" Thin wrapper around Job.delete, wrapping a try/except since Job is an independent class that doesn't have knowledge of CloudRunJob and its associated logic. \"\"\" try : Job . delete ( client = client , namespace = namespace , job_name = job_name ) except Exception as exc : if \"does not exist\" in str ( exc ): raise InfrastructureNotFound ( f \"Cannot stop Cloud Run Job; the job name { job_name !r} \" \"could not be found.\" ) from exc raise def _create_job_and_wait_for_registration ( self , client : Resource ) -> None : \"\"\"Create a new job wait for it to finish registering.\"\"\" try : self . logger . info ( f \"Creating Cloud Run Job { self . job_name } \" ) Job . create ( client = client , namespace = self . credentials . project , body = self . _jobs_body (), ) except googleapiclient . errors . HttpError as exc : self . _create_job_error ( exc ) try : self . _wait_for_job_creation ( client = client , timeout = self . timeout ) except Exception : self . logger . exception ( \"Encountered an exception while waiting for job run creation\" ) if not self . keep_job : self . logger . info ( f \"Deleting Cloud Run Job { self . job_name } from Google Cloud Run.\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete\" f \" Cloud Run Job { self . job_name !r} \" ) raise def _begin_job_execution ( self , client : Resource ) -> Execution : \"\"\"Submit a job run for execution and return the execution object.\"\"\" try : self . logger . info ( f \"Submitting Cloud Run Job { self . job_name !r} for execution.\" ) submission = Job . run ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) job_execution = Execution . get ( client = client , namespace = submission [ \"metadata\" ][ \"namespace\" ], execution_name = submission [ \"metadata\" ][ \"name\" ], ) command = ( \" \" . join ( self . command ) if self . command else \"default container command\" ) self . logger . info ( f \"Cloud Run Job { self . job_name !r} : Running command { command !r} \" ) except Exception as exc : self . _job_run_submission_error ( exc ) return job_execution def _watch_job_execution_and_get_result ( self , client : Resource , execution : Execution , poll_interval : int ) -> CloudRunJobResult : \"\"\"Wait for execution to complete and then return result.\"\"\" try : job_execution = self . _watch_job_execution ( client = client , job_execution = execution , timeout = self . timeout , poll_interval = poll_interval , ) except Exception : self . logger . exception ( \"Received an unexpected exception while monitoring Cloud Run Job \" f \" { self . job_name !r} \" ) raise if job_execution . succeeded (): status_code = 0 self . logger . info ( f \"Job Run { self . job_name } completed successfully\" ) else : status_code = 1 error_msg = job_execution . condition_after_completion ()[ \"message\" ] self . logger . error ( f \"Job Run { self . job_name } did not complete successfully. { error_msg } \" ) self . logger . info ( f \"Job Run logs can be found on GCP at: { job_execution . log_uri } \" ) if not self . keep_job : self . logger . info ( f \"Deleting completed Cloud Run Job { self . job_name !r} from Google Cloud\" \" Run...\" ) try : Job . delete ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) except Exception : self . logger . exception ( \"Received an unexpected exception while attempting to delete Cloud\" f \" Run Job { self . job_name } \" ) return CloudRunJobResult ( identifier = self . job_name , status_code = status_code ) def _jobs_body ( self ) -> dict : \"\"\"Create properly formatted body used for a Job CREATE request. See: https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs \"\"\" jobs_metadata = { \"name\" : self . job_name , \"annotations\" : { # See: https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation # noqa \"run.googleapis.com/launch-stage\" : \"BETA\" }, } # env and command here containers = [ self . _add_container_settings ({ \"image\" : self . image })] body = { \"apiVersion\" : \"run.googleapis.com/v1\" , \"kind\" : \"Job\" , \"metadata\" : jobs_metadata , \"spec\" : { # JobSpec \"template\" : { # ExecutionTemplateSpec \"spec\" : { # ExecutionSpec \"template\" : { # TaskTemplateSpec \"spec\" : { \"containers\" : containers } # TaskSpec } }, } }, } return body def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 ) def _watch_job_execution ( self , client , job_execution : Execution , timeout : int , poll_interval : int = 5 ): \"\"\" Update job_execution status until it is no longer running or timeout is reached. \"\"\" t0 = time . time () while job_execution . is_running (): job_execution = Execution . get ( client = client , namespace = job_execution . namespace , execution_name = job_execution . name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) return job_execution def _wait_for_job_creation ( self , client : Resource , timeout : int , poll_interval : int = 5 ): \"\"\"Give created job time to register.\"\"\" job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name ) t0 = time . time () while not job . is_ready (): ready_condition = ( job . ready_condition if job . ready_condition else \"waiting for condition update\" ) self . logger . info ( f \"Job is not yet ready... Current condition: { ready_condition } \" ) job = Job . get ( client = client , namespace = self . credentials . project , job_name = self . job_name , ) elapsed_time = time . time () - t0 if timeout is not None and elapsed_time > timeout : raise RuntimeError ( f \"Timed out after { elapsed_time } s while waiting for Cloud Run Job \" \"execution to complete. Your job may still be running on GCP.\" ) time . sleep ( poll_interval ) def _get_client ( self ) -> Resource : \"\"\"Get the base client needed for interacting with GCP APIs.\"\"\" # region needed for 'v1' API api_endpoint = f \"https:// { self . region } -run.googleapis.com\" gcp_creds = self . credentials . get_credentials_from_service_account () options = ClientOptions ( api_endpoint = api_endpoint ) return discovery . build ( \"run\" , \"v1\" , client_options = options , credentials = gcp_creds ) . namespaces () # CONTAINER SETTINGS def _add_container_settings ( self , base_settings : Dict [ str , Any ]) -> Dict [ str , Any ]: \"\"\" Add settings related to containers for Cloud Run Jobs to a dictionary. Includes environment variables, entrypoint command, entrypoint arguments, and cpu and memory limits. See: https://cloud.google.com/run/docs/reference/rest/v1/Container and https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements \"\"\" # noqa container_settings = base_settings . copy () container_settings . update ( self . _add_env ()) container_settings . update ( self . _add_resources ()) container_settings . update ( self . _add_command ()) container_settings . update ( self . _add_args ()) return container_settings def _add_args ( self ) -> dict : \"\"\"Set the arguments that will be passed to the entrypoint for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"args\" : self . args } if self . args else {} def _add_command ( self ) -> dict : \"\"\"Set the command that a container will run for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container \"\"\" # noqa return { \"command\" : self . command } def _add_resources ( self ) -> dict : \"\"\"Set specified resources limits for a Cloud Run Job. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements See also: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/ \"\"\" # noqa resources = { \"limits\" : {}, \"requests\" : {}} if self . cpu is not None : cpu = self . _cpu_as_k8s_quantity () resources [ \"limits\" ][ \"cpu\" ] = cpu resources [ \"requests\" ][ \"cpu\" ] = cpu if self . memory_string is not None : resources [ \"limits\" ][ \"memory\" ] = self . memory_string resources [ \"requests\" ][ \"memory\" ] = self . memory_string return { \"resources\" : resources } if resources [ \"requests\" ] else {} def _add_env ( self ) -> dict : \"\"\"Add environment variables for a Cloud Run Job. Method `self._base_environment()` gets necessary Prefect environment variables from the config. See: https://cloud.google.com/run/docs/reference/rest/v1/Container#envvar for how environment variables are specified for Cloud Run Jobs. \"\"\" # noqa env = { ** self . _base_environment (), ** self . env } cloud_run_env = [{ \"name\" : k , \"value\" : v } for k , v in env . items ()] return { \"env\" : cloud_run_env }","title":"CloudRunJob"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob-attributes","text":"","title":"Attributes"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.args","text":"Arguments to be passed to your Cloud Run Job's entrypoint command.","title":"args"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.cpu","text":"The amount of compute allocated to the Cloud Run Job. The int must be valid based on the rules specified at https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .","title":"cpu"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.image","text":"The image to use for a new Cloud Run Job. This value must refer to an image within either Google Container Registry or Google Artifact Registry, like gcr.io/<project_name>/<repo>/ .","title":"image"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.job_name","text":"Create a unique and valid job name.","title":"job_name"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.keep_job","text":"Keep the completed Cloud Run Job on Google Cloud Platform.","title":"keep_job"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory","text":"The amount of memory allocated to the Cloud Run Job.","title":"memory"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory_string","text":"Returns the string expected for memory resources argument.","title":"memory_string"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory_unit","text":"The unit of memory. See https://cloud.google.com/run/docs/configuring/memory-limits#setting for additional details.","title":"memory_unit"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.region","text":"The region where the Cloud Run Job resides.","title":"region"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.timeout","text":"The length of time that Prefect will wait for a Cloud Run Job to complete before raising an exception.","title":"timeout"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob-methods","text":"","title":"Methods"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.kill","text":"Kill a task running Cloud Run. Parameters: Name Type Description Default identifier str The Cloud Run Job name. This should match a value yielded by CloudRunJob.run. required Source code in prefect_gcp/cloud_run.py @sync_compatible async def kill ( self , identifier : str , grace_seconds : int = 30 ) -> None : \"\"\" Kill a task running Cloud Run. Args: identifier: The Cloud Run Job name. This should match a value yielded by CloudRunJob.run. \"\"\" if grace_seconds != 30 : self . logger . warning ( f \"Kill grace period of { grace_seconds } s requested, but GCP does not \" \"support dynamic grace period configuration. See here for more info: \" \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\" # noqa ) with self . _get_client () as client : await run_sync_in_worker_thread ( self . _kill_job , client = client , namespace = self . credentials . project , job_name = identifier , )","title":"kill()"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.preview","text":"Generate a preview of the job definition that will be sent to GCP. Source code in prefect_gcp/cloud_run.py def preview ( self ) -> str : \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\" body = self . _jobs_body () container_settings = body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] body [ \"spec\" ][ \"template\" ][ \"spec\" ][ \"template\" ][ \"spec\" ][ \"containers\" ][ 0 ][ \"env\" ] = [ container_setting for container_setting in container_settings if container_setting [ \"name\" ] != \"PREFECT_API_KEY\" ] return json . dumps ( body , indent = 2 )","title":"preview()"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.run","text":"Run the configured job on a Google Cloud Run Job. Source code in prefect_gcp/cloud_run.py @sync_compatible async def run ( self , task_status : Optional [ TaskStatus ] = None ): \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\" with self . _get_client () as client : await run_sync_in_worker_thread ( self . _create_job_and_wait_for_registration , client ) job_execution = await run_sync_in_worker_thread ( self . _begin_job_execution , client ) if task_status : task_status . started ( self . job_name ) result = await run_sync_in_worker_thread ( self . _watch_job_execution_and_get_result , client , job_execution , 5 , ) return result","title":"run()"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJobResult","text":"Result from a Cloud Run Job. Source code in prefect_gcp/cloud_run.py class CloudRunJobResult ( InfrastructureResult ): \"\"\"Result from a Cloud Run Job.\"\"\"","title":"CloudRunJobResult"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution","text":"Utility class to call GCP executions API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Execution ( BaseModel ): \"\"\" Utility class to call GCP `executions` API and interact with the returned objects. \"\"\" name : str namespace : str metadata : dict spec : dict status : dict log_uri : str def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], )","title":"Execution"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution-methods","text":"","title":"Methods"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.condition_after_completion","text":"Returns Execution condition if Execution has completed. Source code in prefect_gcp/cloud_run.py def condition_after_completion ( self ): \"\"\"Returns Execution condition if Execution has completed.\"\"\" for condition in self . status [ \"conditions\" ]: if condition [ \"type\" ] == \"Completed\" : return condition","title":"condition_after_completion()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.get","text":"Make a get request to the GCP executions API and return an Execution instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , execution_name : str ): \"\"\" Make a get request to the GCP executions API and return an Execution instance. \"\"\" request = client . executions () . get ( name = f \"namespaces/ { namespace } /executions/ { execution_name } \" ) response = request . execute () return cls ( name = response [ \"metadata\" ][ \"name\" ], namespace = response [ \"metadata\" ][ \"namespace\" ], metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], log_uri = response [ \"status\" ][ \"logUri\" ], )","title":"get()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.is_running","text":"Returns True if Execution is not completed. Source code in prefect_gcp/cloud_run.py def is_running ( self ) -> bool : \"\"\"Returns True if Execution is not completed.\"\"\" return self . status . get ( \"completionTime\" ) is None","title":"is_running()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.succeeded","text":"Whether or not the Execution completed is a successful state. Source code in prefect_gcp/cloud_run.py def succeeded ( self ): \"\"\"Whether or not the Execution completed is a successful state.\"\"\" completed_condition = self . condition_after_completion () if completed_condition and completed_condition [ \"status\" ] == \"True\" : return True return False","title":"succeeded()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job","text":"Utility class to call GCP jobs API and interact with the returned objects. Source code in prefect_gcp/cloud_run.py class Job ( BaseModel ): \"\"\" Utility class to call GCP `jobs` API and interact with the returned objects. \"\"\" metadata : dict spec : dict status : dict name : str ready_condition : dict execution_status : dict def _is_missing_container ( self ): \"\"\" Check if Job status is not ready because the specified container cannot be found. \"\"\" if ( self . ready_condition . get ( \"status\" ) == \"False\" and self . ready_condition . get ( \"reason\" ) == \"ContainerMissing\" ): return True return False def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\" def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None ) @staticmethod def _get_ready_condition ( job : dict ) -> dict : \"\"\"Utility to access JSON field containing ready condition.\"\"\" if job [ \"status\" ] . get ( \"conditions\" ): for condition in job [ \"status\" ][ \"conditions\" ]: if condition [ \"type\" ] == \"Ready\" : return condition return {} @staticmethod def _get_execution_status ( job : dict ): \"\"\"Utility to access JSON field containing execution status.\"\"\" if job [ \"status\" ] . get ( \"latestCreatedExecution\" ): return job [ \"status\" ][ \"latestCreatedExecution\" ] return {} @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), ) @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"Job"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job-methods","text":"","title":"Methods"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.create","text":"Make a create request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def create ( client : Resource , namespace : str , body : dict ): \"\"\"Make a create request to the GCP jobs API.\"\"\" request = client . jobs () . create ( parent = f \"namespaces/ { namespace } \" , body = body ) response = request . execute () return response","title":"create()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.delete","text":"Make a delete request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def delete ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a delete request to the GCP jobs API.\"\"\" request = client . jobs () . delete ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"delete()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.get","text":"Make a get request to the GCP jobs API and return a Job instance. Source code in prefect_gcp/cloud_run.py @classmethod def get ( cls , client : Resource , namespace : str , job_name : str ): \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\" request = client . jobs () . get ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return cls ( metadata = response [ \"metadata\" ], spec = response [ \"spec\" ], status = response [ \"status\" ], name = response [ \"metadata\" ][ \"name\" ], ready_condition = cls . _get_ready_condition ( response ), execution_status = cls . _get_execution_status ( response ), )","title":"get()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.has_execution_in_progress","text":"See if job has a run in progress. Source code in prefect_gcp/cloud_run.py def has_execution_in_progress ( self ) -> bool : \"\"\"See if job has a run in progress.\"\"\" return ( self . execution_status == {} or self . execution_status . get ( \"completionTimestamp\" ) is None )","title":"has_execution_in_progress()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.is_ready","text":"Whether a job is finished registering and ready to be executed Source code in prefect_gcp/cloud_run.py def is_ready ( self ) -> bool : \"\"\"Whether a job is finished registering and ready to be executed\"\"\" if self . _is_missing_container (): raise Exception ( f \" { self . ready_condition [ 'message' ] } \" ) return self . ready_condition . get ( \"status\" ) == \"True\"","title":"is_ready()"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.run","text":"Make a run request to the GCP jobs API. Source code in prefect_gcp/cloud_run.py @staticmethod def run ( client : Resource , namespace : str , job_name : str ): \"\"\"Make a run request to the GCP jobs API.\"\"\" request = client . jobs () . run ( name = f \"namespaces/ { namespace } /jobs/ { job_name } \" ) response = request . execute () return response","title":"run()"},{"location":"cloud_storage/","text":"prefect_gcp.cloud_storage Tasks for interacting with GCP Cloud Storage. Classes GcsBucket ( WritableDeploymentStorage , WritableFileSystem , ObjectStorageBlock ) pydantic-model Block used to store data using GCP Cloud Storage Buckets. Attributes: Name Type Description bucket str Name of the bucket. gcp_credentials GcpCredentials The credentials to authenticate with GCP. bucket_folder str A default path to a folder within the GCS bucket to use for reading and writing objects. Examples: Load stored GCP Cloud Storage Bucket: from prefect_gcp.cloud_storage import GcsBucketBucket gcp_cloud_storage_bucket_block = GcsBucket . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/cloud_storage.py class GcsBucket ( WritableDeploymentStorage , WritableFileSystem , ObjectStorageBlock ): \"\"\" Block used to store data using GCP Cloud Storage Buckets. Attributes: bucket: Name of the bucket. gcp_credentials: The credentials to authenticate with GCP. bucket_folder: A default path to a folder within the GCS bucket to use for reading and writing objects. Example: Load stored GCP Cloud Storage Bucket: ```python from prefect_gcp.cloud_storage import GcsBucketBucket gcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCS Bucket\" bucket : str = Field ( ... , description = \"Name of the bucket.\" ) gcp_credentials : GcpCredentials = Field ( default_factory = GcpCredentials , description = \"The credentials to authenticate with GCP.\" , ) bucket_folder : str = Field ( default = \"\" , description = ( \"A default path to a folder within the GCS bucket to use \" \"for reading and writing objects.\" ), ) @validator ( \"bucket_folder\" , pre = True , always = True ) def _bucket_folder_suffix ( cls , value ): \"\"\" Ensures that the bucket folder is suffixed with a forward slash. \"\"\" if value != \"\" and not value . endswith ( \"/\" ): value = f \" { value } /\" return value def _resolve_path ( self , path : str ) -> str : \"\"\" A helper function used in write_path to join `self.bucket_folder` and `path`. Args: path: Name of the key, e.g. \"file1\". Each object in your bucket has a unique key (or key name). Returns: The joined path. \"\"\" path = path or str ( uuid4 ()) # If bucket_folder provided, it means we won't write to the root dir of # the bucket. So we need to add it on the front of the path. path = os . path . join ( self . bucket_folder , path ) if self . bucket_folder else path return path @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , ) @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path # NEW BLOCK INTERFACE METHODS BELOW def _join_bucket_folder ( self , bucket_path : str = \"\" ) -> str : \"\"\" Joins the base bucket folder to the bucket path. NOTE: If a method reuses another method in this class, be careful to not call this twice because it'll join the bucket folder twice. See https://github.com/PrefectHQ/prefect-aws/issues/141 for a past issue. \"\"\" bucket_path = str ( bucket_path ) if self . bucket_folder != \"\" and bucket_path . startswith ( self . bucket_folder ): self . logger . info ( f \"Bucket path { bucket_path !r} is already prefixed with \" f \"bucket folder { self . bucket_folder !r} ; is this intentional?\" ) return str ( Path ( self . bucket_folder ) / bucket_path ) @sync_compatible async def get_bucket ( self ) -> \"Bucket\" : \"\"\" Returns the bucket object. Returns: The bucket object. Examples: Get the bucket object. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.get_bucket() ``` \"\"\" self . logger . info ( f \"Getting bucket { self . bucket !r} .\" ) client = self . gcp_credentials . get_cloud_storage_client () bucket = await run_sync_in_worker_thread ( client . get_bucket , self . bucket ) return bucket @sync_compatible async def list_blobs ( self , folder : str = \"\" ) -> List [ Blob ]: \"\"\" Lists all blobs in the bucket that are in a folder. Folders are not included in the output. Args: folder: The folder to list blobs from. Returns: A list of Blob objects. Examples: Get all blobs from a folder named \"prefect\". ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.list_blobs(\"prefect\") ``` \"\"\" client = self . gcp_credentials . get_cloud_storage_client () bucket_path = self . _join_bucket_folder ( folder ) self . logger . info ( f \"Listing blobs in bucket { bucket_path } .\" ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = bucket_path ) # Ignore folders return [ blob for blob in blobs if not blob . name . endswith ( \"/\" )] @sync_compatible async def download_object_to_path ( self , from_path : str , to_path : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads an object from the object storage service to a path. Args: from_path: The path to the blob to download; this gets prefixed with the bucket_folder. to_path: The path to download the blob to. If not provided, the blob's name will be used. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the object was downloaded to. Examples: Download my_folder/notes.txt object to notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name # making path absolute, but converting back to str here # since !r looks nicer that way and filename arg expects str to_path = str ( Path ( to_path ) . absolute ()) bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to { to_path !r} .\" ) await run_sync_in_worker_thread ( blob . download_to_filename , filename = to_path , ** download_kwargs ) return Path ( to_path ) @sync_compatible async def download_object_to_file_object ( self , from_path : str , to_file_object : BinaryIO , ** download_kwargs : Dict [ str , Any ], ) -> BinaryIO : \"\"\" Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter. Args: from_path: The path to the blob to download from; this gets prefixed with the bucket_folder. to_file_object: The file-like object to download the blob to. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_file`. Returns: The file-like object that the object was downloaded to. Examples: Download my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with BytesIO() as buf: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf) ``` Download my_folder/notes.txt object to a BufferedWriter. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"wb\") as f: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to file object.\" ) await run_sync_in_worker_thread ( blob . download_to_file , file_obj = to_file_object , ** download_kwargs ) return to_file_object @sync_compatible async def download_folder_to_path ( self , from_folder : str , to_folder : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads objects *within* a folder (excluding the folder itself) from the object storage service to a folder. Args: from_folder: The path to the folder to download from; this gets prefixed with the bucket_folder. to_folder: The path to download the folder to. If not provided, will default to the current directory. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the folder was downloaded to. Examples: Download my_folder to a local folder named my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\") ``` \"\"\" if to_folder is None : to_folder = \"\" to_folder = Path ( to_folder ) . absolute () blobs = await self . list_blobs ( folder = from_folder ) if len ( blobs ) == 0 : self . logger . warning ( f \"No blobs were downloaded from \" f \"bucket { self . bucket !r} path { from_folder !r} .\" ) return to_folder # do not call self._join_bucket_folder for list_blobs # because it's built-in to that method already! # however, we still need to do it because we're using relative_to bucket_folder = self . _join_bucket_folder ( from_folder ) async_coros = [] for blob in blobs : bucket_path = Path ( blob . name ) . relative_to ( bucket_folder ) if bucket_path . is_dir (): continue to_path = to_folder / bucket_path to_path . parent . mkdir ( parents = True , exist_ok = True ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path \" f \" { str ( bucket_path ) !r} to { to_path } .\" ) async_coros . append ( run_sync_in_worker_thread ( blob . download_to_filename , filename = str ( to_path ), ** download_kwargs ) ) await asyncio . gather ( * async_coros ) return to_folder @sync_compatible async def upload_from_path ( self , from_path : Union [ str , Path ], to_path : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads an object from a path to the object storage service. Args: from_path: The path to the file to upload from. to_path: The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the object was uploaded to. Examples: Upload notes.txt to my_folder/notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name bucket_path = self . _join_bucket_folder ( to_path ) bucket = await self . get_bucket () blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from { from_path !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) return bucket_path @sync_compatible async def upload_from_file_object ( self , from_file_object : BinaryIO , to_path : str , ** upload_kwargs ) -> str : \"\"\" Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader. Args: from_file_object: The file-like object to upload from. to_path: The path to upload the object to; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file`. Returns: The path that the object was uploaded to. Examples: Upload my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\") ``` Upload BufferedReader object to my_folder/notes.txt. ```python from io import BufferedReader from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object( BufferedReader(f), \"my_folder/notes.txt\" ) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( to_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from file object to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_file , from_file_object , ** upload_kwargs ) return bucket_path @sync_compatible async def upload_from_folder ( self , from_folder : Union [ str , Path ], to_folder : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads files *within* a folder (excluding the folder itself) to the object storage service folder. Args: from_folder: The path to the folder to upload from. to_folder: The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the folder was uploaded to. Examples: Upload local folder my_folder to the bucket's folder my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_folder(\"my_folder\") ``` \"\"\" from_folder = Path ( from_folder ) bucket_folder = self . _join_bucket_folder ( to_folder or \"\" ) num_uploaded = 0 bucket = await self . get_bucket () async_coros = [] for from_path in from_folder . rglob ( \"**/*\" ): if from_path . is_dir (): continue bucket_path = str ( Path ( bucket_folder ) / from_path . relative_to ( from_folder )) self . logger . info ( f \"Uploading from { str ( from_path ) !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) blob = bucket . blob ( bucket_path ) async_coros . append ( run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) ) num_uploaded += 1 await asyncio . gather ( * async_coros ) if num_uploaded == 0 : self . logger . warning ( f \"No files were uploaded from { from_folder } .\" ) return bucket_folder Attributes bucket : str pydantic-field required Name of the bucket. bucket_folder : str pydantic-field A default path to a folder within the GCS bucket to use for reading and writing objects. gcp_credentials : GcpCredentials pydantic-field The credentials to authenticate with GCP. Methods download_folder_to_path async Downloads objects within a folder (excluding the folder itself) from the object storage service to a folder. Parameters: Name Type Description Default from_folder str The path to the folder to download from; this gets prefixed with the bucket_folder. required to_folder Union[str, pathlib.Path] The path to download the folder to. If not provided, will default to the current directory. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Path The absolute path that the folder was downloaded to. Examples: Download my_folder to a local folder named my_folder. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . download_folder_to_path ( \"my_folder\" , \"my_folder\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def download_folder_to_path ( self , from_folder : str , to_folder : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads objects *within* a folder (excluding the folder itself) from the object storage service to a folder. Args: from_folder: The path to the folder to download from; this gets prefixed with the bucket_folder. to_folder: The path to download the folder to. If not provided, will default to the current directory. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the folder was downloaded to. Examples: Download my_folder to a local folder named my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\") ``` \"\"\" if to_folder is None : to_folder = \"\" to_folder = Path ( to_folder ) . absolute () blobs = await self . list_blobs ( folder = from_folder ) if len ( blobs ) == 0 : self . logger . warning ( f \"No blobs were downloaded from \" f \"bucket { self . bucket !r} path { from_folder !r} .\" ) return to_folder # do not call self._join_bucket_folder for list_blobs # because it's built-in to that method already! # however, we still need to do it because we're using relative_to bucket_folder = self . _join_bucket_folder ( from_folder ) async_coros = [] for blob in blobs : bucket_path = Path ( blob . name ) . relative_to ( bucket_folder ) if bucket_path . is_dir (): continue to_path = to_folder / bucket_path to_path . parent . mkdir ( parents = True , exist_ok = True ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path \" f \" { str ( bucket_path ) !r} to { to_path } .\" ) async_coros . append ( run_sync_in_worker_thread ( blob . download_to_filename , filename = str ( to_path ), ** download_kwargs ) ) await asyncio . gather ( * async_coros ) return to_folder download_object_to_file_object async Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter. Parameters: Name Type Description Default from_path str The path to the blob to download from; this gets prefixed with the bucket_folder. required to_file_object BinaryIO The file-like object to download the blob to. required **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_file . {} Returns: Type Description BinaryIO The file-like object that the object was downloaded to. Examples: Download my_folder/notes.txt object to a BytesIO object. from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with BytesIO () as buf : gcs_bucket . download_object_to_file_object ( \"my_folder/notes.txt\" , buf ) Download my_folder/notes.txt object to a BufferedWriter. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with open ( \"notes.txt\" , \"wb\" ) as f : gcs_bucket . download_object_to_file_object ( \"my_folder/notes.txt\" , f ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def download_object_to_file_object ( self , from_path : str , to_file_object : BinaryIO , ** download_kwargs : Dict [ str , Any ], ) -> BinaryIO : \"\"\" Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter. Args: from_path: The path to the blob to download from; this gets prefixed with the bucket_folder. to_file_object: The file-like object to download the blob to. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_file`. Returns: The file-like object that the object was downloaded to. Examples: Download my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with BytesIO() as buf: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf) ``` Download my_folder/notes.txt object to a BufferedWriter. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"wb\") as f: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to file object.\" ) await run_sync_in_worker_thread ( blob . download_to_file , file_obj = to_file_object , ** download_kwargs ) return to_file_object download_object_to_path async Downloads an object from the object storage service to a path. Parameters: Name Type Description Default from_path str The path to the blob to download; this gets prefixed with the bucket_folder. required to_path Union[str, pathlib.Path] The path to download the blob to. If not provided, the blob's name will be used. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Path The absolute path that the object was downloaded to. Examples: Download my_folder/notes.txt object to notes.txt. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . download_object_to_path ( \"my_folder/notes.txt\" , \"notes.txt\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def download_object_to_path ( self , from_path : str , to_path : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads an object from the object storage service to a path. Args: from_path: The path to the blob to download; this gets prefixed with the bucket_folder. to_path: The path to download the blob to. If not provided, the blob's name will be used. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the object was downloaded to. Examples: Download my_folder/notes.txt object to notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name # making path absolute, but converting back to str here # since !r looks nicer that way and filename arg expects str to_path = str ( Path ( to_path ) . absolute ()) bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to { to_path !r} .\" ) await run_sync_in_worker_thread ( blob . download_to_filename , filename = to_path , ** download_kwargs ) return Path ( to_path ) get_bucket async Returns the bucket object. Returns: Type Description Bucket The bucket object. Examples: Get the bucket object. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . get_bucket () Source code in prefect_gcp/cloud_storage.py @sync_compatible async def get_bucket ( self ) -> \"Bucket\" : \"\"\" Returns the bucket object. Returns: The bucket object. Examples: Get the bucket object. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.get_bucket() ``` \"\"\" self . logger . info ( f \"Getting bucket { self . bucket !r} .\" ) client = self . gcp_credentials . get_cloud_storage_client () bucket = await run_sync_in_worker_thread ( client . get_bucket , self . bucket ) return bucket get_directory async Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Parameters: Name Type Description Default from_path Optional[str] Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. None local_path Optional[str] Local path to download GCS bucket contents to. Defaults to the current working directory. None Source code in prefect_gcp/cloud_storage.py @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , ) list_blobs async Lists all blobs in the bucket that are in a folder. Folders are not included in the output. Parameters: Name Type Description Default folder str The folder to list blobs from. '' Returns: Type Description List[google.cloud.storage.blob.Blob] A list of Blob objects. Examples: Get all blobs from a folder named \"prefect\". from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . list_blobs ( \"prefect\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def list_blobs ( self , folder : str = \"\" ) -> List [ Blob ]: \"\"\" Lists all blobs in the bucket that are in a folder. Folders are not included in the output. Args: folder: The folder to list blobs from. Returns: A list of Blob objects. Examples: Get all blobs from a folder named \"prefect\". ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.list_blobs(\"prefect\") ``` \"\"\" client = self . gcp_credentials . get_cloud_storage_client () bucket_path = self . _join_bucket_folder ( folder ) self . logger . info ( f \"Listing blobs in bucket { bucket_path } .\" ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = bucket_path ) # Ignore folders return [ blob for blob in blobs if not blob . name . endswith ( \"/\" )] put_directory async Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Parameters: Name Type Description Default local_path Optional[str] Path to local directory to upload from. None to_path Optional[str] Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. None ignore_file Optional[str] Path to file containing gitignore style expressions for filepaths to ignore. None Returns: Type Description int The number of files uploaded. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count read_path async Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Parameters: Name Type Description Default path str Entire path to (and including) the key. required Returns: Type Description bytes A bytes or string representation of the blob object. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents upload_from_file_object async Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader. Parameters: Name Type Description Default from_file_object BinaryIO The file-like object to upload from. required to_path str The path to upload the object to; this gets prefixed with the bucket_folder. required **upload_kwargs Additional keyword arguments to pass to Blob.upload_from_file . {} Returns: Type Description str The path that the object was uploaded to. Examples: Upload my_folder/notes.txt object to a BytesIO object. from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with open ( \"notes.txt\" , \"rb\" ) as f : gcs_bucket . upload_from_file_object ( f , \"my_folder/notes.txt\" ) Upload BufferedReader object to my_folder/notes.txt. from io import BufferedReader from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with open ( \"notes.txt\" , \"rb\" ) as f : gcs_bucket . upload_from_file_object ( BufferedReader ( f ), \"my_folder/notes.txt\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def upload_from_file_object ( self , from_file_object : BinaryIO , to_path : str , ** upload_kwargs ) -> str : \"\"\" Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader. Args: from_file_object: The file-like object to upload from. to_path: The path to upload the object to; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file`. Returns: The path that the object was uploaded to. Examples: Upload my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\") ``` Upload BufferedReader object to my_folder/notes.txt. ```python from io import BufferedReader from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object( BufferedReader(f), \"my_folder/notes.txt\" ) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( to_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from file object to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_file , from_file_object , ** upload_kwargs ) return bucket_path upload_from_folder async Uploads files within a folder (excluding the folder itself) to the object storage service folder. Parameters: Name Type Description Default from_folder Union[str, pathlib.Path] The path to the folder to upload from. required to_folder Optional[str] The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_filename . {} Returns: Type Description str The path that the folder was uploaded to. Examples: Upload local folder my_folder to the bucket's folder my_folder. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . upload_from_folder ( \"my_folder\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def upload_from_folder ( self , from_folder : Union [ str , Path ], to_folder : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads files *within* a folder (excluding the folder itself) to the object storage service folder. Args: from_folder: The path to the folder to upload from. to_folder: The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the folder was uploaded to. Examples: Upload local folder my_folder to the bucket's folder my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_folder(\"my_folder\") ``` \"\"\" from_folder = Path ( from_folder ) bucket_folder = self . _join_bucket_folder ( to_folder or \"\" ) num_uploaded = 0 bucket = await self . get_bucket () async_coros = [] for from_path in from_folder . rglob ( \"**/*\" ): if from_path . is_dir (): continue bucket_path = str ( Path ( bucket_folder ) / from_path . relative_to ( from_folder )) self . logger . info ( f \"Uploading from { str ( from_path ) !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) blob = bucket . blob ( bucket_path ) async_coros . append ( run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) ) num_uploaded += 1 await asyncio . gather ( * async_coros ) if num_uploaded == 0 : self . logger . warning ( f \"No files were uploaded from { from_folder } .\" ) return bucket_folder upload_from_path async Uploads an object from a path to the object storage service. Parameters: Name Type Description Default from_path Union[str, pathlib.Path] The path to the file to upload from. required to_path Optional[str] The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_filename . {} Returns: Type Description str The path that the object was uploaded to. Examples: Upload notes.txt to my_folder/notes.txt. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . upload_from_path ( \"notes.txt\" , \"my_folder/notes.txt\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def upload_from_path ( self , from_path : Union [ str , Path ], to_path : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads an object from a path to the object storage service. Args: from_path: The path to the file to upload from. to_path: The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the object was uploaded to. Examples: Upload notes.txt to my_folder/notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name bucket_path = self . _join_bucket_folder ( to_path ) bucket = await self . get_bucket () blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from { from_path !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) return bucket_path write_path async Writes to an GCS bucket. Parameters: Name Type Description Default path str The key name. Each object in your bucket has a unique key (or key name). required content bytes What you are uploading to GCS Bucket. required Returns: Type Description str The path that the contents were written to. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path Functions cloud_storage_copy_blob async Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Parameters: Name Type Description Default source_bucket str Source bucket name. required dest_bucket str Destination bucket name. required source_blob str Source blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required dest_blob Optional[str] Destination blob name; if not provided, defaults to source_blob. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **copy_kwargs Dict[str, Any] Additional keyword arguments to pass to Bucket.copy_blob . {} Returns: Type Description str Destination blob name. Examples: Copies blob from one bucket to another. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow () def example_cloud_storage_copy_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_copy_blob ( \"source_bucket\" , \"dest_bucket\" , \"source_blob\" , gcp_credentials ) return blob example_cloud_storage_copy_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_copy_blob ( source_bucket : str , dest_bucket : str , source_blob : str , gcp_credentials : GcpCredentials , dest_blob : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** copy_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Args: source_bucket: Source bucket name. dest_bucket: Destination bucket name. source_blob: Source blob name. gcp_credentials: Credentials to use for authentication with GCP. dest_blob: Destination blob name; if not provided, defaults to source_blob. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **copy_kwargs: Additional keyword arguments to pass to `Bucket.copy_blob`. Returns: Destination blob name. Example: Copies blob from one bucket to another. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow() def example_cloud_storage_copy_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_copy_blob( \"source_bucket\", \"dest_bucket\", \"source_blob\", gcp_credentials ) return blob example_cloud_storage_copy_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Copying blob named %s from the %s bucket to the %s bucket\" , source_blob , source_bucket , dest_bucket , ) source_bucket_obj = await _get_bucket ( source_bucket , gcp_credentials , project = project ) dest_bucket_obj = await _get_bucket ( dest_bucket , gcp_credentials , project = project ) if dest_blob is None : dest_blob = source_blob source_blob_obj = source_bucket_obj . blob ( source_blob ) await run_sync_in_worker_thread ( source_bucket_obj . copy_blob , blob = source_blob_obj , destination_bucket = dest_bucket_obj , new_name = dest_blob , timeout = timeout , ** copy_kwargs , ) return dest_blob cloud_storage_create_bucket async Creates a bucket. Parameters: Name Type Description Default bucket str Name of the bucket. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None location Optional[str] Location of the bucket. None **create_kwargs Dict[str, Any] Additional keyword arguments to pass to client.create_bucket . {} Returns: Type Description str The bucket name. Examples: Creates a bucket named \"prefect\". from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow () def example_cloud_storage_create_bucket_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) bucket = cloud_storage_create_bucket ( \"prefect\" , gcp_credentials ) example_cloud_storage_create_bucket_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_create_bucket ( bucket : str , gcp_credentials : GcpCredentials , project : Optional [ str ] = None , location : Optional [ str ] = None , ** create_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Creates a bucket. Args: bucket: Name of the bucket. gcp_credentials: Credentials to use for authentication with GCP. project: Name of the project to use; overrides the gcp_credentials project if provided. location: Location of the bucket. **create_kwargs: Additional keyword arguments to pass to `client.create_bucket`. Returns: The bucket name. Example: Creates a bucket named \"prefect\". ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow() def example_cloud_storage_create_bucket_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials) example_cloud_storage_create_bucket_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s bucket\" , bucket ) client = gcp_credentials . get_cloud_storage_client ( project = project ) await run_sync_in_worker_thread ( client . create_bucket , bucket , location = location , ** create_kwargs ) return bucket cloud_storage_download_blob_as_bytes async Downloads a blob as bytes. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_as_bytes . {} Returns: Type Description bytes A bytes or string representation of the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = cloud_storage_download_blob_as_bytes ( \"bucket\" , \"blob\" , gcp_credentials ) return contents example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_as_bytes ( bucket : str , blob : str , gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> bytes : \"\"\" Downloads a blob as bytes. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_as_bytes`. Returns: A bytes or string representation of the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") contents = cloud_storage_download_blob_as_bytes( \"bucket\", \"blob\", gcp_credentials) return contents example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) contents = await run_sync_in_worker_thread ( blob_obj . download_as_bytes , timeout = timeout , ** download_kwargs ) return contents cloud_storage_download_blob_to_file async Downloads a blob to a file path. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required path Union[str, pathlib.Path] Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Union[str, pathlib.Path] The path to the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) path = cloud_storage_download_blob_to_file ( \"bucket\" , \"blob\" , \"file_path\" , gcp_credentials ) return path example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_to_file ( bucket : str , blob : str , path : Union [ str , Path ], gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> Union [ str , Path ]: \"\"\" Downloads a blob to a file path. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. path: Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The path to the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") path = cloud_storage_download_blob_to_file( \"bucket\", \"blob\", \"file_path\", gcp_credentials) return path example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket to %s \" , blob , bucket , path ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if os . path . isdir ( path ): if isinstance ( path , Path ): path = path . joinpath ( blob ) # keep as Path if Path is passed else : path = os . path . join ( path , blob ) # keep as str if a str is passed await run_sync_in_worker_thread ( blob_obj . download_to_filename , path , timeout = timeout , ** download_kwargs ) return path cloud_storage_upload_blob_from_file async Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Parameters: Name Type Description Default file Union[str, pathlib.Path, _io.BytesIO] Path to data or file like object to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_file or Blob.upload_from_filename . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow () def example_cloud_storage_upload_blob_from_file_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_file ( \"/path/somewhere\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_file_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_file ( file : Union [ str , Path , BytesIO ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Args: file: Path to data or file like object to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file` or `Blob.upload_from_filename`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow() def example_cloud_storage_upload_blob_from_file_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_file( \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if isinstance ( file , BytesIO ): await run_sync_in_worker_thread ( blob_obj . upload_from_file , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) else : await run_sync_in_worker_thread ( blob_obj . upload_from_filename , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob cloud_storage_upload_blob_from_string async Uploads a blob from a string or bytes representation of data. Parameters: Name Type Description Default data Union[str, bytes] String or bytes representation of data to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_string . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow () def example_cloud_storage_upload_blob_from_string_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_string ( \"data\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_string_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_string ( data : Union [ str , bytes ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from a string or bytes representation of data. Args: data: String or bytes representation of data to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_string`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow() def example_cloud_storage_upload_blob_from_string_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_string( \"data\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_string_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) await run_sync_in_worker_thread ( blob_obj . upload_from_string , data , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob","title":"Cloud Storage"},{"location":"cloud_storage/#prefect_gcp.cloud_storage","text":"Tasks for interacting with GCP Cloud Storage.","title":"cloud_storage"},{"location":"cloud_storage/#prefect_gcp.cloud_storage-classes","text":"","title":"Classes"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket","text":"Block used to store data using GCP Cloud Storage Buckets. Attributes: Name Type Description bucket str Name of the bucket. gcp_credentials GcpCredentials The credentials to authenticate with GCP. bucket_folder str A default path to a folder within the GCS bucket to use for reading and writing objects. Examples: Load stored GCP Cloud Storage Bucket: from prefect_gcp.cloud_storage import GcsBucketBucket gcp_cloud_storage_bucket_block = GcsBucket . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/cloud_storage.py class GcsBucket ( WritableDeploymentStorage , WritableFileSystem , ObjectStorageBlock ): \"\"\" Block used to store data using GCP Cloud Storage Buckets. Attributes: bucket: Name of the bucket. gcp_credentials: The credentials to authenticate with GCP. bucket_folder: A default path to a folder within the GCS bucket to use for reading and writing objects. Example: Load stored GCP Cloud Storage Bucket: ```python from prefect_gcp.cloud_storage import GcsBucketBucket gcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCS Bucket\" bucket : str = Field ( ... , description = \"Name of the bucket.\" ) gcp_credentials : GcpCredentials = Field ( default_factory = GcpCredentials , description = \"The credentials to authenticate with GCP.\" , ) bucket_folder : str = Field ( default = \"\" , description = ( \"A default path to a folder within the GCS bucket to use \" \"for reading and writing objects.\" ), ) @validator ( \"bucket_folder\" , pre = True , always = True ) def _bucket_folder_suffix ( cls , value ): \"\"\" Ensures that the bucket folder is suffixed with a forward slash. \"\"\" if value != \"\" and not value . endswith ( \"/\" ): value = f \" { value } /\" return value def _resolve_path ( self , path : str ) -> str : \"\"\" A helper function used in write_path to join `self.bucket_folder` and `path`. Args: path: Name of the key, e.g. \"file1\". Each object in your bucket has a unique key (or key name). Returns: The joined path. \"\"\" path = path or str ( uuid4 ()) # If bucket_folder provided, it means we won't write to the root dir of # the bucket. So we need to add it on the front of the path. path = os . path . join ( self . bucket_folder , path ) if self . bucket_folder else path return path @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , ) @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path # NEW BLOCK INTERFACE METHODS BELOW def _join_bucket_folder ( self , bucket_path : str = \"\" ) -> str : \"\"\" Joins the base bucket folder to the bucket path. NOTE: If a method reuses another method in this class, be careful to not call this twice because it'll join the bucket folder twice. See https://github.com/PrefectHQ/prefect-aws/issues/141 for a past issue. \"\"\" bucket_path = str ( bucket_path ) if self . bucket_folder != \"\" and bucket_path . startswith ( self . bucket_folder ): self . logger . info ( f \"Bucket path { bucket_path !r} is already prefixed with \" f \"bucket folder { self . bucket_folder !r} ; is this intentional?\" ) return str ( Path ( self . bucket_folder ) / bucket_path ) @sync_compatible async def get_bucket ( self ) -> \"Bucket\" : \"\"\" Returns the bucket object. Returns: The bucket object. Examples: Get the bucket object. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.get_bucket() ``` \"\"\" self . logger . info ( f \"Getting bucket { self . bucket !r} .\" ) client = self . gcp_credentials . get_cloud_storage_client () bucket = await run_sync_in_worker_thread ( client . get_bucket , self . bucket ) return bucket @sync_compatible async def list_blobs ( self , folder : str = \"\" ) -> List [ Blob ]: \"\"\" Lists all blobs in the bucket that are in a folder. Folders are not included in the output. Args: folder: The folder to list blobs from. Returns: A list of Blob objects. Examples: Get all blobs from a folder named \"prefect\". ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.list_blobs(\"prefect\") ``` \"\"\" client = self . gcp_credentials . get_cloud_storage_client () bucket_path = self . _join_bucket_folder ( folder ) self . logger . info ( f \"Listing blobs in bucket { bucket_path } .\" ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = bucket_path ) # Ignore folders return [ blob for blob in blobs if not blob . name . endswith ( \"/\" )] @sync_compatible async def download_object_to_path ( self , from_path : str , to_path : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads an object from the object storage service to a path. Args: from_path: The path to the blob to download; this gets prefixed with the bucket_folder. to_path: The path to download the blob to. If not provided, the blob's name will be used. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the object was downloaded to. Examples: Download my_folder/notes.txt object to notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name # making path absolute, but converting back to str here # since !r looks nicer that way and filename arg expects str to_path = str ( Path ( to_path ) . absolute ()) bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to { to_path !r} .\" ) await run_sync_in_worker_thread ( blob . download_to_filename , filename = to_path , ** download_kwargs ) return Path ( to_path ) @sync_compatible async def download_object_to_file_object ( self , from_path : str , to_file_object : BinaryIO , ** download_kwargs : Dict [ str , Any ], ) -> BinaryIO : \"\"\" Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter. Args: from_path: The path to the blob to download from; this gets prefixed with the bucket_folder. to_file_object: The file-like object to download the blob to. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_file`. Returns: The file-like object that the object was downloaded to. Examples: Download my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with BytesIO() as buf: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf) ``` Download my_folder/notes.txt object to a BufferedWriter. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"wb\") as f: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to file object.\" ) await run_sync_in_worker_thread ( blob . download_to_file , file_obj = to_file_object , ** download_kwargs ) return to_file_object @sync_compatible async def download_folder_to_path ( self , from_folder : str , to_folder : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads objects *within* a folder (excluding the folder itself) from the object storage service to a folder. Args: from_folder: The path to the folder to download from; this gets prefixed with the bucket_folder. to_folder: The path to download the folder to. If not provided, will default to the current directory. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the folder was downloaded to. Examples: Download my_folder to a local folder named my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\") ``` \"\"\" if to_folder is None : to_folder = \"\" to_folder = Path ( to_folder ) . absolute () blobs = await self . list_blobs ( folder = from_folder ) if len ( blobs ) == 0 : self . logger . warning ( f \"No blobs were downloaded from \" f \"bucket { self . bucket !r} path { from_folder !r} .\" ) return to_folder # do not call self._join_bucket_folder for list_blobs # because it's built-in to that method already! # however, we still need to do it because we're using relative_to bucket_folder = self . _join_bucket_folder ( from_folder ) async_coros = [] for blob in blobs : bucket_path = Path ( blob . name ) . relative_to ( bucket_folder ) if bucket_path . is_dir (): continue to_path = to_folder / bucket_path to_path . parent . mkdir ( parents = True , exist_ok = True ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path \" f \" { str ( bucket_path ) !r} to { to_path } .\" ) async_coros . append ( run_sync_in_worker_thread ( blob . download_to_filename , filename = str ( to_path ), ** download_kwargs ) ) await asyncio . gather ( * async_coros ) return to_folder @sync_compatible async def upload_from_path ( self , from_path : Union [ str , Path ], to_path : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads an object from a path to the object storage service. Args: from_path: The path to the file to upload from. to_path: The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the object was uploaded to. Examples: Upload notes.txt to my_folder/notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name bucket_path = self . _join_bucket_folder ( to_path ) bucket = await self . get_bucket () blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from { from_path !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) return bucket_path @sync_compatible async def upload_from_file_object ( self , from_file_object : BinaryIO , to_path : str , ** upload_kwargs ) -> str : \"\"\" Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader. Args: from_file_object: The file-like object to upload from. to_path: The path to upload the object to; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file`. Returns: The path that the object was uploaded to. Examples: Upload my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\") ``` Upload BufferedReader object to my_folder/notes.txt. ```python from io import BufferedReader from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object( BufferedReader(f), \"my_folder/notes.txt\" ) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( to_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from file object to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_file , from_file_object , ** upload_kwargs ) return bucket_path @sync_compatible async def upload_from_folder ( self , from_folder : Union [ str , Path ], to_folder : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads files *within* a folder (excluding the folder itself) to the object storage service folder. Args: from_folder: The path to the folder to upload from. to_folder: The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the folder was uploaded to. Examples: Upload local folder my_folder to the bucket's folder my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_folder(\"my_folder\") ``` \"\"\" from_folder = Path ( from_folder ) bucket_folder = self . _join_bucket_folder ( to_folder or \"\" ) num_uploaded = 0 bucket = await self . get_bucket () async_coros = [] for from_path in from_folder . rglob ( \"**/*\" ): if from_path . is_dir (): continue bucket_path = str ( Path ( bucket_folder ) / from_path . relative_to ( from_folder )) self . logger . info ( f \"Uploading from { str ( from_path ) !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) blob = bucket . blob ( bucket_path ) async_coros . append ( run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) ) num_uploaded += 1 await asyncio . gather ( * async_coros ) if num_uploaded == 0 : self . logger . warning ( f \"No files were uploaded from { from_folder } .\" ) return bucket_folder","title":"GcsBucket"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket-attributes","text":"","title":"Attributes"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.bucket","text":"Name of the bucket.","title":"bucket"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.bucket_folder","text":"A default path to a folder within the GCS bucket to use for reading and writing objects.","title":"bucket_folder"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.gcp_credentials","text":"The credentials to authenticate with GCP.","title":"gcp_credentials"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket-methods","text":"","title":"Methods"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_folder_to_path","text":"Downloads objects within a folder (excluding the folder itself) from the object storage service to a folder. Parameters: Name Type Description Default from_folder str The path to the folder to download from; this gets prefixed with the bucket_folder. required to_folder Union[str, pathlib.Path] The path to download the folder to. If not provided, will default to the current directory. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Path The absolute path that the folder was downloaded to. Examples: Download my_folder to a local folder named my_folder. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . download_folder_to_path ( \"my_folder\" , \"my_folder\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def download_folder_to_path ( self , from_folder : str , to_folder : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads objects *within* a folder (excluding the folder itself) from the object storage service to a folder. Args: from_folder: The path to the folder to download from; this gets prefixed with the bucket_folder. to_folder: The path to download the folder to. If not provided, will default to the current directory. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the folder was downloaded to. Examples: Download my_folder to a local folder named my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\") ``` \"\"\" if to_folder is None : to_folder = \"\" to_folder = Path ( to_folder ) . absolute () blobs = await self . list_blobs ( folder = from_folder ) if len ( blobs ) == 0 : self . logger . warning ( f \"No blobs were downloaded from \" f \"bucket { self . bucket !r} path { from_folder !r} .\" ) return to_folder # do not call self._join_bucket_folder for list_blobs # because it's built-in to that method already! # however, we still need to do it because we're using relative_to bucket_folder = self . _join_bucket_folder ( from_folder ) async_coros = [] for blob in blobs : bucket_path = Path ( blob . name ) . relative_to ( bucket_folder ) if bucket_path . is_dir (): continue to_path = to_folder / bucket_path to_path . parent . mkdir ( parents = True , exist_ok = True ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path \" f \" { str ( bucket_path ) !r} to { to_path } .\" ) async_coros . append ( run_sync_in_worker_thread ( blob . download_to_filename , filename = str ( to_path ), ** download_kwargs ) ) await asyncio . gather ( * async_coros ) return to_folder","title":"download_folder_to_path()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_object_to_file_object","text":"Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter. Parameters: Name Type Description Default from_path str The path to the blob to download from; this gets prefixed with the bucket_folder. required to_file_object BinaryIO The file-like object to download the blob to. required **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_file . {} Returns: Type Description BinaryIO The file-like object that the object was downloaded to. Examples: Download my_folder/notes.txt object to a BytesIO object. from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with BytesIO () as buf : gcs_bucket . download_object_to_file_object ( \"my_folder/notes.txt\" , buf ) Download my_folder/notes.txt object to a BufferedWriter. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with open ( \"notes.txt\" , \"wb\" ) as f : gcs_bucket . download_object_to_file_object ( \"my_folder/notes.txt\" , f ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def download_object_to_file_object ( self , from_path : str , to_file_object : BinaryIO , ** download_kwargs : Dict [ str , Any ], ) -> BinaryIO : \"\"\" Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter. Args: from_path: The path to the blob to download from; this gets prefixed with the bucket_folder. to_file_object: The file-like object to download the blob to. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_file`. Returns: The file-like object that the object was downloaded to. Examples: Download my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with BytesIO() as buf: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf) ``` Download my_folder/notes.txt object to a BufferedWriter. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"wb\") as f: gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to file object.\" ) await run_sync_in_worker_thread ( blob . download_to_file , file_obj = to_file_object , ** download_kwargs ) return to_file_object","title":"download_object_to_file_object()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_object_to_path","text":"Downloads an object from the object storage service to a path. Parameters: Name Type Description Default from_path str The path to the blob to download; this gets prefixed with the bucket_folder. required to_path Union[str, pathlib.Path] The path to download the blob to. If not provided, the blob's name will be used. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Path The absolute path that the object was downloaded to. Examples: Download my_folder/notes.txt object to notes.txt. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . download_object_to_path ( \"my_folder/notes.txt\" , \"notes.txt\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def download_object_to_path ( self , from_path : str , to_path : Optional [ Union [ str , Path ]] = None , ** download_kwargs : Dict [ str , Any ], ) -> Path : \"\"\" Downloads an object from the object storage service to a path. Args: from_path: The path to the blob to download; this gets prefixed with the bucket_folder. to_path: The path to download the blob to. If not provided, the blob's name will be used. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The absolute path that the object was downloaded to. Examples: Download my_folder/notes.txt object to notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name # making path absolute, but converting back to str here # since !r looks nicer that way and filename arg expects str to_path = str ( Path ( to_path ) . absolute ()) bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( from_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Downloading blob from bucket { self . bucket !r} path { bucket_path !r} \" f \"to { to_path !r} .\" ) await run_sync_in_worker_thread ( blob . download_to_filename , filename = to_path , ** download_kwargs ) return Path ( to_path )","title":"download_object_to_path()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.get_bucket","text":"Returns the bucket object. Returns: Type Description Bucket The bucket object. Examples: Get the bucket object. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . get_bucket () Source code in prefect_gcp/cloud_storage.py @sync_compatible async def get_bucket ( self ) -> \"Bucket\" : \"\"\" Returns the bucket object. Returns: The bucket object. Examples: Get the bucket object. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.get_bucket() ``` \"\"\" self . logger . info ( f \"Getting bucket { self . bucket !r} .\" ) client = self . gcp_credentials . get_cloud_storage_client () bucket = await run_sync_in_worker_thread ( client . get_bucket , self . bucket ) return bucket","title":"get_bucket()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.get_directory","text":"Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Parameters: Name Type Description Default from_path Optional[str] Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. None local_path Optional[str] Local path to download GCS bucket contents to. Defaults to the current working directory. None Source code in prefect_gcp/cloud_storage.py @sync_compatible async def get_directory ( self , from_path : Optional [ str ] = None , local_path : Optional [ str ] = None ) -> None : \"\"\" Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory. Args: from_path: Path in GCS bucket to download from. Defaults to the block's configured bucket_folder. local_path: Local path to download GCS bucket contents to. Defaults to the current working directory. \"\"\" from_path = ( self . bucket_folder if from_path is None else self . _resolve_path ( from_path ) ) if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) project = self . gcp_credentials . project client = self . gcp_credentials . get_cloud_storage_client ( project = project ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = from_path ) for blob in blobs : blob_path = blob . name if blob_path [ - 1 ] == \"/\" : # object is a folder and will be created if it contains any objects continue local_file_path = os . path . join ( local_path , blob_path ) os . makedirs ( os . path . dirname ( local_file_path ), exist_ok = True ) with disable_run_logger (): await cloud_storage_download_blob_to_file . fn ( bucket = self . bucket , blob = blob_path , path = local_file_path , gcp_credentials = self . gcp_credentials , )","title":"get_directory()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.list_blobs","text":"Lists all blobs in the bucket that are in a folder. Folders are not included in the output. Parameters: Name Type Description Default folder str The folder to list blobs from. '' Returns: Type Description List[google.cloud.storage.blob.Blob] A list of Blob objects. Examples: Get all blobs from a folder named \"prefect\". from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . list_blobs ( \"prefect\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def list_blobs ( self , folder : str = \"\" ) -> List [ Blob ]: \"\"\" Lists all blobs in the bucket that are in a folder. Folders are not included in the output. Args: folder: The folder to list blobs from. Returns: A list of Blob objects. Examples: Get all blobs from a folder named \"prefect\". ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.list_blobs(\"prefect\") ``` \"\"\" client = self . gcp_credentials . get_cloud_storage_client () bucket_path = self . _join_bucket_folder ( folder ) self . logger . info ( f \"Listing blobs in bucket { bucket_path } .\" ) blobs = await run_sync_in_worker_thread ( client . list_blobs , self . bucket , prefix = bucket_path ) # Ignore folders return [ blob for blob in blobs if not blob . name . endswith ( \"/\" )]","title":"list_blobs()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.put_directory","text":"Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Parameters: Name Type Description Default local_path Optional[str] Path to local directory to upload from. None to_path Optional[str] Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. None ignore_file Optional[str] Path to file containing gitignore style expressions for filepaths to ignore. None Returns: Type Description int The number of files uploaded. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def put_directory ( self , local_path : Optional [ str ] = None , to_path : Optional [ str ] = None , ignore_file : Optional [ str ] = None , ) -> int : \"\"\" Uploads a directory from a given local path to the configured GCS bucket in a given folder. Defaults to uploading the entire contents the current working directory to the block's bucket_folder. Args: local_path: Path to local directory to upload from. to_path: Path in GCS bucket to upload to. Defaults to block's configured bucket_folder. ignore_file: Path to file containing gitignore style expressions for filepaths to ignore. Returns: The number of files uploaded. \"\"\" if local_path is None : local_path = os . path . abspath ( \".\" ) else : local_path = os . path . expanduser ( local_path ) to_path = self . bucket_folder if to_path is None else self . _resolve_path ( to_path ) included_files = None if ignore_file : with open ( ignore_file , \"r\" ) as f : ignore_patterns = f . readlines () included_files = filter_files ( local_path , ignore_patterns ) uploaded_file_count = 0 for local_file_path in Path ( local_path ) . rglob ( \"*\" ): if ( included_files is not None and local_file_path . name not in included_files ): continue elif not local_file_path . is_dir (): remote_file_path = os . path . join ( to_path , local_file_path . relative_to ( local_path ) ) local_file_content = local_file_path . read_bytes () await self . write_path ( remote_file_path , content = local_file_content ) uploaded_file_count += 1 return uploaded_file_count","title":"put_directory()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.read_path","text":"Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Parameters: Name Type Description Default path str Entire path to (and including) the key. required Returns: Type Description bytes A bytes or string representation of the blob object. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def read_path ( self , path : str ) -> bytes : \"\"\" Read specified path from GCS and return contents. Provide the entire path to the key in GCS. Args: path: Entire path to (and including) the key. Returns: A bytes or string representation of the blob object. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): contents = await cloud_storage_download_blob_as_bytes . fn ( bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials ) return contents","title":"read_path()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_file_object","text":"Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader. Parameters: Name Type Description Default from_file_object BinaryIO The file-like object to upload from. required to_path str The path to upload the object to; this gets prefixed with the bucket_folder. required **upload_kwargs Additional keyword arguments to pass to Blob.upload_from_file . {} Returns: Type Description str The path that the object was uploaded to. Examples: Upload my_folder/notes.txt object to a BytesIO object. from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with open ( \"notes.txt\" , \"rb\" ) as f : gcs_bucket . upload_from_file_object ( f , \"my_folder/notes.txt\" ) Upload BufferedReader object to my_folder/notes.txt. from io import BufferedReader from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) with open ( \"notes.txt\" , \"rb\" ) as f : gcs_bucket . upload_from_file_object ( BufferedReader ( f ), \"my_folder/notes.txt\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def upload_from_file_object ( self , from_file_object : BinaryIO , to_path : str , ** upload_kwargs ) -> str : \"\"\" Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader. Args: from_file_object: The file-like object to upload from. to_path: The path to upload the object to; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file`. Returns: The path that the object was uploaded to. Examples: Upload my_folder/notes.txt object to a BytesIO object. ```python from io import BytesIO from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\") ``` Upload BufferedReader object to my_folder/notes.txt. ```python from io import BufferedReader from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") with open(\"notes.txt\", \"rb\") as f: gcs_bucket.upload_from_file_object( BufferedReader(f), \"my_folder/notes.txt\" ) ``` \"\"\" bucket = await self . get_bucket () bucket_path = self . _join_bucket_folder ( to_path ) blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from file object to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_file , from_file_object , ** upload_kwargs ) return bucket_path","title":"upload_from_file_object()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_folder","text":"Uploads files within a folder (excluding the folder itself) to the object storage service folder. Parameters: Name Type Description Default from_folder Union[str, pathlib.Path] The path to the folder to upload from. required to_folder Optional[str] The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_filename . {} Returns: Type Description str The path that the folder was uploaded to. Examples: Upload local folder my_folder to the bucket's folder my_folder. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . upload_from_folder ( \"my_folder\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def upload_from_folder ( self , from_folder : Union [ str , Path ], to_folder : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads files *within* a folder (excluding the folder itself) to the object storage service folder. Args: from_folder: The path to the folder to upload from. to_folder: The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the folder was uploaded to. Examples: Upload local folder my_folder to the bucket's folder my_folder. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_folder(\"my_folder\") ``` \"\"\" from_folder = Path ( from_folder ) bucket_folder = self . _join_bucket_folder ( to_folder or \"\" ) num_uploaded = 0 bucket = await self . get_bucket () async_coros = [] for from_path in from_folder . rglob ( \"**/*\" ): if from_path . is_dir (): continue bucket_path = str ( Path ( bucket_folder ) / from_path . relative_to ( from_folder )) self . logger . info ( f \"Uploading from { str ( from_path ) !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) blob = bucket . blob ( bucket_path ) async_coros . append ( run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) ) num_uploaded += 1 await asyncio . gather ( * async_coros ) if num_uploaded == 0 : self . logger . warning ( f \"No files were uploaded from { from_folder } .\" ) return bucket_folder","title":"upload_from_folder()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_path","text":"Uploads an object from a path to the object storage service. Parameters: Name Type Description Default from_path Union[str, pathlib.Path] The path to the file to upload from. required to_path Optional[str] The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_filename . {} Returns: Type Description str The path that the object was uploaded to. Examples: Upload notes.txt to my_folder/notes.txt. from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket . load ( \"my-bucket\" ) gcs_bucket . upload_from_path ( \"notes.txt\" , \"my_folder/notes.txt\" ) Source code in prefect_gcp/cloud_storage.py @sync_compatible async def upload_from_path ( self , from_path : Union [ str , Path ], to_path : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads an object from a path to the object storage service. Args: from_path: The path to the file to upload from. to_path: The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_filename`. Returns: The path that the object was uploaded to. Examples: Upload notes.txt to my_folder/notes.txt. ```python from prefect_gcp.cloud_storage import GcsBucket gcs_bucket = GcsBucket.load(\"my-bucket\") gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\") ``` \"\"\" if to_path is None : to_path = Path ( from_path ) . name bucket_path = self . _join_bucket_folder ( to_path ) bucket = await self . get_bucket () blob = bucket . blob ( bucket_path ) self . logger . info ( f \"Uploading from { from_path !r} to the bucket \" f \" { self . bucket !r} path { bucket_path !r} .\" ) await run_sync_in_worker_thread ( blob . upload_from_filename , filename = from_path , ** upload_kwargs ) return bucket_path","title":"upload_from_path()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.write_path","text":"Writes to an GCS bucket. Parameters: Name Type Description Default path str The key name. Each object in your bucket has a unique key (or key name). required content bytes What you are uploading to GCS Bucket. required Returns: Type Description str The path that the contents were written to. Source code in prefect_gcp/cloud_storage.py @sync_compatible async def write_path ( self , path : str , content : bytes ) -> str : \"\"\" Writes to an GCS bucket. Args: path: The key name. Each object in your bucket has a unique key (or key name). content: What you are uploading to GCS Bucket. Returns: The path that the contents were written to. \"\"\" path = self . _resolve_path ( path ) with disable_run_logger (): await cloud_storage_upload_blob_from_string . fn ( data = content , bucket = self . bucket , blob = path , gcp_credentials = self . gcp_credentials , ) return path","title":"write_path()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage-functions","text":"","title":"Functions"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_copy_blob","text":"Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Parameters: Name Type Description Default source_bucket str Source bucket name. required dest_bucket str Destination bucket name. required source_blob str Source blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required dest_blob Optional[str] Destination blob name; if not provided, defaults to source_blob. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **copy_kwargs Dict[str, Any] Additional keyword arguments to pass to Bucket.copy_blob . {} Returns: Type Description str Destination blob name. Examples: Copies blob from one bucket to another. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow () def example_cloud_storage_copy_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_copy_blob ( \"source_bucket\" , \"dest_bucket\" , \"source_blob\" , gcp_credentials ) return blob example_cloud_storage_copy_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_copy_blob ( source_bucket : str , dest_bucket : str , source_blob : str , gcp_credentials : GcpCredentials , dest_blob : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** copy_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Copies data from one Google Cloud Storage bucket to another, without downloading it locally. Args: source_bucket: Source bucket name. dest_bucket: Destination bucket name. source_blob: Source blob name. gcp_credentials: Credentials to use for authentication with GCP. dest_blob: Destination blob name; if not provided, defaults to source_blob. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **copy_kwargs: Additional keyword arguments to pass to `Bucket.copy_blob`. Returns: Destination blob name. Example: Copies blob from one bucket to another. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_copy_blob @flow() def example_cloud_storage_copy_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_copy_blob( \"source_bucket\", \"dest_bucket\", \"source_blob\", gcp_credentials ) return blob example_cloud_storage_copy_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Copying blob named %s from the %s bucket to the %s bucket\" , source_blob , source_bucket , dest_bucket , ) source_bucket_obj = await _get_bucket ( source_bucket , gcp_credentials , project = project ) dest_bucket_obj = await _get_bucket ( dest_bucket , gcp_credentials , project = project ) if dest_blob is None : dest_blob = source_blob source_blob_obj = source_bucket_obj . blob ( source_blob ) await run_sync_in_worker_thread ( source_bucket_obj . copy_blob , blob = source_blob_obj , destination_bucket = dest_bucket_obj , new_name = dest_blob , timeout = timeout , ** copy_kwargs , ) return dest_blob","title":"cloud_storage_copy_blob()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_create_bucket","text":"Creates a bucket. Parameters: Name Type Description Default bucket str Name of the bucket. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None location Optional[str] Location of the bucket. None **create_kwargs Dict[str, Any] Additional keyword arguments to pass to client.create_bucket . {} Returns: Type Description str The bucket name. Examples: Creates a bucket named \"prefect\". from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow () def example_cloud_storage_create_bucket_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) bucket = cloud_storage_create_bucket ( \"prefect\" , gcp_credentials ) example_cloud_storage_create_bucket_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_create_bucket ( bucket : str , gcp_credentials : GcpCredentials , project : Optional [ str ] = None , location : Optional [ str ] = None , ** create_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Creates a bucket. Args: bucket: Name of the bucket. gcp_credentials: Credentials to use for authentication with GCP. project: Name of the project to use; overrides the gcp_credentials project if provided. location: Location of the bucket. **create_kwargs: Additional keyword arguments to pass to `client.create_bucket`. Returns: The bucket name. Example: Creates a bucket named \"prefect\". ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_create_bucket @flow() def example_cloud_storage_create_bucket_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials) example_cloud_storage_create_bucket_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating %s bucket\" , bucket ) client = gcp_credentials . get_cloud_storage_client ( project = project ) await run_sync_in_worker_thread ( client . create_bucket , bucket , location = location , ** create_kwargs ) return bucket","title":"cloud_storage_create_bucket()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_as_bytes","text":"Downloads a blob as bytes. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_as_bytes . {} Returns: Type Description bytes A bytes or string representation of the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) contents = cloud_storage_download_blob_as_bytes ( \"bucket\" , \"blob\" , gcp_credentials ) return contents example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_as_bytes ( bucket : str , blob : str , gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> bytes : \"\"\" Downloads a blob as bytes. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_as_bytes`. Returns: A bytes or string representation of the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") contents = cloud_storage_download_blob_as_bytes( \"bucket\", \"blob\", gcp_credentials) return contents example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) contents = await run_sync_in_worker_thread ( blob_obj . download_as_bytes , timeout = timeout , ** download_kwargs ) return contents","title":"cloud_storage_download_blob_as_bytes()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_to_file","text":"Downloads a blob to a file path. Parameters: Name Type Description Default bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required path Union[str, pathlib.Path] Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required chunk_size int The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **download_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.download_to_filename . {} Returns: Type Description Union[str, pathlib.Path] The path to the blob object. Examples: Downloads blob from bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow () def example_cloud_storage_download_blob_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) path = cloud_storage_download_blob_to_file ( \"bucket\" , \"blob\" , \"file_path\" , gcp_credentials ) return path example_cloud_storage_download_blob_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_download_blob_to_file ( bucket : str , blob : str , path : Union [ str , Path ], gcp_credentials : GcpCredentials , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** download_kwargs : Dict [ str , Any ], ) -> Union [ str , Path ]: \"\"\" Downloads a blob to a file path. Args: bucket: Name of the bucket. blob: Name of the Cloud Storage blob. path: Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name. gcp_credentials: Credentials to use for authentication with GCP. chunk_size (int, optional): The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **download_kwargs: Additional keyword arguments to pass to `Blob.download_to_filename`. Returns: The path to the blob object. Example: Downloads blob from bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file @flow() def example_cloud_storage_download_blob_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") path = cloud_storage_download_blob_to_file( \"bucket\", \"blob\", \"file_path\", gcp_credentials) return path example_cloud_storage_download_blob_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Downloading blob named %s from the %s bucket to %s \" , blob , bucket , path ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if os . path . isdir ( path ): if isinstance ( path , Path ): path = path . joinpath ( blob ) # keep as Path if Path is passed else : path = os . path . join ( path , blob ) # keep as str if a str is passed await run_sync_in_worker_thread ( blob_obj . download_to_filename , path , timeout = timeout , ** download_kwargs ) return path","title":"cloud_storage_download_blob_to_file()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_file","text":"Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Parameters: Name Type Description Default file Union[str, pathlib.Path, _io.BytesIO] Path to data or file like object to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_file or Blob.upload_from_filename . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow () def example_cloud_storage_upload_blob_from_file_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_file ( \"/path/somewhere\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_file_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_file ( file : Union [ str , Path , BytesIO ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage. Args: file: Path to data or file like object to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_file` or `Blob.upload_from_filename`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file @flow() def example_cloud_storage_upload_blob_from_file_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_file( \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_file_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) if isinstance ( file , BytesIO ): await run_sync_in_worker_thread ( blob_obj . upload_from_file , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) else : await run_sync_in_worker_thread ( blob_obj . upload_from_filename , file , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob","title":"cloud_storage_upload_blob_from_file()"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_string","text":"Uploads a blob from a string or bytes representation of data. Parameters: Name Type Description Default data Union[str, bytes] String or bytes representation of data to upload. required bucket str Name of the bucket. required blob str Name of the Cloud Storage blob. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required content_type Optional[str] Type of content being uploaded. None chunk_size Optional[int] The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. None encryption_key Optional[str] An encryption key. None timeout Union[float, Tuple[float, float]] The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None **upload_kwargs Dict[str, Any] Additional keyword arguments to pass to Blob.upload_from_string . {} Returns: Type Description str The blob name. Examples: Uploads blob to bucket. from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow () def example_cloud_storage_upload_blob_from_string_flow (): gcp_credentials = GcpCredentials ( service_account_file = \"/path/to/service/account/keyfile.json\" ) blob = cloud_storage_upload_blob_from_string ( \"data\" , \"bucket\" , \"blob\" , gcp_credentials ) return blob example_cloud_storage_upload_blob_from_string_flow () Source code in prefect_gcp/cloud_storage.py @task async def cloud_storage_upload_blob_from_string ( data : Union [ str , bytes ], bucket : str , blob : str , gcp_credentials : GcpCredentials , content_type : Optional [ str ] = None , chunk_size : Optional [ int ] = None , encryption_key : Optional [ str ] = None , timeout : Union [ float , Tuple [ float , float ]] = 60 , project : Optional [ str ] = None , ** upload_kwargs : Dict [ str , Any ], ) -> str : \"\"\" Uploads a blob from a string or bytes representation of data. Args: data: String or bytes representation of data to upload. bucket: Name of the bucket. blob: Name of the Cloud Storage blob. gcp_credentials: Credentials to use for authentication with GCP. content_type: Type of content being uploaded. chunk_size: The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification. encryption_key: An encryption key. timeout: The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout). project: Name of the project to use; overrides the gcp_credentials project if provided. **upload_kwargs: Additional keyword arguments to pass to `Blob.upload_from_string`. Returns: The blob name. Example: Uploads blob to bucket. ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string @flow() def example_cloud_storage_upload_blob_from_string_flow(): gcp_credentials = GcpCredentials( service_account_file=\"/path/to/service/account/keyfile.json\") blob = cloud_storage_upload_blob_from_string( \"data\", \"bucket\", \"blob\", gcp_credentials) return blob example_cloud_storage_upload_blob_from_string_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Uploading blob named %s to the %s bucket\" , blob , bucket ) bucket_obj = await _get_bucket ( bucket , gcp_credentials , project = project ) blob_obj = bucket_obj . blob ( blob , chunk_size = chunk_size , encryption_key = encryption_key ) await run_sync_in_worker_thread ( blob_obj . upload_from_string , data , content_type = content_type , timeout = timeout , ** upload_kwargs , ) return blob","title":"cloud_storage_upload_blob_from_string()"},{"location":"credentials/","text":"prefect_gcp.credentials Module handling GCP credentials. Classes ClientType ( Enum ) An enumeration. Source code in prefect_gcp/credentials.py class ClientType ( Enum ): CLOUD_STORAGE = \"cloud_storage\" BIGQUERY = \"bigquery\" SECRET_MANAGER = \"secret_manager\" AIPLATFORM = \"job_service\" # vertex ai GcpCredentials ( CredentialsBlock ) pydantic-model Block used to manage authentication with GCP. GCP authentication is handled via the google.oauth2 module or through the CLI. Specify either one of service account_file or service_account_info ; if both are not specified, the client will try to detect the service account info stored in the env from the command, gcloud auth application-default login . Refer to the Authentication docs for more info about the possible credential configurations. Attributes: Name Type Description service_account_file Optional[pathlib.Path] Path to the service account JSON keyfile. service_account_info Optional[prefect.blocks.fields.SecretDict] The contents of the keyfile as a dict. Examples: Load GCP credentials stored in a GCP Credentials Block: from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/credentials.py class GcpCredentials ( CredentialsBlock ): \"\"\" Block used to manage authentication with GCP. GCP authentication is handled via the `google.oauth2` module or through the CLI. Specify either one of service `account_file` or `service_account_info`; if both are not specified, the client will try to detect the service account info stored in the env from the command, `gcloud auth application-default login`. Refer to the [Authentication docs](https://cloud.google.com/docs/authentication/production) for more info about the possible credential configurations. Attributes: service_account_file: Path to the service account JSON keyfile. service_account_info: The contents of the keyfile as a dict. Example: Load GCP credentials stored in a `GCP Credentials` Block: ```python from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCP Credentials\" service_account_file : Optional [ Path ] = Field ( default = None , description = \"Path to the service account JSON keyfile.\" ) service_account_info : Optional [ SecretDict ] = Field ( default = None , description = \"The contents of the keyfile as a dict.\" ) project : Optional [ str ] = Field ( default = None , description = \"The GCP project to use for the client.\" ) _service_account_email : Optional [ str ] = None @root_validator def _provide_one_service_account_source ( cls , values ): \"\"\" Ensure that only a service account file or service account info ias provided. \"\"\" both_service_account = ( values . get ( \"service_account_info\" ) is not None and values . get ( \"service_account_file\" ) is not None ) if both_service_account : raise ValueError ( \"Only one of service_account_info or service_account_file \" \"can be specified at once\" ) return values @validator ( \"service_account_file\" ) def _check_service_account_file ( cls , file ): \"\"\"Get full path of provided file and make sure that it exists.\"\"\" if not file : return file service_account_file = Path ( file ) . expanduser () if not service_account_file . exists (): raise ValueError ( \"The provided path to the service account is invalid\" ) return service_account_file @validator ( \"service_account_info\" , pre = True ) def _convert_json_string_json_service_account_info ( cls , value ): \"\"\" Converts service account info provided as a json formatted string to a dictionary \"\"\" if isinstance ( value , str ): try : service_account_info = json . loads ( value ) return service_account_info except Exception : raise ValueError ( \"Unable to decode service_account_info\" ) else : return value def block_initialization ( self ): credentials = self . get_credentials_from_service_account () if self . project is None : if self . service_account_info or self . service_account_file : credentials_project = credentials . project_id else : # google.auth.default using gcloud auth application-default login credentials_project = credentials . quota_project_id self . project = credentials_project if hasattr ( credentials , \"service_account_email\" ): self . _service_account_email = credentials . service_account_email def get_credentials_from_service_account ( self ) -> Credentials : \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info . get_secret_value (), scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : credentials , _ = google . auth . default () return credentials @sync_compatible async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token def get_client ( self , client_type : Union [ str , ClientType ], ** get_client_kwargs : Dict [ str , Any ], ) -> Any : \"\"\" Helper method to dynamically get a client type. Args: client_type: The name of the client to get. **get_client_kwargs: Additional keyword arguments to pass to the `get_*_client` method. Returns: An authenticated client. Raises: ValueError: if the client is not supported. \"\"\" if isinstance ( client_type , str ): client_type = ClientType ( client_type ) client_type = client_type . value get_client_method = getattr ( self , f \"get_ { client_type } _client\" ) return get_client_method ( ** get_client_kwargs ) @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client @_raise_help_msg ( \"aiplatform\" ) def get_job_service_client ( self , client_options : Dict [ str , Any ] = None ) -> \"JobServiceClient\" : \"\"\" Gets an authenticated Job Service client for Vertex AI. Returns: An authenticated Job Service client. Examples: Gets a GCP Job Service client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_job_service_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_job_service_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () job_service_client = JobServiceClient ( credentials = credentials , client_options = client_options ) return job_service_client Attributes project : str pydantic-field The GCP project to use for the client. service_account_file : Path pydantic-field Path to the service account JSON keyfile. service_account_info : SecretDict pydantic-field The contents of the keyfile as a dict. Methods get_access_token async Source code in prefect_gcp/credentials.py @sync_compatible async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token get_bigquery_client Gets an authenticated BigQuery client. Parameters: Name Type Description Default project str Name of the project to use; overrides the base class's project if provided. None location str Location to use. None Returns: Type Description BigQueryClient An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_bigquery_client () example_get_client_flow () Gets a GCP BigQuery client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_bigquery_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client get_client Helper method to dynamically get a client type. Parameters: Name Type Description Default client_type Union[str, prefect_gcp.credentials.ClientType] The name of the client to get. required **get_client_kwargs Dict[str, Any] Additional keyword arguments to pass to the get_*_client method. {} Returns: Type Description Any An authenticated client. Exceptions: Type Description ValueError if the client is not supported. Source code in prefect_gcp/credentials.py def get_client ( self , client_type : Union [ str , ClientType ], ** get_client_kwargs : Dict [ str , Any ], ) -> Any : \"\"\" Helper method to dynamically get a client type. Args: client_type: The name of the client to get. **get_client_kwargs: Additional keyword arguments to pass to the `get_*_client` method. Returns: An authenticated client. Raises: ValueError: if the client is not supported. \"\"\" if isinstance ( client_type , str ): client_type = ClientType ( client_type ) client_type = client_type . value get_client_method = getattr ( self , f \"get_ { client_type } _client\" ) return get_client_method ( ** get_client_kwargs ) get_cloud_storage_client Gets an authenticated Cloud Storage client. Parameters: Name Type Description Default project Optional[str] Name of the project to use; overrides the base class's project if provided. None Returns: Type Description StorageClient An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_cloud_storage_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_cloud_storage_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client get_credentials_from_service_account Helper method to serialize credentials by using either service_account_file or service_account_info. Source code in prefect_gcp/credentials.py def get_credentials_from_service_account ( self ) -> Credentials : \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info . get_secret_value (), scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : credentials , _ = google . auth . default () return credentials get_job_service_client Gets an authenticated Job Service client for Vertex AI. Returns: Type Description JobServiceClient An authenticated Job Service client. Examples: Gets a GCP Job Service client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_job_service_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_job_service_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"aiplatform\" ) def get_job_service_client ( self , client_options : Dict [ str , Any ] = None ) -> \"JobServiceClient\" : \"\"\" Gets an authenticated Job Service client for Vertex AI. Returns: An authenticated Job Service client. Examples: Gets a GCP Job Service client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_job_service_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_job_service_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () job_service_client = JobServiceClient ( credentials = credentials , client_options = client_options ) return job_service_client get_secret_manager_client Gets an authenticated Secret Manager Service client. Returns: Type Description SecretManagerServiceClient An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_secret_manager_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_secret_manager_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client","title":"Credentials"},{"location":"credentials/#prefect_gcp.credentials","text":"Module handling GCP credentials.","title":"credentials"},{"location":"credentials/#prefect_gcp.credentials-classes","text":"","title":"Classes"},{"location":"credentials/#prefect_gcp.credentials.ClientType","text":"An enumeration. Source code in prefect_gcp/credentials.py class ClientType ( Enum ): CLOUD_STORAGE = \"cloud_storage\" BIGQUERY = \"bigquery\" SECRET_MANAGER = \"secret_manager\" AIPLATFORM = \"job_service\" # vertex ai","title":"ClientType"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials","text":"Block used to manage authentication with GCP. GCP authentication is handled via the google.oauth2 module or through the CLI. Specify either one of service account_file or service_account_info ; if both are not specified, the client will try to detect the service account info stored in the env from the command, gcloud auth application-default login . Refer to the Authentication docs for more info about the possible credential configurations. Attributes: Name Type Description service_account_file Optional[pathlib.Path] Path to the service account JSON keyfile. service_account_info Optional[prefect.blocks.fields.SecretDict] The contents of the keyfile as a dict. Examples: Load GCP credentials stored in a GCP Credentials Block: from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials . load ( \"BLOCK_NAME\" ) Source code in prefect_gcp/credentials.py class GcpCredentials ( CredentialsBlock ): \"\"\" Block used to manage authentication with GCP. GCP authentication is handled via the `google.oauth2` module or through the CLI. Specify either one of service `account_file` or `service_account_info`; if both are not specified, the client will try to detect the service account info stored in the env from the command, `gcloud auth application-default login`. Refer to the [Authentication docs](https://cloud.google.com/docs/authentication/production) for more info about the possible credential configurations. Attributes: service_account_file: Path to the service account JSON keyfile. service_account_info: The contents of the keyfile as a dict. Example: Load GCP credentials stored in a `GCP Credentials` Block: ```python from prefect_gcp import GcpCredentials gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\") ``` \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa _block_type_name = \"GCP Credentials\" service_account_file : Optional [ Path ] = Field ( default = None , description = \"Path to the service account JSON keyfile.\" ) service_account_info : Optional [ SecretDict ] = Field ( default = None , description = \"The contents of the keyfile as a dict.\" ) project : Optional [ str ] = Field ( default = None , description = \"The GCP project to use for the client.\" ) _service_account_email : Optional [ str ] = None @root_validator def _provide_one_service_account_source ( cls , values ): \"\"\" Ensure that only a service account file or service account info ias provided. \"\"\" both_service_account = ( values . get ( \"service_account_info\" ) is not None and values . get ( \"service_account_file\" ) is not None ) if both_service_account : raise ValueError ( \"Only one of service_account_info or service_account_file \" \"can be specified at once\" ) return values @validator ( \"service_account_file\" ) def _check_service_account_file ( cls , file ): \"\"\"Get full path of provided file and make sure that it exists.\"\"\" if not file : return file service_account_file = Path ( file ) . expanduser () if not service_account_file . exists (): raise ValueError ( \"The provided path to the service account is invalid\" ) return service_account_file @validator ( \"service_account_info\" , pre = True ) def _convert_json_string_json_service_account_info ( cls , value ): \"\"\" Converts service account info provided as a json formatted string to a dictionary \"\"\" if isinstance ( value , str ): try : service_account_info = json . loads ( value ) return service_account_info except Exception : raise ValueError ( \"Unable to decode service_account_info\" ) else : return value def block_initialization ( self ): credentials = self . get_credentials_from_service_account () if self . project is None : if self . service_account_info or self . service_account_file : credentials_project = credentials . project_id else : # google.auth.default using gcloud auth application-default login credentials_project = credentials . quota_project_id self . project = credentials_project if hasattr ( credentials , \"service_account_email\" ): self . _service_account_email = credentials . service_account_email def get_credentials_from_service_account ( self ) -> Credentials : \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info . get_secret_value (), scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : credentials , _ = google . auth . default () return credentials @sync_compatible async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token def get_client ( self , client_type : Union [ str , ClientType ], ** get_client_kwargs : Dict [ str , Any ], ) -> Any : \"\"\" Helper method to dynamically get a client type. Args: client_type: The name of the client to get. **get_client_kwargs: Additional keyword arguments to pass to the `get_*_client` method. Returns: An authenticated client. Raises: ValueError: if the client is not supported. \"\"\" if isinstance ( client_type , str ): client_type = ClientType ( client_type ) client_type = client_type . value get_client_method = getattr ( self , f \"get_ { client_type } _client\" ) return get_client_method ( ** get_client_kwargs ) @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client @_raise_help_msg ( \"aiplatform\" ) def get_job_service_client ( self , client_options : Dict [ str , Any ] = None ) -> \"JobServiceClient\" : \"\"\" Gets an authenticated Job Service client for Vertex AI. Returns: An authenticated Job Service client. Examples: Gets a GCP Job Service client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_job_service_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_job_service_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () job_service_client = JobServiceClient ( credentials = credentials , client_options = client_options ) return job_service_client","title":"GcpCredentials"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials-attributes","text":"","title":"Attributes"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.project","text":"The GCP project to use for the client.","title":"project"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.service_account_file","text":"Path to the service account JSON keyfile.","title":"service_account_file"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.service_account_info","text":"The contents of the keyfile as a dict.","title":"service_account_info"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials-methods","text":"","title":"Methods"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_access_token","text":"Source code in prefect_gcp/credentials.py @sync_compatible async def get_access_token ( self ): \"\"\" See: https://stackoverflow.com/a/69107745 Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/ \"\"\" # noqa request = google . auth . transport . requests . Request () credentials = self . get_credentials_from_service_account () await run_sync_in_worker_thread ( credentials . refresh , request ) return credentials . token","title":"get_access_token()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_bigquery_client","text":"Gets an authenticated BigQuery client. Parameters: Name Type Description Default project str Name of the project to use; overrides the base class's project if provided. None location str Location to use. None Returns: Type Description BigQueryClient An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_bigquery_client () example_get_client_flow () Gets a GCP BigQuery client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_bigquery_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"bigquery\" ) def get_bigquery_client ( self , project : str = None , location : str = None ) -> \"BigQueryClient\" : \"\"\" Gets an authenticated BigQuery client. Args: project: Name of the project to use; overrides the base class's project if provided. location: Location to use. Returns: An authenticated BigQuery client. Examples: Gets a GCP BigQuery client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_bigquery_client() example_get_client_flow() ``` Gets a GCP BigQuery client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_bigquery_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project big_query_client = BigQueryClient ( credentials = credentials , project = project , location = location ) return big_query_client","title":"get_bigquery_client()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_client","text":"Helper method to dynamically get a client type. Parameters: Name Type Description Default client_type Union[str, prefect_gcp.credentials.ClientType] The name of the client to get. required **get_client_kwargs Dict[str, Any] Additional keyword arguments to pass to the get_*_client method. {} Returns: Type Description Any An authenticated client. Exceptions: Type Description ValueError if the client is not supported. Source code in prefect_gcp/credentials.py def get_client ( self , client_type : Union [ str , ClientType ], ** get_client_kwargs : Dict [ str , Any ], ) -> Any : \"\"\" Helper method to dynamically get a client type. Args: client_type: The name of the client to get. **get_client_kwargs: Additional keyword arguments to pass to the `get_*_client` method. Returns: An authenticated client. Raises: ValueError: if the client is not supported. \"\"\" if isinstance ( client_type , str ): client_type = ClientType ( client_type ) client_type = client_type . value get_client_method = getattr ( self , f \"get_ { client_type } _client\" ) return get_client_method ( ** get_client_kwargs )","title":"get_client()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_cloud_storage_client","text":"Gets an authenticated Cloud Storage client. Parameters: Name Type Description Default project Optional[str] Name of the project to use; overrides the base class's project if provided. None Returns: Type Description StorageClient An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_cloud_storage_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_cloud_storage_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"cloud_storage\" ) def get_cloud_storage_client ( self , project : Optional [ str ] = None ) -> \"StorageClient\" : \"\"\" Gets an authenticated Cloud Storage client. Args: project: Name of the project to use; overrides the base class's project if provided. Returns: An authenticated Cloud Storage client. Examples: Gets a GCP Cloud Storage client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_cloud_storage_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_cloud_storage_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # override class project if method project is provided project = project or self . project storage_client = StorageClient ( credentials = credentials , project = project ) return storage_client","title":"get_cloud_storage_client()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_credentials_from_service_account","text":"Helper method to serialize credentials by using either service_account_file or service_account_info. Source code in prefect_gcp/credentials.py def get_credentials_from_service_account ( self ) -> Credentials : \"\"\" Helper method to serialize credentials by using either service_account_file or service_account_info. \"\"\" if self . service_account_info : credentials = Credentials . from_service_account_info ( self . service_account_info . get_secret_value (), scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) elif self . service_account_file : credentials = Credentials . from_service_account_file ( self . service_account_file , scopes = [ \"https://www.googleapis.com/auth/cloud-platform\" ], ) else : credentials , _ = google . auth . default () return credentials","title":"get_credentials_from_service_account()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_job_service_client","text":"Gets an authenticated Job Service client for Vertex AI. Returns: Type Description JobServiceClient An authenticated Job Service client. Examples: Gets a GCP Job Service client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_job_service_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_job_service_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"aiplatform\" ) def get_job_service_client ( self , client_options : Dict [ str , Any ] = None ) -> \"JobServiceClient\" : \"\"\" Gets an authenticated Job Service client for Vertex AI. Returns: An authenticated Job Service client. Examples: Gets a GCP Job Service client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_job_service_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_job_service_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () job_service_client = JobServiceClient ( credentials = credentials , client_options = client_options ) return job_service_client","title":"get_job_service_client()"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_secret_manager_client","text":"Gets an authenticated Secret Manager Service client. Returns: Type Description SecretManagerServiceClient An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials ( service_account_file = service_account_file ) . get_secret_manager_client () example_get_client_flow () Gets a GCP Cloud Storage client from a dictionary. from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow () def example_get_client_flow (): service_account_info = { \"type\" : \"service_account\" , \"project_id\" : \"project_id\" , \"private_key_id\" : \"private_key_id\" , \"private_key\" : \"private_key\" , \"client_email\" : \"client_email\" , \"client_id\" : \"client_id\" , \"auth_uri\" : \"auth_uri\" , \"token_uri\" : \"token_uri\" , \"auth_provider_x509_cert_url\" : \"auth_provider_x509_cert_url\" , \"client_x509_cert_url\" : \"client_x509_cert_url\" } client = GcpCredentials ( service_account_info = service_account_info ) . get_secret_manager_client () example_get_client_flow () Source code in prefect_gcp/credentials.py @_raise_help_msg ( \"secret_manager\" ) def get_secret_manager_client ( self ) -> \"SecretManagerServiceClient\" : \"\"\" Gets an authenticated Secret Manager Service client. Returns: An authenticated Secret Manager Service client. Examples: Gets a GCP Secret Manager client from a path. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_file = \"~/.secrets/prefect-service-account.json\" client = GcpCredentials( service_account_file=service_account_file ).get_secret_manager_client() example_get_client_flow() ``` Gets a GCP Cloud Storage client from a dictionary. ```python from prefect import flow from prefect_gcp.credentials import GcpCredentials @flow() def example_get_client_flow(): service_account_info = { \"type\": \"service_account\", \"project_id\": \"project_id\", \"private_key_id\": \"private_key_id\", \"private_key\": \"private_key\", \"client_email\": \"client_email\", \"client_id\": \"client_id\", \"auth_uri\": \"auth_uri\", \"token_uri\": \"token_uri\", \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\", \"client_x509_cert_url\": \"client_x509_cert_url\" } client = GcpCredentials( service_account_info=service_account_info ).get_secret_manager_client() example_get_client_flow() ``` \"\"\" credentials = self . get_credentials_from_service_account () # doesn't accept project; must pass in project in tasks secret_manager_client = SecretManagerServiceClient ( credentials = credentials ) return secret_manager_client","title":"get_secret_manager_client()"},{"location":"secret_manager/","text":"prefect_gcp.secret_manager Classes GcpSecret ( SecretBlock ) pydantic-model Manages a secret in Google Cloud Platform's Secret Manager. Attributes: Name Type Description gcp_credentials GcpCredentials Credentials to use for authentication with GCP. secret_name str Name of the secret to manage. secret_version str Version number of the secret to use, or \"latest\". Source code in prefect_gcp/secret_manager.py class GcpSecret ( SecretBlock ): \"\"\" Manages a secret in Google Cloud Platform's Secret Manager. Attributes: gcp_credentials: Credentials to use for authentication with GCP. secret_name: Name of the secret to manage. secret_version: Version number of the secret to use, or \"latest\". \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa gcp_credentials : GcpCredentials secret_name : str = Field ( default =... , description = \"Name of the secret to manage.\" ) secret_version : str = Field ( default = \"latest\" , description = \"Version number of the secret to use.\" ) @sync_compatible async def read_secret ( self ) -> bytes : \"\"\" Reads the secret data from the secret storage service. Returns: The secret data as bytes. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } /versions/ { self . secret_version } \" # noqa request = AccessSecretVersionRequest ( name = name ) self . logger . debug ( f \"Preparing to read secret data from { name !r} .\" ) response = await run_sync_in_worker_thread ( client . access_secret_version , request = request ) secret = response . payload . data . decode ( \"UTF-8\" ) self . logger . info ( f \"The secret { name !r} data was successfully read.\" ) return secret @sync_compatible async def write_secret ( self , secret_data : bytes ) -> str : \"\"\" Writes the secret data to the secret storage service; if it doesn't exist it will be created. Args: secret_data: The secret to write. Returns: The path that the secret was written to. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project parent = f \"projects/ { project } /secrets/ { self . secret_name } \" payload = SecretPayload ( data = secret_data ) add_request = AddSecretVersionRequest ( parent = parent , payload = payload ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} .\" ) try : response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) except NotFound : self . logger . info ( f \"The secret { parent !r} does not exist yet, creating it now.\" ) create_parent = f \"projects/ { project } \" secret_id = self . secret_name secret = Secret ( replication = Replication ( automatic = Replication . Automatic ())) create_request = CreateSecretRequest ( parent = create_parent , secret_id = secret_id , secret = secret ) await run_sync_in_worker_thread ( client . create_secret , request = create_request ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} again.\" ) response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) self . logger . info ( f \"The secret data was written successfully to { parent !r} .\" ) return response . name @sync_compatible async def delete_secret ( self ) -> str : \"\"\" Deletes the secret from the secret storage service. Returns: The path that the secret was deleted from. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } \" request = DeleteSecretRequest ( name = name ) self . logger . debug ( f \"Preparing to delete the secret { name !r} .\" ) await run_sync_in_worker_thread ( client . delete_secret , request = request ) self . logger . info ( f \"The secret { name !r} was successfully deleted.\" ) return name Attributes secret_name : str pydantic-field required Name of the secret to manage. secret_version : str pydantic-field Version number of the secret to use. Methods delete_secret async Deletes the secret from the secret storage service. Returns: Type Description str The path that the secret was deleted from. Source code in prefect_gcp/secret_manager.py @sync_compatible async def delete_secret ( self ) -> str : \"\"\" Deletes the secret from the secret storage service. Returns: The path that the secret was deleted from. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } \" request = DeleteSecretRequest ( name = name ) self . logger . debug ( f \"Preparing to delete the secret { name !r} .\" ) await run_sync_in_worker_thread ( client . delete_secret , request = request ) self . logger . info ( f \"The secret { name !r} was successfully deleted.\" ) return name read_secret async Reads the secret data from the secret storage service. Returns: Type Description bytes The secret data as bytes. Source code in prefect_gcp/secret_manager.py @sync_compatible async def read_secret ( self ) -> bytes : \"\"\" Reads the secret data from the secret storage service. Returns: The secret data as bytes. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } /versions/ { self . secret_version } \" # noqa request = AccessSecretVersionRequest ( name = name ) self . logger . debug ( f \"Preparing to read secret data from { name !r} .\" ) response = await run_sync_in_worker_thread ( client . access_secret_version , request = request ) secret = response . payload . data . decode ( \"UTF-8\" ) self . logger . info ( f \"The secret { name !r} data was successfully read.\" ) return secret write_secret async Writes the secret data to the secret storage service; if it doesn't exist it will be created. Parameters: Name Type Description Default secret_data bytes The secret to write. required Returns: Type Description str The path that the secret was written to. Source code in prefect_gcp/secret_manager.py @sync_compatible async def write_secret ( self , secret_data : bytes ) -> str : \"\"\" Writes the secret data to the secret storage service; if it doesn't exist it will be created. Args: secret_data: The secret to write. Returns: The path that the secret was written to. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project parent = f \"projects/ { project } /secrets/ { self . secret_name } \" payload = SecretPayload ( data = secret_data ) add_request = AddSecretVersionRequest ( parent = parent , payload = payload ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} .\" ) try : response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) except NotFound : self . logger . info ( f \"The secret { parent !r} does not exist yet, creating it now.\" ) create_parent = f \"projects/ { project } \" secret_id = self . secret_name secret = Secret ( replication = Replication ( automatic = Replication . Automatic ())) create_request = CreateSecretRequest ( parent = create_parent , secret_id = secret_id , secret = secret ) await run_sync_in_worker_thread ( client . create_secret , request = create_request ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} again.\" ) response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) self . logger . info ( f \"The secret data was written successfully to { parent !r} .\" ) return response . name Functions create_secret async Creates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the created secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow () def example_cloud_storage_create_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = create_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_create_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def create_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Creates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the created secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow() def example_cloud_storage_create_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = create_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_create_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } \" secret_settings = { \"replication\" : { \"automatic\" : {}}} partial_create = partial ( client . create_secret , parent = parent , secret_id = secret_name , secret = secret_settings , timeout = timeout , ) response = await to_thread . run_sync ( partial_create ) return response . name delete_secret async Deletes the specified secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to delete. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow () def example_cloud_storage_delete_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = delete_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_delete_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes the specified secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to delete. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow() def example_cloud_storage_delete_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = delete_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_delete_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Deleting %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /\" partial_delete = partial ( client . delete_secret , name = name , timeout = timeout ) await to_thread . run_sync ( partial_delete ) return name delete_secret_version async Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required version_id int Version number of the secret to use; \"latest\" can NOT be used. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret version. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow () def example_cloud_storage_delete_secret_version_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = delete_secret_version ( \"secret_name\" , 1 , gcp_credentials ) return secret_value example_cloud_storage_delete_secret_version_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret_version ( secret_name : str , version_id : int , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. version_id: Version number of the secret to use; \"latest\" can NOT be used. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret version. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow() def example_cloud_storage_delete_secret_version_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials) return secret_value example_cloud_storage_delete_secret_version_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project if version_id == \"latest\" : raise ValueError ( \"The version_id cannot be 'latest'\" ) name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_destroy = partial ( client . destroy_secret_version , name = name , timeout = timeout ) await to_thread . run_sync ( partial_destroy ) return name read_secret async Reads the value of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str Contents of the specified secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow () def example_cloud_storage_read_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = read_secret ( \"secret_name\" , gcp_credentials , version_id = 1 ) return secret_value example_cloud_storage_read_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def read_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , version_id : Union [ str , int ] = \"latest\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Reads the value of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: Contents of the specified secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow() def example_cloud_storage_read_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1) return secret_value example_cloud_storage_read_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_access = partial ( client . access_secret_version , name = name , timeout = timeout ) response = await to_thread . run_sync ( partial_access ) secret = response . payload . data . decode ( \"UTF-8\" ) return secret update_secret async Updates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required secret_value Union[str, bytes] Desired value of the secret. Can be either str or bytes . required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the updated secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow () def example_cloud_storage_update_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = update_secret ( \"secret_name\" , \"secret_value\" , gcp_credentials ) return secret_path example_cloud_storage_update_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def update_secret ( secret_name : str , secret_value : Union [ str , bytes ], gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Updates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. secret_value: Desired value of the secret. Can be either `str` or `bytes`. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the updated secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow() def example_cloud_storage_update_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials) return secret_path example_cloud_storage_update_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Updating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } /secrets/ { secret_name } \" if isinstance ( secret_value , str ): secret_value = secret_value . encode ( \"UTF-8\" ) partial_add = partial ( client . add_secret_version , parent = parent , payload = { \"data\" : secret_value }, timeout = timeout , ) response = await to_thread . run_sync ( partial_add ) return response . name","title":"Secret Manager"},{"location":"secret_manager/#prefect_gcp.secret_manager","text":"","title":"secret_manager"},{"location":"secret_manager/#prefect_gcp.secret_manager-classes","text":"","title":"Classes"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret","text":"Manages a secret in Google Cloud Platform's Secret Manager. Attributes: Name Type Description gcp_credentials GcpCredentials Credentials to use for authentication with GCP. secret_name str Name of the secret to manage. secret_version str Version number of the secret to use, or \"latest\". Source code in prefect_gcp/secret_manager.py class GcpSecret ( SecretBlock ): \"\"\" Manages a secret in Google Cloud Platform's Secret Manager. Attributes: gcp_credentials: Credentials to use for authentication with GCP. secret_name: Name of the secret to manage. secret_version: Version number of the secret to use, or \"latest\". \"\"\" _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\" # noqa gcp_credentials : GcpCredentials secret_name : str = Field ( default =... , description = \"Name of the secret to manage.\" ) secret_version : str = Field ( default = \"latest\" , description = \"Version number of the secret to use.\" ) @sync_compatible async def read_secret ( self ) -> bytes : \"\"\" Reads the secret data from the secret storage service. Returns: The secret data as bytes. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } /versions/ { self . secret_version } \" # noqa request = AccessSecretVersionRequest ( name = name ) self . logger . debug ( f \"Preparing to read secret data from { name !r} .\" ) response = await run_sync_in_worker_thread ( client . access_secret_version , request = request ) secret = response . payload . data . decode ( \"UTF-8\" ) self . logger . info ( f \"The secret { name !r} data was successfully read.\" ) return secret @sync_compatible async def write_secret ( self , secret_data : bytes ) -> str : \"\"\" Writes the secret data to the secret storage service; if it doesn't exist it will be created. Args: secret_data: The secret to write. Returns: The path that the secret was written to. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project parent = f \"projects/ { project } /secrets/ { self . secret_name } \" payload = SecretPayload ( data = secret_data ) add_request = AddSecretVersionRequest ( parent = parent , payload = payload ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} .\" ) try : response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) except NotFound : self . logger . info ( f \"The secret { parent !r} does not exist yet, creating it now.\" ) create_parent = f \"projects/ { project } \" secret_id = self . secret_name secret = Secret ( replication = Replication ( automatic = Replication . Automatic ())) create_request = CreateSecretRequest ( parent = create_parent , secret_id = secret_id , secret = secret ) await run_sync_in_worker_thread ( client . create_secret , request = create_request ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} again.\" ) response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) self . logger . info ( f \"The secret data was written successfully to { parent !r} .\" ) return response . name @sync_compatible async def delete_secret ( self ) -> str : \"\"\" Deletes the secret from the secret storage service. Returns: The path that the secret was deleted from. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } \" request = DeleteSecretRequest ( name = name ) self . logger . debug ( f \"Preparing to delete the secret { name !r} .\" ) await run_sync_in_worker_thread ( client . delete_secret , request = request ) self . logger . info ( f \"The secret { name !r} was successfully deleted.\" ) return name","title":"GcpSecret"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret-attributes","text":"","title":"Attributes"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.secret_name","text":"Name of the secret to manage.","title":"secret_name"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.secret_version","text":"Version number of the secret to use.","title":"secret_version"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret-methods","text":"","title":"Methods"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.delete_secret","text":"Deletes the secret from the secret storage service. Returns: Type Description str The path that the secret was deleted from. Source code in prefect_gcp/secret_manager.py @sync_compatible async def delete_secret ( self ) -> str : \"\"\" Deletes the secret from the secret storage service. Returns: The path that the secret was deleted from. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } \" request = DeleteSecretRequest ( name = name ) self . logger . debug ( f \"Preparing to delete the secret { name !r} .\" ) await run_sync_in_worker_thread ( client . delete_secret , request = request ) self . logger . info ( f \"The secret { name !r} was successfully deleted.\" ) return name","title":"delete_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.read_secret","text":"Reads the secret data from the secret storage service. Returns: Type Description bytes The secret data as bytes. Source code in prefect_gcp/secret_manager.py @sync_compatible async def read_secret ( self ) -> bytes : \"\"\" Reads the secret data from the secret storage service. Returns: The secret data as bytes. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project name = f \"projects/ { project } /secrets/ { self . secret_name } /versions/ { self . secret_version } \" # noqa request = AccessSecretVersionRequest ( name = name ) self . logger . debug ( f \"Preparing to read secret data from { name !r} .\" ) response = await run_sync_in_worker_thread ( client . access_secret_version , request = request ) secret = response . payload . data . decode ( \"UTF-8\" ) self . logger . info ( f \"The secret { name !r} data was successfully read.\" ) return secret","title":"read_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.write_secret","text":"Writes the secret data to the secret storage service; if it doesn't exist it will be created. Parameters: Name Type Description Default secret_data bytes The secret to write. required Returns: Type Description str The path that the secret was written to. Source code in prefect_gcp/secret_manager.py @sync_compatible async def write_secret ( self , secret_data : bytes ) -> str : \"\"\" Writes the secret data to the secret storage service; if it doesn't exist it will be created. Args: secret_data: The secret to write. Returns: The path that the secret was written to. \"\"\" client = self . gcp_credentials . get_secret_manager_client () project = self . gcp_credentials . project parent = f \"projects/ { project } /secrets/ { self . secret_name } \" payload = SecretPayload ( data = secret_data ) add_request = AddSecretVersionRequest ( parent = parent , payload = payload ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} .\" ) try : response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) except NotFound : self . logger . info ( f \"The secret { parent !r} does not exist yet, creating it now.\" ) create_parent = f \"projects/ { project } \" secret_id = self . secret_name secret = Secret ( replication = Replication ( automatic = Replication . Automatic ())) create_request = CreateSecretRequest ( parent = create_parent , secret_id = secret_id , secret = secret ) await run_sync_in_worker_thread ( client . create_secret , request = create_request ) self . logger . debug ( f \"Preparing to write secret data to { parent !r} again.\" ) response = await run_sync_in_worker_thread ( client . add_secret_version , request = add_request ) self . logger . info ( f \"The secret data was written successfully to { parent !r} .\" ) return response . name","title":"write_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager-functions","text":"","title":"Functions"},{"location":"secret_manager/#prefect_gcp.secret_manager.create_secret","text":"Creates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the created secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow () def example_cloud_storage_create_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = create_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_create_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def create_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Creates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the created secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import create_secret @flow() def example_cloud_storage_create_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = create_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_create_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Creating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } \" secret_settings = { \"replication\" : { \"automatic\" : {}}} partial_create = partial ( client . create_secret , parent = parent , secret_id = secret_name , secret = secret_settings , timeout = timeout , ) response = await to_thread . run_sync ( partial_create ) return response . name","title":"create_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret","text":"Deletes the specified secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to delete. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow () def example_cloud_storage_delete_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = delete_secret ( \"secret_name\" , gcp_credentials ) return secret_path example_cloud_storage_delete_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes the specified secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to delete. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret @flow() def example_cloud_storage_delete_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = delete_secret(\"secret_name\", gcp_credentials) return secret_path example_cloud_storage_delete_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Deleting %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /\" partial_delete = partial ( client . delete_secret , name = name , timeout = timeout ) await to_thread . run_sync ( partial_delete ) return name","title":"delete_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret_version","text":"Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required version_id int Version number of the secret to use; \"latest\" can NOT be used. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the deleted secret version. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow () def example_cloud_storage_delete_secret_version_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = delete_secret_version ( \"secret_name\" , 1 , gcp_credentials ) return secret_value example_cloud_storage_delete_secret_version_flow () Source code in prefect_gcp/secret_manager.py @task async def delete_secret_version ( secret_name : str , version_id : int , gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Deletes a version of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. version_id: Version number of the secret to use; \"latest\" can NOT be used. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the deleted secret version. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import delete_secret_version @flow() def example_cloud_storage_delete_secret_version_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials) return secret_value example_cloud_storage_delete_secret_version_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project if version_id == \"latest\" : raise ValueError ( \"The version_id cannot be 'latest'\" ) name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_destroy = partial ( client . destroy_secret_version , name = name , timeout = timeout ) await to_thread . run_sync ( partial_destroy ) return name","title":"delete_secret_version()"},{"location":"secret_manager/#prefect_gcp.secret_manager.read_secret","text":"Reads the value of a given secret from Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str Contents of the specified secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow () def example_cloud_storage_read_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_value = read_secret ( \"secret_name\" , gcp_credentials , version_id = 1 ) return secret_value example_cloud_storage_read_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def read_secret ( secret_name : str , gcp_credentials : \"GcpCredentials\" , version_id : Union [ str , int ] = \"latest\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Reads the value of a given secret from Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: Contents of the specified secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import read_secret @flow() def example_cloud_storage_read_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1) return secret_value example_cloud_storage_read_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Reading %s version of %s secret\" , version_id , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project name = f \"projects/ { project } /secrets/ { secret_name } /versions/ { version_id } \" partial_access = partial ( client . access_secret_version , name = name , timeout = timeout ) response = await to_thread . run_sync ( partial_access ) secret = response . payload . data . decode ( \"UTF-8\" ) return secret","title":"read_secret()"},{"location":"secret_manager/#prefect_gcp.secret_manager.update_secret","text":"Updates a secret in Google Cloud Platform's Secret Manager. Parameters: Name Type Description Default secret_name str Name of the secret to retrieve. required secret_value Union[str, bytes] Desired value of the secret. Can be either str or bytes . required gcp_credentials GcpCredentials Credentials to use for authentication with GCP. required timeout float The number of seconds the transport should wait for the server response. 60 project Optional[str] Name of the project to use; overrides the gcp_credentials project if provided. None Returns: Type Description str The path of the updated secret. Examples: from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow () def example_cloud_storage_update_secret_flow (): gcp_credentials = GcpCredentials ( project = \"project\" ) secret_path = update_secret ( \"secret_name\" , \"secret_value\" , gcp_credentials ) return secret_path example_cloud_storage_update_secret_flow () Source code in prefect_gcp/secret_manager.py @task async def update_secret ( secret_name : str , secret_value : Union [ str , bytes ], gcp_credentials : \"GcpCredentials\" , timeout : float = 60 , project : Optional [ str ] = None , ) -> str : \"\"\" Updates a secret in Google Cloud Platform's Secret Manager. Args: secret_name: Name of the secret to retrieve. secret_value: Desired value of the secret. Can be either `str` or `bytes`. gcp_credentials: Credentials to use for authentication with GCP. timeout: The number of seconds the transport should wait for the server response. project: Name of the project to use; overrides the gcp_credentials project if provided. Returns: The path of the updated secret. Example: ```python from prefect import flow from prefect_gcp import GcpCredentials from prefect_gcp.secret_manager import update_secret @flow() def example_cloud_storage_update_secret_flow(): gcp_credentials = GcpCredentials(project=\"project\") secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials) return secret_path example_cloud_storage_update_secret_flow() ``` \"\"\" logger = get_run_logger () logger . info ( \"Updating the %s secret\" , secret_name ) client = gcp_credentials . get_secret_manager_client () project = project or gcp_credentials . project parent = f \"projects/ { project } /secrets/ { secret_name } \" if isinstance ( secret_value , str ): secret_value = secret_value . encode ( \"UTF-8\" ) partial_add = partial ( client . add_secret_version , parent = parent , payload = { \"data\" : secret_value }, timeout = timeout , ) response = await to_thread . run_sync ( partial_add ) return response . name","title":"update_secret()"}]}
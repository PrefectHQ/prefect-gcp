{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Coordinate and use GCP in your dataflow with <code>prefect-gcp</code>","text":"<p>The <code>prefect-openai</code> collection makes it easy to leverage the capabilities of Google Cloud Platform (GCP) in your flows. Check out the examples below to get started!</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#saving-credentials-to-a-block","title":"Saving credentials to a block","text":"<p>You will need to obtain GCP credentials in order to use <code>prefect-gcp</code>.</p> <ol> <li>Refer to the GCP service account documentation on how to create and download a service account key file</li> <li>Copy the JSON contents</li> <li>Create a short script, replacing the placeholders (or do so in the UI)</li> </ol> <pre><code>from prefect_gcp import GcpCredentials\n\n# replace this PLACEHOLDER dict with your own service account info\nservice_account_info = {\n  \"type\": \"service_account\",\n  \"project_id\": \"PROJECT_ID\",\n  \"private_key_id\": \"KEY_ID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"SERVICE_ACCOUNT_EMAIL\",\n  \"client_id\": \"CLIENT_ID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\"\n}\n\nGcpCredentials(\n    service_account_info=service_account_info\n).save(\"BLOCK_NAME_PLACEHOLDER\")\n</code></pre> <p>Congrats! You can now easily load the saved block, which holds your credentials:</p> <pre><code>from prefect_gcp import GcpCredentials\nGcpCredentials.load(\"BLOCK_NAME_PLACEHOLDER\")\n</code></pre> <p>Info</p> <p>Register blocks in this module to view and edit them on Prefect Cloud:</p> <pre><code>prefect block register -m prefect_gcp\n</code></pre>"},{"location":"#download-blob-from-bucket","title":"Download blob from bucket","text":"<pre><code>from prefect import flow\nfrom prefect_gcp.cloud_storage import GcsBucket\n\n@flow\ndef download_flow():\n    gcs_bucket = GcsBucket.load(\"my-bucket\")\n    path = gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n    return path\n\ndownload_flow()\n</code></pre>"},{"location":"#deploy-command-on-cloud-run","title":"Deploy command on Cloud Run","text":"<p>Save the following as <code>prefect_gcp_flow.py</code>:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_run import CloudRunJob\n\n@flow\ndef cloud_run_job_flow():\n    cloud_run_job = CloudRunJob(\n        image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n        credentials=GcpCredentials.load(\"MY_BLOCK_NAME\"),\n        region=\"us-central1\",\n        command=[\"echo\", \"hello world\"],\n    )\n    return cloud_run_job.run()\n</code></pre> <p>Deploy <code>prefect_gcp_flow.py</code>:</p> <pre><code>from prefect.deployments import Deployment\nfrom prefect_gcp_flow import cloud_run_job_flow\n\ndeployment = Deployment.build_from_flow(\n    flow=cloud_run_job_flow,\n    name=\"cloud_run_job_deployment\", \n    version=1, \n    work_queue_name=\"demo\",\n)\ndeployment.apply()\n</code></pre> <p>Run the deployment either on the UI or through the CLI: <pre><code>prefect deployment run cloud-run-job-flow/cloud_run_job_deployment\n</code></pre></p> <p>Visit Prefect Deployments for more information about deployments.</p>"},{"location":"#get-google-auth-credentials-from-gcpcredentials","title":"Get Google auth credentials from GcpCredentials","text":"<p>To instantiate a Google Cloud client, like <code>bigquery.Client</code>, <code>GcpCredentials</code> is not a valid input. Instead, use the <code>get_credentials_from_service_account</code> method.</p> <pre><code>import google.cloud.bigquery\nfrom prefect import flow\nfrom prefect_gcp import GcpCredentials\n\n@flow\ndef create_bigquery_client():\n    gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\")\n    google_auth_credentials = gcp_credentials_block.get_credentials_from_service_account()\n    bigquery_client = bigquery.Client(credentials=google_auth_credentials)\n</code></pre> <p>Or simply call <code>get_bigquery_client</code> from <code>GcpCredentials</code>.</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\n\n@flow\ndef create_bigquery_client():\n    gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\")\n    bigquery_client = gcp_credentials_block.get_bigquery_client()\n</code></pre>"},{"location":"#deploy-command-on-vertex-ai-as-a-flow","title":"Deploy command on Vertex AI as a flow","text":"<p>Save the following as <code>prefect_gcp_flow.py</code>:</p> <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\n@flow\ndef vertex_ai_job_flow():\n    gcp_credentials = GcpCredentials.load(\"MY_BLOCK\")\n    job = VertexAICustomTrainingJob(\n        command=[\"echo\", \"hello world\"],\n        region=\"us-east1\",\n        image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n        gcp_credentials=gcp_credentials,\n    )\n    job.run()\n\nvertex_ai_job_flow()\n</code></pre> <p>Deploy <code>prefect_gcp_flow.py</code>:</p> <pre><code>from prefect.deployments import Deployment\nfrom prefect_gcp_flow import vertex_ai_job_flow\n\ndeployment = Deployment.build_from_flow(\n    flow=vertex_ai_job_flow,\n    name=\"vertex-ai-job-deployment\", \n    version=1, \n    work_queue_name=\"demo\",\n)\ndeployment.apply()\n</code></pre> <p>Run the deployment either on the UI or through the CLI: <pre><code>prefect deployment run vertex-ai-job-flow/vertex-ai-job-deployment\n</code></pre></p> <p>Visit Prefect Deployments for more information about deployments.</p>"},{"location":"#resources","title":"Resources","text":"<p>For more tips on how to use tasks and flows in a Collection, check out Using Collections!</p>"},{"location":"#installation","title":"Installation","text":"<p>To use <code>prefect-gcp</code> and Cloud Run:</p> <pre><code>pip install prefect-gcp\n</code></pre> <p>To use Cloud Storage: <pre><code>pip install \"prefect-gcp[cloud_storage]\"\n</code></pre></p> <p>To use BigQuery:</p> <pre><code>pip install \"prefect-gcp[bigquery]\"\n</code></pre> <p>To use Secret Manager: <pre><code>pip install \"prefect-gcp[secret_manager]\"\n</code></pre></p> <p>To use Vertex AI: <pre><code>pip install \"prefect-gcp[aiplatform]\"\n</code></pre></p> <p>A list of available blocks in <code>prefect-gcp</code> and their setup instructions can be found here.</p> <p>Requires an installation of Python 3.7+.</p> <p>We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv.</p> <p>These tasks are designed to work with Prefect 2.0. For more information about how to use Prefect, please refer to the Prefect documentation.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you encounter any bugs while using <code>prefect-gcp</code>, feel free to open an issue in the prefect-gcp repository.</p> <p>If you have any questions or issues while using <code>prefect-gcp</code>, you can find help in either the Prefect Discourse forum or the Prefect Slack community.</p> <p>Feel free to star or watch <code>prefect-gcp</code> for updates too!</p>"},{"location":"#contributing","title":"Contributing","text":"<p>If you'd like to help contribute to fix an issue or add a feature to <code>prefect-gcp</code>, please propose changes through a pull request from a fork of the repository.</p> <p>Here are the steps:</p> <ol> <li>Fork the repository</li> <li>Clone the forked repository</li> <li>Install the repository and its dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Make desired changes</li> <li>Add tests</li> <li>Insert an entry to CHANGELOG.md</li> <li>Install <code>pre-commit</code> to perform quality checks prior to commit: <pre><code>pre-commit install\n</code></pre></li> <li><code>git commit</code>, <code>git push</code>, and create a pull request</li> </ol>"},{"location":"aiplatform/","title":"AI Platform","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform","title":"<code>prefect_gcp.aiplatform</code>","text":"<p>Integrations with Google AI Platform.</p> <p>Note this module is experimental. The intefaces within may change without notice.</p> <p>Examples:</p> <p>Run a job using Vertex AI Custom Training: <pre><code>from prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.run()\n</code></pre></p> <p>Run a job that runs the command <code>echo hello world</code> using Google Cloud Run Jobs: <pre><code>from prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    command=[\"echo\", \"hello world\"],\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.run()\n</code></pre></p> <p>Preview job specs: <pre><code>from prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    command=[\"echo\", \"hello world\"],\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.preview()\n</code></pre></p>"},{"location":"aiplatform/#prefect_gcp.aiplatform-classes","title":"Classes","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob","title":"<code> VertexAICustomTrainingJob            (Infrastructure)         </code>  <code>pydantic-model</code>","text":"<p>Infrastructure block used to run Vertex AI custom training jobs.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>class VertexAICustomTrainingJob(Infrastructure):\n\"\"\"\n    Infrastructure block used to run Vertex AI custom training jobs.\n    \"\"\"\n\n    _block_type_name = \"Vertex AI Custom Training Job\"\n    _block_type_slug = \"vertex-ai-custom-training-job\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob\"  # noqa: E501\n\n    type: Literal[\"vertex-ai-custom-training-job\"] = Field(\n        \"vertex-ai-custom-training-job\", description=\"The slug for this task type.\"\n    )\n\n    gcp_credentials: GcpCredentials = Field(\n        default_factory=GcpCredentials,\n        description=(\n            \"GCP credentials to use when running the configured Vertex AI custom \"\n            \"training job. If not provided, credentials will be inferred from the \"\n            \"environment. See `GcpCredentials` for details.\"\n        ),\n    )\n    region: str = Field(\n        default=...,\n        description=\"The region where the Vertex AI custom training job resides.\",\n    )\n    image: str = Field(\n        default=...,\n        title=\"Image Name\",\n        description=(\n            \"The image to use for a new Vertex AI custom training job. This value must \"\n            \"refer to an image within either Google Container Registry \"\n            \"or Google Artifact Registry, like `gcr.io/&lt;project_name&gt;/&lt;repo&gt;/`.\"\n        ),\n    )\n    env: Dict[str, str] = Field(\n        default_factory=dict,\n        title=\"Environment Variables\",\n        description=\"Environment variables to be passed to your Cloud Run Job.\",\n    )\n    machine_type: str = Field(\n        default=\"n1-standard-4\",\n        description=\"The machine type to use for the run, which controls the available \"\n        \"CPU and memory.\",\n    )\n    accelerator_type: Optional[str] = Field(\n        default=None, description=\"The type of accelerator to attach to the machine.\"\n    )\n    maximum_run_time: datetime.timedelta = Field(\n        default=datetime.timedelta(days=7), description=\"The maximum job running time.\"\n    )\n    network: Optional[str] = Field(\n        default=None,\n        description=\"The full name of the Compute Engine network\"\n        \"to which the Job should be peered. Private services access must \"\n        \"already be configured for the network. If left unspecified, the job \"\n        \"is not peered with any network.\",\n    )\n    reserved_ip_ranges: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of names for the reserved ip ranges under the VPC \"\n        \"network that can be used for this job. If set, we will deploy the job \"\n        \"within the provided ip ranges. Otherwise, the job will be deployed to \"\n        \"any ip ranges under the provided VPC network.\",\n    )\n    service_account: Optional[str] = Field(\n        default=None,\n        description=(\n            \"Specifies the service account to use \"\n            \"as the run-as account in Vertex AI. The agent submitting jobs must have \"\n            \"act-as permission on this run-as account. If unspecified, the AI \"\n            \"Platform Custom Code Service Agent for the CustomJob's project is \"\n            \"used. Takes precedence over the service account found in gcp_credentials, \"\n            \"and required if a service account cannot be detected in gcp_credentials.\"\n        ),\n    )\n\n    job_watch_poll_interval: float = Field(\n        default=5.0,\n        description=(\n            \"The amount of time to wait between GCP API calls while monitoring the \"\n            \"state of a Vertex AI Job.\"\n        ),\n    )\n\n    @property\n    def job_name(self):\n\"\"\"\n        The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference:\n        https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name\n        \"\"\"  # noqa\n        try:\n            repo_name = self.image.split(\"/\")[2]  # `gcr.io/&lt;project_name&gt;/&lt;repo&gt;/`\"\n        except IndexError:\n            raise ValueError(\n                \"The provided image must be from either Google Container Registry \"\n                \"or Google Artifact Registry\"\n            )\n\n        unique_suffix = uuid4().hex\n        job_name = f\"{repo_name}-{unique_suffix}\"\n        return job_name\n\n    def preview(self) -&gt; str:\n\"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n        job_spec = self._build_job_spec()\n        custom_job = CustomJob(display_name=self.job_name, job_spec=job_spec)\n        return str(custom_job)  # outputs a json string\n\n    def _build_job_spec(self) -&gt; \"CustomJobSpec\":\n\"\"\"\n        Builds a job spec by gathering details.\n        \"\"\"\n        # gather worker pool spec\n        env_list = [\n            {\"name\": name, \"value\": value}\n            for name, value in {\n                **self._base_environment(),\n                **self.env,\n            }.items()\n        ]\n        container_spec = ContainerSpec(\n            image_uri=self.image, command=self.command, args=[], env=env_list\n        )\n        machine_spec = MachineSpec(\n            machine_type=self.machine_type, accelerator_type=self.accelerator_type\n        )\n        worker_pool_spec = WorkerPoolSpec(\n            container_spec=container_spec, machine_spec=machine_spec, replica_count=1\n        )\n\n        # look for service account\n        service_account = (\n            self.service_account or self.gcp_credentials._service_account_email\n        )\n        if service_account is None:\n            raise ValueError(\n                \"A service account is required for the Vertex job. \"\n                \"A service account could not be detected in the attached credentials; \"\n                \"please set a service account explicitly, e.g. \"\n                '`VertexAICustomTrainingJob(service_acount=\"...\")`'\n            )\n\n        # build custom job specs\n        timeout = Duration().FromTimedelta(td=self.maximum_run_time)\n        scheduling = Scheduling(timeout=timeout)\n        job_spec = CustomJobSpec(\n            worker_pool_specs=[worker_pool_spec],\n            service_account=service_account,\n            scheduling=scheduling,\n            network=self.network,\n            reserved_ip_ranges=self.reserved_ip_ranges,\n        )\n        return job_spec\n\n    async def _create_and_begin_job(\n        self, job_spec: \"CustomJobSpec\", job_service_client: \"JobServiceClient\"\n    ) -&gt; \"CustomJob\":\n\"\"\"\n        Builds a custom job and begins running it.\n        \"\"\"\n        # create custom job\n        custom_job = CustomJob(display_name=self.job_name, job_spec=job_spec)\n\n        # run job\n        self.logger.info(\n            f\"{self._log_prefix}: Job {self.job_name!r} starting to run \"\n            f\"the command {' '.join(self.command)!r} in region \"\n            f\"{self.region!r} using image {self.image!r}\"\n        )\n\n        project = self.gcp_credentials.project\n        resource_name = f\"projects/{project}/locations/{self.region}\"\n        custom_job_run = await run_sync_in_worker_thread(\n            job_service_client.create_custom_job,\n            parent=resource_name,\n            custom_job=custom_job,\n        )\n\n        self.logger.info(\n            f\"{self._log_prefix}: Job {self.job_name!r} has successfully started; \"\n            f\"the full job name is {custom_job_run.name!r}\"\n        )\n\n        return custom_job_run\n\n    async def _watch_job_run(\n        self,\n        full_job_name: str,  # different from self.job_name\n        job_service_client: \"JobServiceClient\",\n        current_state: \"JobState\",\n        until_states: Tuple[\"JobState\"],\n        timeout: int = None,\n    ) -&gt; \"CustomJob\":\n\"\"\"\n        Polls job run to see if status changed.\n        \"\"\"\n        state = JobState.JOB_STATE_UNSPECIFIED\n        last_state = current_state\n        t0 = time.time()\n\n        while state not in until_states:\n            job_run = await run_sync_in_worker_thread(\n                job_service_client.get_custom_job,\n                name=full_job_name,\n            )\n            state = job_run.state\n            if state != last_state:\n                state_label = (\n                    state.name.replace(\"_\", \" \")\n                    .lower()\n                    .replace(\"state\", \"state is now:\")\n                )\n                # results in \"New job state is now: succeeded\"\n                self.logger.info(\n                    f\"{self._log_prefix}: {self.job_name} has new {state_label}\"\n                )\n                last_state = state\n            else:\n                # Intermittently, the job will not be described. We want to respect the\n                # watch timeout though.\n                self.logger.debug(f\"{self._log_prefix}: Job not found.\")\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while watching job for states \"\n                    \"{until_states!r}\"\n                )\n            time.sleep(self.job_watch_poll_interval)\n\n        return job_run\n\n    @sync_compatible\n    async def run(\n        self, task_status: Optional[\"TaskStatus\"] = None\n    ) -&gt; VertexAICustomTrainingJobResult:\n\"\"\"\n        Run the configured task on VertexAI.\n\n        Args:\n            task_status: An optional `TaskStatus` to update when the container starts.\n\n        Returns:\n            The `VertexAICustomTrainingJobResult`.\n        \"\"\"\n        client_options = ClientOptions(\n            api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n        )\n\n        job_spec = self._build_job_spec()\n        with self.gcp_credentials.get_job_service_client(\n            client_options=client_options\n        ) as job_service_client:\n            job_run = await self._create_and_begin_job(job_spec, job_service_client)\n\n            if task_status:\n                task_status.started(self.job_name)\n\n            final_job_run = await self._watch_job_run(\n                full_job_name=job_run.name,\n                job_service_client=job_service_client,\n                current_state=job_run.state,\n                until_states=(\n                    JobState.JOB_STATE_SUCCEEDED,\n                    JobState.JOB_STATE_FAILED,\n                    JobState.JOB_STATE_CANCELLED,\n                    JobState.JOB_STATE_EXPIRED,\n                ),\n                timeout=self.maximum_run_time.total_seconds(),\n            )\n\n        error_msg = final_job_run.error.message\n        if error_msg:\n            raise RuntimeError(f\"{self._log_prefix}: {error_msg}\")\n\n        status_code = 0 if final_job_run.state == JobState.JOB_STATE_SUCCEEDED else 1\n        return VertexAICustomTrainingJobResult(\n            identifier=final_job_run.display_name, status_code=status_code\n        )\n\n    @sync_compatible\n    async def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n\"\"\"\n        Kill a job running Cloud Run.\n\n        Args:\n            identifier: The Vertex AI full job name, formatted like\n                \"projects/{project}/locations/{location}/customJobs/{custom_job}\".\n\n        Returns:\n            The `VertexAICustomTrainingJobResult`.\n        \"\"\"\n        client_options = ClientOptions(\n            api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n        )\n        with self.gcp_credentials.get_job_service_client(\n            client_options=client_options\n        ) as job_service_client:\n            await run_sync_in_worker_thread(\n                self._kill_job,\n                job_service_client=job_service_client,\n                full_job_name=identifier,\n            )\n            self.logger.info(f\"Requested to cancel {identifier}...\")\n\n    def _kill_job(\n        self, job_service_client: \"JobServiceClient\", full_job_name: str\n    ) -&gt; None:\n\"\"\"\n        Thin wrapper around Job.delete, wrapping a try/except since\n        Job is an independent class that doesn't have knowledge of\n        CloudRunJob and its associated logic.\n        \"\"\"\n        cancel_custom_job_request = CancelCustomJobRequest(name=full_job_name)\n        try:\n            job_service_client.cancel_custom_job(\n                request=cancel_custom_job_request,\n            )\n        except Exception as exc:\n            if \"does not exist\" in str(exc):\n                raise InfrastructureNotFound(\n                    f\"Cannot stop Vertex AI job; the job name {full_job_name!r} \"\n                    \"could not be found.\"\n                ) from exc\n            raise\n\n    @property\n    def _log_prefix(self) -&gt; str:\n\"\"\"\n        Internal property for generating a prefix for logs where `name` may be null\n        \"\"\"\n        if self.name is not None:\n            return f\"VertexAICustomTrainingJob {self.name!r}\"\n        else:\n            return \"VertexAICustomTrainingJob\"\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob-attributes","title":"Attributes","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.accelerator_type","title":"<code>accelerator_type: str</code>  <code>pydantic-field</code>","text":"<p>The type of accelerator to attach to the machine.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.gcp_credentials","title":"<code>gcp_credentials: GcpCredentials</code>  <code>pydantic-field</code>","text":"<p>GCP credentials to use when running the configured Vertex AI custom training job. If not provided, credentials will be inferred from the environment. See <code>GcpCredentials</code> for details.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.image","title":"<code>image: str</code>  <code>pydantic-field</code> <code>required</code>","text":"<p>The image to use for a new Vertex AI custom training job. This value must refer to an image within either Google Container Registry or Google Artifact Registry, like <code>gcr.io/&lt;project_name&gt;/&lt;repo&gt;/</code>.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.job_name","title":"<code>job_name</code>  <code>property</code> <code>readonly</code>","text":"<p>The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.job_watch_poll_interval","title":"<code>job_watch_poll_interval: float</code>  <code>pydantic-field</code>","text":"<p>The amount of time to wait between GCP API calls while monitoring the state of a Vertex AI Job.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.machine_type","title":"<code>machine_type: str</code>  <code>pydantic-field</code>","text":"<p>The machine type to use for the run, which controls the available CPU and memory.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.maximum_run_time","title":"<code>maximum_run_time: timedelta</code>  <code>pydantic-field</code>","text":"<p>The maximum job running time.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.network","title":"<code>network: str</code>  <code>pydantic-field</code>","text":"<p>The full name of the Compute Engine networkto which the Job should be peered. Private services access must already be configured for the network. If left unspecified, the job is not peered with any network.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.region","title":"<code>region: str</code>  <code>pydantic-field</code> <code>required</code>","text":"<p>The region where the Vertex AI custom training job resides.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.reserved_ip_ranges","title":"<code>reserved_ip_ranges: List[str]</code>  <code>pydantic-field</code>","text":"<p>A list of names for the reserved ip ranges under the VPC network that can be used for this job. If set, we will deploy the job within the provided ip ranges. Otherwise, the job will be deployed to any ip ranges under the provided VPC network.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.service_account","title":"<code>service_account: str</code>  <code>pydantic-field</code>","text":"<p>Specifies the service account to use as the run-as account in Vertex AI. The agent submitting jobs must have act-as permission on this run-as account. If unspecified, the AI Platform Custom Code Service Agent for the CustomJob's project is used. Takes precedence over the service account found in gcp_credentials, and required if a service account cannot be detected in gcp_credentials.</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob-methods","title":"Methods","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.kill","title":"<code>kill</code>  <code>async</code>","text":"<p>Kill a job running Cloud Run.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\".</p> required <p>Returns:</p> Type Description <code>None</code> <p>The <code>VertexAICustomTrainingJobResult</code>.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>@sync_compatible\nasync def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n\"\"\"\n    Kill a job running Cloud Run.\n\n    Args:\n        identifier: The Vertex AI full job name, formatted like\n            \"projects/{project}/locations/{location}/customJobs/{custom_job}\".\n\n    Returns:\n        The `VertexAICustomTrainingJobResult`.\n    \"\"\"\n    client_options = ClientOptions(\n        api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n    )\n    with self.gcp_credentials.get_job_service_client(\n        client_options=client_options\n    ) as job_service_client:\n        await run_sync_in_worker_thread(\n            self._kill_job,\n            job_service_client=job_service_client,\n            full_job_name=identifier,\n        )\n        self.logger.info(f\"Requested to cancel {identifier}...\")\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.preview","title":"<code>preview</code>","text":"<p>Generate a preview of the job definition that will be sent to GCP.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>def preview(self) -&gt; str:\n\"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n    job_spec = self._build_job_spec()\n    custom_job = CustomJob(display_name=self.job_name, job_spec=job_spec)\n    return str(custom_job)  # outputs a json string\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.run","title":"<code>run</code>  <code>async</code>","text":"<p>Run the configured task on VertexAI.</p> <p>Parameters:</p> Name Type Description Default <code>task_status</code> <code>Optional[TaskStatus]</code> <p>An optional <code>TaskStatus</code> to update when the container starts.</p> <code>None</code> <p>Returns:</p> Type Description <code>VertexAICustomTrainingJobResult</code> <p>The <code>VertexAICustomTrainingJobResult</code>.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>@sync_compatible\nasync def run(\n    self, task_status: Optional[\"TaskStatus\"] = None\n) -&gt; VertexAICustomTrainingJobResult:\n\"\"\"\n    Run the configured task on VertexAI.\n\n    Args:\n        task_status: An optional `TaskStatus` to update when the container starts.\n\n    Returns:\n        The `VertexAICustomTrainingJobResult`.\n    \"\"\"\n    client_options = ClientOptions(\n        api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n    )\n\n    job_spec = self._build_job_spec()\n    with self.gcp_credentials.get_job_service_client(\n        client_options=client_options\n    ) as job_service_client:\n        job_run = await self._create_and_begin_job(job_spec, job_service_client)\n\n        if task_status:\n            task_status.started(self.job_name)\n\n        final_job_run = await self._watch_job_run(\n            full_job_name=job_run.name,\n            job_service_client=job_service_client,\n            current_state=job_run.state,\n            until_states=(\n                JobState.JOB_STATE_SUCCEEDED,\n                JobState.JOB_STATE_FAILED,\n                JobState.JOB_STATE_CANCELLED,\n                JobState.JOB_STATE_EXPIRED,\n            ),\n            timeout=self.maximum_run_time.total_seconds(),\n        )\n\n    error_msg = final_job_run.error.message\n    if error_msg:\n        raise RuntimeError(f\"{self._log_prefix}: {error_msg}\")\n\n    status_code = 0 if final_job_run.state == JobState.JOB_STATE_SUCCEEDED else 1\n    return VertexAICustomTrainingJobResult(\n        identifier=final_job_run.display_name, status_code=status_code\n    )\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJobResult","title":"<code> VertexAICustomTrainingJobResult            (InfrastructureResult)         </code>  <code>pydantic-model</code>","text":"<p>Result from a Vertex AI custom training job.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>class VertexAICustomTrainingJobResult(InfrastructureResult):\n\"\"\"Result from a Vertex AI custom training job.\"\"\"\n</code></pre>"},{"location":"bigquery/","title":"BigQuery","text":""},{"location":"bigquery/#prefect_gcp.bigquery","title":"<code>prefect_gcp.bigquery</code>","text":"<p>Tasks for interacting with GCP BigQuery</p>"},{"location":"bigquery/#prefect_gcp.bigquery-classes","title":"Classes","text":""},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse","title":"<code> BigQueryWarehouse            (DatabaseBlock)         </code>  <code>pydantic-model</code>","text":"<p>A block for querying a database with BigQuery.</p> <p>Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called.</p> <p>It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited.</p> <p>It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost.</p> <p>Attributes:</p> Name Type Description <code>gcp_credentials</code> <code>GcpCredentials</code> <p>The credentials to use to authenticate.</p> <code>fetch_size</code> <code>int</code> <p>The number of rows to fetch at a time when calling fetch_many. Note, this parameter is executed on the client side and is not passed to the database. To limit on the server side, add the <code>LIMIT</code> clause, or the dialect's equivalent clause, like <code>TOP</code>, to the query.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>class BigQueryWarehouse(DatabaseBlock):\n\"\"\"\n    A block for querying a database with BigQuery.\n\n    Upon instantiating, a connection to BigQuery is established\n    and maintained for the life of the object until the close method is called.\n\n    It is recommended to use this block as a context manager, which will automatically\n    close the connection and its cursors when the context is exited.\n\n    It is also recommended that this block is loaded and consumed within a single task\n    or flow because if the block is passed across separate tasks and flows,\n    the state of the block's connection and cursor could be lost.\n\n    Attributes:\n        gcp_credentials: The credentials to use to authenticate.\n        fetch_size: The number of rows to fetch at a time when calling fetch_many.\n            Note, this parameter is executed on the client side and is not\n            passed to the database. To limit on the server side, add the `LIMIT`\n            clause, or the dialect's equivalent clause, like `TOP`, to the query.\n    \"\"\"  # noqa\n\n    _block_type_name = \"BigQuery Warehouse\"\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/bigquery/#prefect_gcp.bigquery.BigQueryWarehouse\"  # noqa: E501\n\n    gcp_credentials: GcpCredentials\n    fetch_size: int = Field(\n        default=1, description=\"The number of rows to fetch at a time.\"\n    )\n\n    _connection: Optional[\"Connection\"] = None\n    _unique_cursors: Dict[str, \"Cursor\"] = None\n\n    def _start_connection(self):\n\"\"\"\n        Starts a connection.\n        \"\"\"\n        with self.gcp_credentials.get_bigquery_client() as client:\n            self._connection = Connection(client=client)\n\n    def block_initialization(self) -&gt; None:\n        super().block_initialization()\n        if self._connection is None:\n            self._start_connection()\n\n        if self._unique_cursors is None:\n            self._unique_cursors = {}\n\n    def get_connection(self) -&gt; \"Connection\":\n\"\"\"\n        Get the opened connection to BigQuery.\n        \"\"\"\n        return self._connection\n\n    def _get_cursor(self, inputs: Dict[str, Any]) -&gt; Tuple[bool, \"Cursor\"]:\n\"\"\"\n        Get a BigQuery cursor.\n\n        Args:\n            inputs: The inputs to generate a unique hash, used to decide\n                whether a new cursor should be used.\n\n        Returns:\n            Whether a cursor is new and a BigQuery cursor.\n        \"\"\"\n        input_hash = hash_objects(inputs)\n        assert input_hash is not None, (\n            \"We were not able to hash your inputs, \"\n            \"which resulted in an unexpected data return; \"\n            \"please open an issue with a reproducible example.\"\n        )\n        if input_hash not in self._unique_cursors.keys():\n            new_cursor = self._connection.cursor()\n            self._unique_cursors[input_hash] = new_cursor\n            return True, new_cursor\n        else:\n            existing_cursor = self._unique_cursors[input_hash]\n            return False, existing_cursor\n\n    def reset_cursors(self) -&gt; None:\n\"\"\"\n        Tries to close all opened cursors.\n        \"\"\"\n        input_hashes = tuple(self._unique_cursors.keys())\n        for input_hash in input_hashes:\n            cursor = self._unique_cursors.pop(input_hash)\n            try:\n                cursor.close()\n            except Exception as exc:\n                self.logger.warning(\n                    f\"Failed to close cursor for input hash {input_hash!r}: {exc}\"\n                )\n\n    @sync_compatible\n    async def fetch_one(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; \"Row\":\n\"\"\"\n        Fetch a single result from the database.\n\n        Repeated calls using the same inputs to *any* of the fetch methods of this\n        block will skip executing the operation again, and instead,\n        return the next set of results from the previous execution,\n        until the reset_cursors method is called.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Returns:\n            A tuple containing the data returned by the database,\n                where each row is a tuple and each column is a value in the tuple.\n\n        Examples:\n            Execute operation with parameters, fetching one new row at a time:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    SELECT word, word_count\n                    FROM `bigquery-public-data.samples.shakespeare`\n                    WHERE corpus = %(corpus)s\n                    AND word_count &gt;= %(min_word_count)s\n                    ORDER BY word_count DESC\n                    LIMIT 3;\n                '''\n                parameters = {\n                    \"corpus\": \"romeoandjuliet\",\n                    \"min_word_count\": 250,\n                }\n                for _ in range(0, 3):\n                    result = warehouse.fetch_one(operation, parameters=parameters)\n                    print(result)\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        new, cursor = self._get_cursor(inputs)\n        if new:\n            await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n        result = await run_sync_in_worker_thread(cursor.fetchone)\n        return result\n\n    @sync_compatible\n    async def fetch_many(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        size: Optional[int] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; List[\"Row\"]:\n\"\"\"\n        Fetch a limited number of results from the database.\n\n        Repeated calls using the same inputs to *any* of the fetch methods of this\n        block will skip executing the operation again, and instead,\n        return the next set of results from the previous execution,\n        until the reset_cursors method is called.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            size: The number of results to return; if None or 0, uses the value of\n                `fetch_size` configured on the block.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Returns:\n            A list of tuples containing the data returned by the database,\n                where each row is a tuple and each column is a value in the tuple.\n\n        Examples:\n            Execute operation with parameters, fetching two new rows at a time:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    SELECT word, word_count\n                    FROM `bigquery-public-data.samples.shakespeare`\n                    WHERE corpus = %(corpus)s\n                    AND word_count &gt;= %(min_word_count)s\n                    ORDER BY word_count DESC\n                    LIMIT 6;\n                '''\n                parameters = {\n                    \"corpus\": \"romeoandjuliet\",\n                    \"min_word_count\": 250,\n                }\n                for _ in range(0, 3):\n                    result = warehouse.fetch_many(\n                        operation,\n                        parameters=parameters,\n                        size=2\n                    )\n                    print(result)\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        new, cursor = self._get_cursor(inputs)\n        if new:\n            await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n        size = size or self.fetch_size\n        result = await run_sync_in_worker_thread(cursor.fetchmany, size=size)\n        return result\n\n    @sync_compatible\n    async def fetch_all(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; List[\"Row\"]:\n\"\"\"\n        Fetch all results from the database.\n\n        Repeated calls using the same inputs to *any* of the fetch methods of this\n        block will skip executing the operation again, and instead,\n        return the next set of results from the previous execution,\n        until the reset_cursors method is called.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Returns:\n            A list of tuples containing the data returned by the database,\n                where each row is a tuple and each column is a value in the tuple.\n\n        Examples:\n            Execute operation with parameters, fetching all rows:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    SELECT word, word_count\n                    FROM `bigquery-public-data.samples.shakespeare`\n                    WHERE corpus = %(corpus)s\n                    AND word_count &gt;= %(min_word_count)s\n                    ORDER BY word_count DESC\n                    LIMIT 3;\n                '''\n                parameters = {\n                    \"corpus\": \"romeoandjuliet\",\n                    \"min_word_count\": 250,\n                }\n                result = warehouse.fetch_all(operation, parameters=parameters)\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        new, cursor = self._get_cursor(inputs)\n        if new:\n            await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n        result = await run_sync_in_worker_thread(cursor.fetchall)\n        return result\n\n    @sync_compatible\n    async def execute(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; None:\n\"\"\"\n        Executes an operation on the database. This method is intended to be used\n        for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n        Unlike the fetch methods, this method will always execute the operation\n        upon calling.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Examples:\n            Execute operation with parameters:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    CREATE TABLE mydataset.trips AS (\n                    SELECT\n                        bikeid,\n                        start_time,\n                        duration_minutes\n                    FROM\n                        bigquery-public-data.austin_bikeshare.bikeshare_trips\n                    LIMIT %(limit)s\n                    );\n                '''\n                warehouse.execute(operation, parameters={\"limit\": 5})\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        cursor = self._get_cursor(inputs)[1]\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    @sync_compatible\n    async def execute_many(\n        self,\n        operation: str,\n        seq_of_parameters: List[Dict[str, Any]],\n    ) -&gt; None:\n\"\"\"\n        Executes many operations on the database. This method is intended to be used\n        for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n        Unlike the fetch methods, this method will always execute the operations\n        upon calling.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            seq_of_parameters: The sequence of parameters for the operation.\n\n        Examples:\n            Create mytable in mydataset and insert two rows into it:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"bigquery\") as warehouse:\n                create_operation = '''\n                CREATE TABLE IF NOT EXISTS mydataset.mytable (\n                    col1 STRING,\n                    col2 INTEGER,\n                    col3 BOOLEAN\n                )\n                '''\n                warehouse.execute(create_operation)\n                insert_operation = '''\n                INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n                '''\n                seq_of_parameters = [\n                    (\"a\", 1, True),\n                    (\"b\", 2, False),\n                ]\n                warehouse.execute_many(\n                    insert_operation,\n                    seq_of_parameters=seq_of_parameters\n                )\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            seq_of_parameters=seq_of_parameters,\n        )\n        cursor = self._get_cursor(inputs)[1]\n        await run_sync_in_worker_thread(cursor.executemany, **inputs)\n\n    def close(self):\n\"\"\"\n        Closes connection and its cursors.\n        \"\"\"\n        try:\n            self.reset_cursors()\n        finally:\n            if self._connection is not None:\n                self._connection.close()\n                self._connection = None\n\n    def __enter__(self):\n\"\"\"\n        Start a connection upon entry.\n        \"\"\"\n        return self\n\n    def __exit__(self, *args):\n\"\"\"\n        Closes connection and its cursors upon exit.\n        \"\"\"\n        self.close()\n\n    def __getstate__(self):\n\"\"\" \"\"\"\n        data = self.__dict__.copy()\n        data.update({k: None for k in {\"_connection\", \"_unique_cursors\"}})\n        return data\n\n    def __setstate__(self, data: dict):\n\"\"\" \"\"\"\n        self.__dict__.update(data)\n        self._unique_cursors = {}\n        self._start_connection()\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse-attributes","title":"Attributes","text":""},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_size","title":"<code>fetch_size: int</code>  <code>pydantic-field</code>","text":"<p>The number of rows to fetch at a time.</p>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse-methods","title":"Methods","text":""},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.close","title":"<code>close</code>","text":"<p>Closes connection and its cursors.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>def close(self):\n\"\"\"\n    Closes connection and its cursors.\n    \"\"\"\n    try:\n        self.reset_cursors()\n    finally:\n        if self._connection is not None:\n            self._connection.close()\n            self._connection = None\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.execute","title":"<code>execute</code>  <code>async</code>","text":"<p>Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE.</p> <p>Unlike the fetch methods, this method will always execute the operation upon calling.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Examples:</p> <p>Execute operation with parameters: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        CREATE TABLE mydataset.trips AS (\n        SELECT\n            bikeid,\n            start_time,\n            duration_minutes\n        FROM\n            bigquery-public-data.austin_bikeshare.bikeshare_trips\n        LIMIT %(limit)s\n        );\n    '''\n    warehouse.execute(operation, parameters={\"limit\": 5})\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def execute(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; None:\n\"\"\"\n    Executes an operation on the database. This method is intended to be used\n    for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n    Unlike the fetch methods, this method will always execute the operation\n    upon calling.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Examples:\n        Execute operation with parameters:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                CREATE TABLE mydataset.trips AS (\n                SELECT\n                    bikeid,\n                    start_time,\n                    duration_minutes\n                FROM\n                    bigquery-public-data.austin_bikeshare.bikeshare_trips\n                LIMIT %(limit)s\n                );\n            '''\n            warehouse.execute(operation, parameters={\"limit\": 5})\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    cursor = self._get_cursor(inputs)[1]\n    await run_sync_in_worker_thread(cursor.execute, **inputs)\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.execute_many","title":"<code>execute_many</code>  <code>async</code>","text":"<p>Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE.</p> <p>Unlike the fetch methods, this method will always execute the operations upon calling.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>seq_of_parameters</code> <code>List[Dict[str, Any]]</code> <p>The sequence of parameters for the operation.</p> required <p>Examples:</p> <p>Create mytable in mydataset and insert two rows into it: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"bigquery\") as warehouse:\n    create_operation = '''\n    CREATE TABLE IF NOT EXISTS mydataset.mytable (\n        col1 STRING,\n        col2 INTEGER,\n        col3 BOOLEAN\n    )\n    '''\n    warehouse.execute(create_operation)\n    insert_operation = '''\n    INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n    '''\n    seq_of_parameters = [\n        (\"a\", 1, True),\n        (\"b\", 2, False),\n    ]\n    warehouse.execute_many(\n        insert_operation,\n        seq_of_parameters=seq_of_parameters\n    )\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def execute_many(\n    self,\n    operation: str,\n    seq_of_parameters: List[Dict[str, Any]],\n) -&gt; None:\n\"\"\"\n    Executes many operations on the database. This method is intended to be used\n    for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n    Unlike the fetch methods, this method will always execute the operations\n    upon calling.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        seq_of_parameters: The sequence of parameters for the operation.\n\n    Examples:\n        Create mytable in mydataset and insert two rows into it:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"bigquery\") as warehouse:\n            create_operation = '''\n            CREATE TABLE IF NOT EXISTS mydataset.mytable (\n                col1 STRING,\n                col2 INTEGER,\n                col3 BOOLEAN\n            )\n            '''\n            warehouse.execute(create_operation)\n            insert_operation = '''\n            INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n            '''\n            seq_of_parameters = [\n                (\"a\", 1, True),\n                (\"b\", 2, False),\n            ]\n            warehouse.execute_many(\n                insert_operation,\n                seq_of_parameters=seq_of_parameters\n            )\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        seq_of_parameters=seq_of_parameters,\n    )\n    cursor = self._get_cursor(inputs)[1]\n    await run_sync_in_worker_thread(cursor.executemany, **inputs)\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_all","title":"<code>fetch_all</code>  <code>async</code>","text":"<p>Fetch all results from the database.</p> <p>Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Row]</code> <p>A list of tuples containing the data returned by the database,     where each row is a tuple and each column is a value in the tuple.</p> <p>Examples:</p> <p>Execute operation with parameters, fetching all rows: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    result = warehouse.fetch_all(operation, parameters=parameters)\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def fetch_all(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; List[\"Row\"]:\n\"\"\"\n    Fetch all results from the database.\n\n    Repeated calls using the same inputs to *any* of the fetch methods of this\n    block will skip executing the operation again, and instead,\n    return the next set of results from the previous execution,\n    until the reset_cursors method is called.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Returns:\n        A list of tuples containing the data returned by the database,\n            where each row is a tuple and each column is a value in the tuple.\n\n    Examples:\n        Execute operation with parameters, fetching all rows:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = %(corpus)s\n                AND word_count &gt;= %(min_word_count)s\n                ORDER BY word_count DESC\n                LIMIT 3;\n            '''\n            parameters = {\n                \"corpus\": \"romeoandjuliet\",\n                \"min_word_count\": 250,\n            }\n            result = warehouse.fetch_all(operation, parameters=parameters)\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    new, cursor = self._get_cursor(inputs)\n    if new:\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    result = await run_sync_in_worker_thread(cursor.fetchall)\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_many","title":"<code>fetch_many</code>  <code>async</code>","text":"<p>Fetch a limited number of results from the database.</p> <p>Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>size</code> <code>Optional[int]</code> <p>The number of results to return; if None or 0, uses the value of <code>fetch_size</code> configured on the block.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Row]</code> <p>A list of tuples containing the data returned by the database,     where each row is a tuple and each column is a value in the tuple.</p> <p>Examples:</p> <p>Execute operation with parameters, fetching two new rows at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 6;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_many(\n            operation,\n            parameters=parameters,\n            size=2\n        )\n        print(result)\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def fetch_many(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    size: Optional[int] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; List[\"Row\"]:\n\"\"\"\n    Fetch a limited number of results from the database.\n\n    Repeated calls using the same inputs to *any* of the fetch methods of this\n    block will skip executing the operation again, and instead,\n    return the next set of results from the previous execution,\n    until the reset_cursors method is called.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        size: The number of results to return; if None or 0, uses the value of\n            `fetch_size` configured on the block.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Returns:\n        A list of tuples containing the data returned by the database,\n            where each row is a tuple and each column is a value in the tuple.\n\n    Examples:\n        Execute operation with parameters, fetching two new rows at a time:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = %(corpus)s\n                AND word_count &gt;= %(min_word_count)s\n                ORDER BY word_count DESC\n                LIMIT 6;\n            '''\n            parameters = {\n                \"corpus\": \"romeoandjuliet\",\n                \"min_word_count\": 250,\n            }\n            for _ in range(0, 3):\n                result = warehouse.fetch_many(\n                    operation,\n                    parameters=parameters,\n                    size=2\n                )\n                print(result)\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    new, cursor = self._get_cursor(inputs)\n    if new:\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    size = size or self.fetch_size\n    result = await run_sync_in_worker_thread(cursor.fetchmany, size=size)\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_one","title":"<code>fetch_one</code>  <code>async</code>","text":"<p>Fetch a single result from the database.</p> <p>Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Row</code> <p>A tuple containing the data returned by the database,     where each row is a tuple and each column is a value in the tuple.</p> <p>Examples:</p> <p>Execute operation with parameters, fetching one new row at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_one(operation, parameters=parameters)\n        print(result)\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def fetch_one(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; \"Row\":\n\"\"\"\n    Fetch a single result from the database.\n\n    Repeated calls using the same inputs to *any* of the fetch methods of this\n    block will skip executing the operation again, and instead,\n    return the next set of results from the previous execution,\n    until the reset_cursors method is called.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Returns:\n        A tuple containing the data returned by the database,\n            where each row is a tuple and each column is a value in the tuple.\n\n    Examples:\n        Execute operation with parameters, fetching one new row at a time:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = %(corpus)s\n                AND word_count &gt;= %(min_word_count)s\n                ORDER BY word_count DESC\n                LIMIT 3;\n            '''\n            parameters = {\n                \"corpus\": \"romeoandjuliet\",\n                \"min_word_count\": 250,\n            }\n            for _ in range(0, 3):\n                result = warehouse.fetch_one(operation, parameters=parameters)\n                print(result)\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    new, cursor = self._get_cursor(inputs)\n    if new:\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    result = await run_sync_in_worker_thread(cursor.fetchone)\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.get_connection","title":"<code>get_connection</code>","text":"<p>Get the opened connection to BigQuery.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>def get_connection(self) -&gt; \"Connection\":\n\"\"\"\n    Get the opened connection to BigQuery.\n    \"\"\"\n    return self._connection\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.reset_cursors","title":"<code>reset_cursors</code>","text":"<p>Tries to close all opened cursors.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>def reset_cursors(self) -&gt; None:\n\"\"\"\n    Tries to close all opened cursors.\n    \"\"\"\n    input_hashes = tuple(self._unique_cursors.keys())\n    for input_hash in input_hashes:\n        cursor = self._unique_cursors.pop(input_hash)\n        try:\n            cursor.close()\n        except Exception as exc:\n            self.logger.warning(\n                f\"Failed to close cursor for input hash {input_hash!r}: {exc}\"\n            )\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery-functions","title":"Functions","text":""},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_create_table","title":"<code>bigquery_create_table</code>  <code>async</code>","text":"<p>Creates table in BigQuery.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>Name of a dataset in that the table will be created.</p> required <code>table</code> <code>str</code> <p>Name of a table to create.</p> required <code>schema</code> <code>Optional[List[SchemaField]]</code> <p>Schema to use when creating the table.</p> <code>None</code> <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>clustering_fields</code> <code>List[str]</code> <p>List of fields to cluster the table by.</p> <code>None</code> <code>time_partitioning</code> <code>TimePartitioning</code> <p><code>bigquery.TimePartitioning</code> object specifying a partitioning of the newly created table</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>location</code> <code>str</code> <p>The location of the dataset that will be written to.</p> <code>'US'</code> <code>external_config</code> <code>Optional[ExternalConfig]</code> <p>The external data source.  # noqa</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Table name.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_create_table\nfrom google.cloud.bigquery import SchemaField\n@flow\ndef example_bigquery_create_table_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    schema = [\n        SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"),\n        SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"),\n        SchemaField(\"bool\", field_type=\"BOOLEAN\")\n    ]\n    result = bigquery_create_table(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        schema=schema,\n        gcp_credentials=gcp_credentials\n    )\n    return result\nexample_bigquery_create_table_flow()\n</code></pre> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_create_table(\n    dataset: str,\n    table: str,\n    gcp_credentials: GcpCredentials,\n    schema: Optional[List[\"SchemaField\"]] = None,\n    clustering_fields: List[str] = None,\n    time_partitioning: \"TimePartitioning\" = None,\n    project: Optional[str] = None,\n    location: str = \"US\",\n    external_config: Optional[\"ExternalConfig\"] = None,\n) -&gt; str:\n\"\"\"\n    Creates table in BigQuery.\n    Args:\n        dataset: Name of a dataset in that the table will be created.\n        table: Name of a table to create.\n        schema: Schema to use when creating the table.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        clustering_fields: List of fields to cluster the table by.\n        time_partitioning: `bigquery.TimePartitioning` object specifying a partitioning\n            of the newly created table\n        project: Project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: The location of the dataset that will be written to.\n        external_config: The [external data source](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_table#nested_external_data_configuration).  # noqa\n    Returns:\n        Table name.\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_create_table\n        from google.cloud.bigquery import SchemaField\n        @flow\n        def example_bigquery_create_table_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            schema = [\n                SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"),\n                SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"),\n                SchemaField(\"bool\", field_type=\"BOOLEAN\")\n            ]\n            result = bigquery_create_table(\n                dataset=\"dataset\",\n                table=\"test_table\",\n                schema=schema,\n                gcp_credentials=gcp_credentials\n            )\n            return result\n        example_bigquery_create_table_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Creating %s.%s\", dataset, table)\n\n    if not external_config and not schema:\n        raise ValueError(\"Either a schema or an external config must be provided.\")\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n    try:\n        partial_get_dataset = partial(client.get_dataset, dataset)\n        dataset_ref = await to_thread.run_sync(partial_get_dataset)\n    except NotFound:\n        logger.debug(\"Dataset %s not found, creating\", dataset)\n        partial_create_dataset = partial(client.create_dataset, dataset)\n        dataset_ref = await to_thread.run_sync(partial_create_dataset)\n\n    table_ref = dataset_ref.table(table)\n    try:\n        partial_get_table = partial(client.get_table, table_ref)\n        await to_thread.run_sync(partial_get_table)\n        logger.info(\"%s.%s already exists\", dataset, table)\n    except NotFound:\n        logger.debug(\"Table %s not found, creating\", table)\n        table_obj = Table(table_ref, schema=schema)\n\n        # external data configuration\n        if external_config:\n            table_obj.external_data_configuration = external_config\n\n        # cluster for optimal data sorting/access\n        if clustering_fields:\n            table_obj.clustering_fields = clustering_fields\n\n        # partitioning\n        if time_partitioning:\n            table_obj.time_partitioning = time_partitioning\n\n        partial_create_table = partial(client.create_table, table_obj)\n        await to_thread.run_sync(partial_create_table)\n\n    return table\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_insert_stream","title":"<code>bigquery_insert_stream</code>  <code>async</code>","text":"<p>Insert records in a Google BigQuery table via the streaming API.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>Name of a dataset where the records will be written to.</p> required <code>table</code> <code>str</code> <p>Name of a table to write to.</p> required <code>records</code> <code>List[dict]</code> <p>The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>project</code> <code>Optional[str]</code> <p>The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>location</code> <code>str</code> <p>Location of the dataset that will be written to.</p> <code>'US'</code> <p>Returns:</p> Type Description <code>List</code> <p>List of inserted rows.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_insert_stream\nfrom google.cloud.bigquery import SchemaField\n\n@flow\ndef example_bigquery_insert_stream_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    records = [\n        {\"number\": 1, \"text\": \"abc\", \"bool\": True},\n        {\"number\": 2, \"text\": \"def\", \"bool\": False},\n    ]\n    result = bigquery_insert_stream(\n        dataset=\"integrations\",\n        table=\"test_table\",\n        records=records,\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_insert_stream_flow()\n</code></pre> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_insert_stream(\n    dataset: str,\n    table: str,\n    records: List[dict],\n    gcp_credentials: GcpCredentials,\n    project: Optional[str] = None,\n    location: str = \"US\",\n) -&gt; List:\n\"\"\"\n    Insert records in a Google BigQuery table via the [streaming\n    API](https://cloud.google.com/bigquery/streaming-data-into-bigquery).\n\n    Args:\n        dataset: Name of a dataset where the records will be written to.\n        table: Name of a table to write to.\n        records: The list of records to insert as rows into the BigQuery table;\n            each item in the list should be a dictionary whose keys correspond to\n            columns in the table.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        project: The project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: Location of the dataset that will be written to.\n\n    Returns:\n        List of inserted rows.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_insert_stream\n        from google.cloud.bigquery import SchemaField\n\n        @flow\n        def example_bigquery_insert_stream_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            records = [\n                {\"number\": 1, \"text\": \"abc\", \"bool\": True},\n                {\"number\": 2, \"text\": \"def\", \"bool\": False},\n            ]\n            result = bigquery_insert_stream(\n                dataset=\"integrations\",\n                table=\"test_table\",\n                records=records,\n                gcp_credentials=gcp_credentials\n            )\n            return result\n\n        example_bigquery_insert_stream_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Inserting into %s.%s as a stream\", dataset, table)\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n    table_ref = client.dataset(dataset).table(table)\n    partial_insert = partial(\n        client.insert_rows_json, table=table_ref, json_rows=records\n    )\n    response = await to_thread.run_sync(partial_insert)\n\n    errors = []\n    output = []\n    for row in response:\n        output.append(row)\n        if \"errors\" in row:\n            errors.append(row[\"errors\"])\n\n    if errors:\n        raise ValueError(errors)\n\n    return output\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_cloud_storage","title":"<code>bigquery_load_cloud_storage</code>  <code>async</code>","text":"<p>Run method for this Task.  Invoked by calling this Task within a Flow context, after initialization.</p> <p>Parameters:</p> Name Type Description Default <code>uri</code> <code>str</code> <p>GCS path to load data from.</p> required <code>dataset</code> <code>str</code> <p>The id of a destination dataset to write the records to.</p> required <code>table</code> <code>str</code> <p>The name of a destination table to write the records to.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>schema</code> <code>Optional[List[SchemaField]]</code> <p>The schema to use when creating the table.</p> <code>None</code> <code>job_config</code> <code>Optional[dict]</code> <p>Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected).</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>location</code> <code>str</code> <p>Location of the dataset that will be written to.</p> <code>'US'</code> <p>Returns:</p> Type Description <code>LoadJob</code> <p>The response from <code>load_table_from_uri</code>.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_load_cloud_storage\n\n@flow\ndef example_bigquery_load_cloud_storage_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    result = bigquery_load_cloud_storage(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        uri=\"uri\",\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_load_cloud_storage_flow()\n</code></pre> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_load_cloud_storage(\n    dataset: str,\n    table: str,\n    uri: str,\n    gcp_credentials: GcpCredentials,\n    schema: Optional[List[\"SchemaField\"]] = None,\n    job_config: Optional[dict] = None,\n    project: Optional[str] = None,\n    location: str = \"US\",\n) -&gt; \"LoadJob\":\n\"\"\"\n    Run method for this Task.  Invoked by _calling_ this\n    Task within a Flow context, after initialization.\n    Args:\n        uri: GCS path to load data from.\n        dataset: The id of a destination dataset to write the records to.\n        table: The name of a destination table to write the records to.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        schema: The schema to use when creating the table.\n        job_config: Dictionary of job configuration parameters;\n            note that the parameters provided here must be pickleable\n            (e.g., dataset references will be rejected).\n        project: The project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: Location of the dataset that will be written to.\n\n    Returns:\n        The response from `load_table_from_uri`.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_load_cloud_storage\n\n        @flow\n        def example_bigquery_load_cloud_storage_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            result = bigquery_load_cloud_storage(\n                dataset=\"dataset\",\n                table=\"test_table\",\n                uri=\"uri\",\n                gcp_credentials=gcp_credentials\n            )\n            return result\n\n        example_bigquery_load_cloud_storage_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Loading into %s.%s from cloud storage\", dataset, table)\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n    table_ref = client.dataset(dataset).table(table)\n\n    job_config = job_config or {}\n    if \"autodetect\" not in job_config:\n        job_config[\"autodetect\"] = True\n    job_config = LoadJobConfig(**job_config)\n    if schema:\n        job_config.schema = schema\n\n    result = None\n    try:\n        partial_load = partial(\n            _result_sync,\n            client.load_table_from_uri,\n            uri,\n            table_ref,\n            job_config=job_config,\n        )\n        result = await to_thread.run_sync(partial_load)\n    except Exception as exception:\n        logger.exception(exception)\n        if result is not None and result.errors is not None:\n            for error in result.errors:\n                logger.exception(error)\n        raise\n\n    if result is not None:\n        # remove unpickleable attributes\n        result._client = None\n        result._completion_lock = None\n\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_file","title":"<code>bigquery_load_file</code>  <code>async</code>","text":"<p>Loads file into BigQuery.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization.</p> required <code>table</code> <code>str</code> <p>Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization.</p> required <code>path</code> <code>Union[str, pathlib.Path]</code> <p>A string or path-like object of the file to be loaded.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>schema</code> <code>Optional[List[SchemaField]]</code> <p>Schema to use when creating the table.</p> <code>None</code> <code>job_config</code> <code>Optional[dict]</code> <p>An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected).</p> <code>None</code> <code>rewind</code> <code>bool</code> <p>if True, seek to the beginning of the file handle before reading the file.</p> <code>False</code> <code>size</code> <code>Optional[int]</code> <p>Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used.</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>location</code> <code>str</code> <p>location of the dataset that will be written to.</p> <code>'US'</code> <p>Returns:</p> Type Description <code>LoadJob</code> <p>The response from <code>load_table_from_file</code>.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_load_file\nfrom google.cloud.bigquery import SchemaField\n\n@flow\ndef example_bigquery_load_file_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    result = bigquery_load_file(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        path=\"path\",\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_load_file_flow()\n</code></pre> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_load_file(\n    dataset: str,\n    table: str,\n    path: Union[str, Path],\n    gcp_credentials: GcpCredentials,\n    schema: Optional[List[\"SchemaField\"]] = None,\n    job_config: Optional[dict] = None,\n    rewind: bool = False,\n    size: Optional[int] = None,\n    project: Optional[str] = None,\n    location: str = \"US\",\n) -&gt; \"LoadJob\":\n\"\"\"\n    Loads file into BigQuery.\n\n    Args:\n        dataset: ID of a destination dataset to write the records to;\n            if not provided here, will default to the one provided at initialization.\n        table: Name of a destination table to write the records to;\n            if not provided here, will default to the one provided at initialization.\n        path: A string or path-like object of the file to be loaded.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        schema: Schema to use when creating the table.\n        job_config: An optional dictionary of job configuration parameters;\n            note that the parameters provided here must be pickleable\n            (e.g., dataset references will be rejected).\n        rewind: if True, seek to the beginning of the file handle\n            before reading the file.\n        size: Number of bytes to read from the file handle. If size is None or large,\n            resumable upload will be used. Otherwise, multipart upload will be used.\n        project: Project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: location of the dataset that will be written to.\n\n    Returns:\n        The response from `load_table_from_file`.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_load_file\n        from google.cloud.bigquery import SchemaField\n\n        @flow\n        def example_bigquery_load_file_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            result = bigquery_load_file(\n                dataset=\"dataset\",\n                table=\"test_table\",\n                path=\"path\",\n                gcp_credentials=gcp_credentials\n            )\n            return result\n\n        example_bigquery_load_file_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Loading into %s.%s from file\", dataset, table)\n\n    if not os.path.exists(path):\n        raise ValueError(f\"{path} does not exist\")\n    elif not os.path.isfile(path):\n        raise ValueError(f\"{path} is not a file\")\n\n    client = gcp_credentials.get_bigquery_client(project=project)\n    table_ref = client.dataset(dataset).table(table)\n\n    job_config = job_config or {}\n    if \"autodetect\" not in job_config:\n        job_config[\"autodetect\"] = True\n        # TODO: test if autodetect is needed when schema is passed\n    job_config = LoadJobConfig(**job_config)\n    if schema:\n        # TODO: test if schema can be passed directly in job_config\n        job_config.schema = schema\n\n    try:\n        with open(path, \"rb\") as file_obj:\n            partial_load = partial(\n                _result_sync,\n                client.load_table_from_file,\n                file_obj,\n                table_ref,\n                rewind=rewind,\n                size=size,\n                location=location,\n                job_config=job_config,\n            )\n            result = await to_thread.run_sync(partial_load)\n    except IOError:\n        logger.exception(f\"Could not open and read from {path}\")\n        raise\n\n    if result is not None:\n        # remove unpickleable attributes\n        result._client = None\n        result._completion_lock = None\n\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_query","title":"<code>bigquery_query</code>  <code>async</code>","text":"<p>Runs a BigQuery query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>String of the query to execute.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>query_params</code> <code>Optional[List[tuple]]</code> <p>List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported.  See the Google documentation for more details on how both the query and the query parameters should be formatted.</p> <code>None</code> <code>dry_run_max_bytes</code> <code>Optional[int]</code> <p>If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a <code>ValueError</code> if the maximum is exceeded.</p> <code>None</code> <code>dataset</code> <code>Optional[str]</code> <p>Name of a destination dataset to write the query results to, if you don't want them returned; if provided, <code>table</code> must also be provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Name of a destination table to write the query results to, if you don't want them returned; if provided, <code>dataset</code> must also be provided.</p> <code>None</code> <code>to_dataframe</code> <code>bool</code> <p>If provided, returns the results of the query as a pandas dataframe instead of a list of <code>bigquery.table.Row</code> objects.</p> <code>False</code> <code>job_config</code> <code>Optional[dict]</code> <p>Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected).</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>location</code> <code>str</code> <p>Location of the dataset that will be queried.</p> <code>'US'</code> <p>Returns:</p> Type Description <code>List[Row]</code> <p>A list of rows, or pandas DataFrame if to_dataframe, matching the query criteria.</p> <p>Examples:</p> <p>Queries the public names database, returning 10 results. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_query\n\n@flow\ndef example_bigquery_query_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\",\n        project=\"project\"\n    )\n    query = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = @corpus\n        AND word_count &gt;= @min_word_count\n        ORDER BY word_count DESC;\n    '''\n    query_params = [\n        (\"corpus\", \"STRING\", \"romeoandjuliet\"),\n        (\"min_word_count\", \"INT64\", 250)\n    ]\n    result = bigquery_query(\n        query, gcp_credentials, query_params=query_params\n    )\n    return result\n\nexample_bigquery_query_flow()\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_query(\n    query: str,\n    gcp_credentials: GcpCredentials,\n    query_params: Optional[List[tuple]] = None,  # 3-tuples\n    dry_run_max_bytes: Optional[int] = None,\n    dataset: Optional[str] = None,\n    table: Optional[str] = None,\n    to_dataframe: bool = False,\n    job_config: Optional[dict] = None,\n    project: Optional[str] = None,\n    location: str = \"US\",\n) -&gt; List[\"Row\"]:\n\"\"\"\n    Runs a BigQuery query.\n\n    Args:\n        query: String of the query to execute.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        query_params: List of 3-tuples specifying BigQuery query parameters; currently\n            only scalar query parameters are supported.  See the\n            [Google documentation](https://cloud.google.com/bigquery/docs/parameterized-queries#bigquery-query-params-python)\n            for more details on how both the query and the query parameters should be formatted.\n        dry_run_max_bytes: If provided, the maximum number of bytes the query\n            is allowed to process; this will be determined by executing a dry run\n            and raising a `ValueError` if the maximum is exceeded.\n        dataset: Name of a destination dataset to write the query results to,\n            if you don't want them returned; if provided, `table` must also be provided.\n        table: Name of a destination table to write the query results to,\n            if you don't want them returned; if provided, `dataset` must also be provided.\n        to_dataframe: If provided, returns the results of the query as a pandas\n            dataframe instead of a list of `bigquery.table.Row` objects.\n        job_config: Dictionary of job configuration parameters;\n            note that the parameters provided here must be pickleable\n            (e.g., dataset references will be rejected).\n        project: The project to initialize the BigQuery Client with; if not\n            provided, will default to the one inferred from your credentials.\n        location: Location of the dataset that will be queried.\n\n    Returns:\n        A list of rows, or pandas DataFrame if to_dataframe,\n        matching the query criteria.\n\n    Example:\n        Queries the public names database, returning 10 results.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_query\n\n        @flow\n        def example_bigquery_query_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\",\n                project=\"project\"\n            )\n            query = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = @corpus\n                AND word_count &gt;= @min_word_count\n                ORDER BY word_count DESC;\n            '''\n            query_params = [\n                (\"corpus\", \"STRING\", \"romeoandjuliet\"),\n                (\"min_word_count\", \"INT64\", 250)\n            ]\n            result = bigquery_query(\n                query, gcp_credentials, query_params=query_params\n            )\n            return result\n\n        example_bigquery_query_flow()\n        ```\n    \"\"\"  # noqa\n    logger = get_run_logger()\n    logger.info(\"Running BigQuery query\")\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n\n    # setup job config\n    job_config = QueryJobConfig(**job_config or {})\n    if query_params is not None:\n        job_config.query_parameters = [ScalarQueryParameter(*qp) for qp in query_params]\n\n    # perform dry_run if requested\n    if dry_run_max_bytes is not None:\n        saved_info = dict(\n            dry_run=job_config.dry_run, use_query_cache=job_config.use_query_cache\n        )\n        job_config.dry_run = True\n        job_config.use_query_cache = False\n        partial_query = partial(client.query, query, job_config=job_config)\n        response = await to_thread.run_sync(partial_query)\n        total_bytes_processed = response.total_bytes_processed\n        if total_bytes_processed &gt; dry_run_max_bytes:\n            raise RuntimeError(\n                f\"Query will process {total_bytes_processed} bytes which is above \"\n                f\"the set maximum of {dry_run_max_bytes} for this task.\"\n            )\n        job_config.dry_run = saved_info[\"dry_run\"]\n        job_config.use_query_cache = saved_info[\"use_query_cache\"]\n\n    # if writing to a destination table\n    if dataset is not None:\n        table_ref = client.dataset(dataset).table(table)\n        job_config.destination = table_ref\n\n    partial_query = partial(\n        _result_sync,\n        client.query,\n        query,\n        job_config=job_config,\n    )\n    result = await to_thread.run_sync(partial_query)\n    if to_dataframe:\n        return result.to_dataframe()\n    else:\n        return list(result)\n</code></pre>"},{"location":"blocks_catalog/","title":"Blocks Catalog","text":"<p>Below is a list of Blocks available for registration in <code>prefect-gcp</code>.</p> <p>To register blocks in this module to view and edit them on Prefect Cloud: <pre><code>prefect block register -m prefect_gcp\n</code></pre> Note, to use the <code>load</code> method on Blocks, you must already have a block document saved through code or saved through the UI.</p>"},{"location":"blocks_catalog/#credentials-module","title":"Credentials Module","text":"<p>GcpCredentials</p> <p>Block used to manage authentication with GCP. Google authentication is handled via the <code>google.oauth2</code> module or through the CLI. Specify either one of service <code>account_file</code> or <code>service_account_info</code>; if both are not specified, the client will try to detect the credentials following Google's Application Default Credentials. See Google's Authentication documentation for details on inference and recommended authentication patterns.</p> <p>To load the GcpCredentials: <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow\ndef my_flow():\n    my_block = GcpCredentials.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Credentials Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#bigquery-module","title":"Bigquery Module","text":"<p>BigQueryWarehouse</p> <p>A block for querying a database with BigQuery.</p> <p>Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called.</p> <p>It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited.</p> <p>It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost.</p> <p>To load the BigQueryWarehouse: <pre><code>from prefect import flow\nfrom prefect_gcp.bigquery import BigQueryWarehouse\n\n@flow\ndef my_flow():\n    my_block = BigQueryWarehouse.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Bigquery Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#aiplatform-module","title":"Aiplatform Module","text":"<p>VertexAICustomTrainingJob</p> <p>Infrastructure block used to run Vertex AI custom training jobs.</p> <p>To load the VertexAICustomTrainingJob: <pre><code>from prefect import flow\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\n@flow\ndef my_flow():\n    my_block = VertexAICustomTrainingJob.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Aiplatform Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#cloud-storage-module","title":"Cloud Storage Module","text":"<p>GcsBucket</p> <p>Block used to store data using GCP Cloud Storage Buckets.</p> <p>Note! <code>GcsBucket</code> in <code>prefect-gcp</code> is a unique block, separate from <code>GCS</code> in core Prefect. <code>GcsBucket</code> does not use <code>gcsfs</code> under the hood, instead using the <code>google-cloud-storage</code> package, and offers more configuration and functionality.</p> <p>To load the GcsBucket: <pre><code>from prefect import flow\nfrom prefect_gcp.cloud_storage import GcsBucket\n\n@flow\ndef my_flow():\n    my_block = GcsBucket.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Cloud Storage Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#cloud-run-module","title":"Cloud Run Module","text":"<p>CloudRunJob</p> <p>Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.</p> <p>To load the CloudRunJob: <pre><code>from prefect import flow\nfrom prefect_gcp.cloud_run import CloudRunJob\n\n@flow\ndef my_flow():\n    my_block = CloudRunJob.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Cloud Run Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#secret-manager-module","title":"Secret Manager Module","text":"<p>GcpSecret</p> <p>Manages a secret in Google Cloud Platform's Secret Manager.</p> <p>To load the GcpSecret: <pre><code>from prefect import flow\nfrom prefect_gcp.secret_manager import GcpSecret\n\n@flow\ndef my_flow():\n    my_block = GcpSecret.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Secret Manager Module under Examples Catalog.</p>"},{"location":"cloud_run/","title":"Cloud Run","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run","title":"<code>prefect_gcp.cloud_run</code>","text":"<p>Integrations with Google Cloud Run Job.</p> <p>Note this module is experimental. The intefaces within may change without notice.</p> <p>Examples:</p> <p>Run a job using Google Cloud Run Jobs: <pre><code>CloudRunJob(\n    image=\"gcr.io/my-project/my-image\",\n    region=\"us-east1\",\n    credentials=my_gcp_credentials\n).run()\n</code></pre></p> <p>Run a job that runs the command <code>echo hello world</code> using Google Cloud Run Jobs: <pre><code>CloudRunJob(\n    image=\"gcr.io/my-project/my-image\",\n    region=\"us-east1\",\n    credentials=my_gcp_credentials\n    command=[\"echo\", \"hello world\"]\n).run()\n</code></pre></p>"},{"location":"cloud_run/#prefect_gcp.cloud_run-classes","title":"Classes","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob","title":"<code> CloudRunJob            (Infrastructure)         </code>  <code>pydantic-model</code>","text":"<p>Infrastructure block used to run GCP Cloud Run Jobs.</p> <p>Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project.</p> <p>Note this block is experimental. The interface may change without notice.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class CloudRunJob(Infrastructure):\n\"\"\"\n    &lt;span class=\"badge-api experimental\"/&gt;\n\n    Infrastructure block used to run GCP Cloud Run Jobs.\n\n    Project name information is provided by the Credentials object, and should always\n    be correct as long as the Credentials object is for the correct project.\n\n    Note this block is experimental. The interface may change without notice.\n    \"\"\"\n\n    _block_type_slug = \"cloud-run-job\"\n    _block_type_name = \"GCP Cloud Run Job\"\n    _description = \"Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.\"  # noqa\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/cloud_run/#prefect_gcp.cloud_run.CloudRunJob\"  # noqa: E501\n\n    type: Literal[\"cloud-run-job\"] = Field(\n        \"cloud-run-job\", description=\"The slug for this task type.\"\n    )\n    image: str = Field(\n        ...,\n        title=\"Image Name\",\n        description=(\n            \"The image to use for a new Cloud Run Job. This value must \"\n            \"refer to an image within either Google Container Registry \"\n            \"or Google Artifact Registry, like `gcr.io/&lt;project_name&gt;/&lt;repo&gt;/`.\"\n        ),\n    )\n    region: str = Field(..., description=\"The region where the Cloud Run Job resides.\")\n    credentials: GcpCredentials  # cannot be Field; else it shows as Json\n\n    # Job settings\n    cpu: Optional[int] = Field(\n        default=None,\n        title=\"CPU\",\n        description=(\n            \"The amount of compute allocated to the Cloud Run Job. \"\n            \"The int must be valid based on the rules specified at \"\n            \"https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .\"\n        ),\n    )\n    memory: Optional[int] = Field(\n        default=None,\n        title=\"Memory\",\n        description=\"The amount of memory allocated to the Cloud Run Job.\",\n    )\n    memory_unit: Optional[Literal[\"G\", \"Gi\", \"M\", \"Mi\"]] = Field(\n        default=None,\n        title=\"Memory Units\",\n        description=(\n            \"The unit of memory. See \"\n            \"https://cloud.google.com/run/docs/configuring/memory-limits#setting \"\n            \"for additional details.\"\n        ),\n    )\n    vpc_connector_name: Optional[str] = Field(\n        default=None,\n        title=\"VPC Connector Name\",\n        description=\"The name of the VPC connector to use for the Cloud Run Job.\",\n    )\n    args: Optional[List[str]] = Field(\n        default=None,\n        description=(\n            \"Arguments to be passed to your Cloud Run Job's entrypoint command.\"\n        ),\n    )\n    env: Dict[str, str] = Field(\n        default_factory=dict,\n        description=\"Environment variables to be passed to your Cloud Run Job.\",\n    )\n\n    # Cleanup behavior\n    keep_job: Optional[bool] = Field(\n        default=False,\n        title=\"Keep Job After Completion\",\n        description=\"Keep the completed Cloud Run Job on Google Cloud Platform.\",\n    )\n    timeout: Optional[int] = Field(\n        default=600,\n        gt=0,\n        le=3600,\n        title=\"Job Timeout\",\n        description=(\n            \"The length of time that Prefect will wait for a Cloud Run Job to complete \"\n            \"before raising an exception.\"\n        ),\n    )\n    # For private use\n    _job_name: str = None\n    _execution: Optional[Execution] = None\n\n    @property\n    def job_name(self):\n\"\"\"Create a unique and valid job name.\"\"\"\n\n        if self._job_name is None:\n            # get `repo` from `gcr.io/&lt;project_name&gt;/repo/other`\n            components = self.image.split(\"/\")\n            image_name = components[2]\n            # only alphanumeric and '-' allowed for a job name\n            modified_image_name = image_name.replace(\":\", \"-\").replace(\".\", \"-\")\n            # make 50 char limit for final job name, which will be '&lt;name&gt;-&lt;uuid&gt;'\n            if len(modified_image_name) &gt; 17:\n                modified_image_name = modified_image_name[:17]\n            name = f\"{modified_image_name}-{uuid4().hex}\"\n            self._job_name = name\n\n        return self._job_name\n\n    @property\n    def memory_string(self):\n\"\"\"Returns the string expected for memory resources argument.\"\"\"\n        if self.memory and self.memory_unit:\n            return str(self.memory) + self.memory_unit\n        return None\n\n    @validator(\"image\")\n    def _remove_image_spaces(cls, value):\n\"\"\"Deal with spaces in image names.\"\"\"\n        if value is not None:\n            return value.strip()\n\n    @root_validator\n    def _check_valid_memory(cls, values):\n\"\"\"Make sure memory conforms to expected values for API.\n        See: https://cloud.google.com/run/docs/configuring/memory-limits#setting\n        \"\"\"  # noqa\n        if (values.get(\"memory\") is not None and values.get(\"memory_unit\") is None) or (\n            values.get(\"memory_unit\") is not None and values.get(\"memory\") is None\n        ):\n            raise ValueError(\n                \"A memory value and unit must both be supplied to specify a memory\"\n                \" value other than the default memory value.\"\n            )\n        return values\n\n    def _create_job_error(self, exc):\n\"\"\"Provides a nicer error for 404s when trying to create a Cloud Run Job.\"\"\"\n        # TODO consider lookup table instead of the if/else,\n        # also check for documented errors\n        if exc.status_code == 404:\n            raise RuntimeError(\n                f\"Failed to find resources at {exc.uri}. Confirm that region\"\n                f\" '{self.region}' is the correct region for your Cloud Run Job and\"\n                f\" that {self.credentials.project} is the correct GCP project. If\"\n                f\" your project ID is not correct, you are using a Credentials block\"\n                f\" with permissions for the wrong project.\"\n            ) from exc\n        raise exc\n\n    def _job_run_submission_error(self, exc):\n\"\"\"Provides a nicer error for 404s when submitting job runs.\"\"\"\n        if exc.status_code == 404:\n            pat1 = r\"The requested URL [^ ]+ was not found on this server\"\n            # pat2 = (\n            #     r\"Resource '[^ ]+' of kind 'JOB' in region '[\\w\\-0-9]+' \"\n            #     r\"in project '[\\w\\-0-9]+' does not exist\"\n            # )\n            if re.findall(pat1, str(exc)):\n                raise RuntimeError(\n                    f\"Failed to find resources at {exc.uri}. \"\n                    f\"Confirm that region '{self.region}' is \"\n                    f\"the correct region for your Cloud Run Job \"\n                    f\"and that '{self.credentials.project}' is the \"\n                    f\"correct GCP project. If your project ID is not \"\n                    f\"correct, you are using a Credentials \"\n                    f\"block with permissions for the wrong project.\"\n                ) from exc\n            else:\n                raise exc\n\n        raise exc\n\n    def _cpu_as_k8s_quantity(self) -&gt; str:\n\"\"\"Return the CPU integer in the format expected by GCP Cloud Run Jobs API.\n        See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n        See also: https://cloud.google.com/run/docs/configuring/cpu#setting-jobs\n        \"\"\"  # noqa\n        return str(self.cpu * 1000) + \"m\"\n\n    @sync_compatible\n    async def run(self, task_status: Optional[TaskStatus] = None):\n\"\"\"Run the configured job on a Google Cloud Run Job.\"\"\"\n        with self._get_client() as client:\n            await run_sync_in_worker_thread(\n                self._create_job_and_wait_for_registration, client\n            )\n            job_execution = await run_sync_in_worker_thread(\n                self._begin_job_execution, client\n            )\n\n            if task_status:\n                task_status.started(self.job_name)\n\n            result = await run_sync_in_worker_thread(\n                self._watch_job_execution_and_get_result,\n                client,\n                job_execution,\n                5,\n            )\n            return result\n\n    @sync_compatible\n    async def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n\"\"\"\n        Kill a task running Cloud Run.\n\n        Args:\n            identifier: The Cloud Run Job name. This should match a\n                value yielded by CloudRunJob.run.\n        \"\"\"\n        if grace_seconds != 30:\n            self.logger.warning(\n                f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n                \"support dynamic grace period configuration. See here for more info: \"\n                \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\"  # noqa\n            )\n\n        with self._get_client() as client:\n            await run_sync_in_worker_thread(\n                self._kill_job,\n                client=client,\n                namespace=self.credentials.project,\n                job_name=identifier,\n            )\n\n    def _kill_job(self, client: Resource, namespace: str, job_name: str) -&gt; None:\n\"\"\"\n        Thin wrapper around Job.delete, wrapping a try/except since\n        Job is an independent class that doesn't have knowledge of\n        CloudRunJob and its associated logic.\n        \"\"\"\n        try:\n            Job.delete(client=client, namespace=namespace, job_name=job_name)\n        except Exception as exc:\n            if \"does not exist\" in str(exc):\n                raise InfrastructureNotFound(\n                    f\"Cannot stop Cloud Run Job; the job name {job_name!r} \"\n                    \"could not be found.\"\n                ) from exc\n            raise\n\n    def _create_job_and_wait_for_registration(self, client: Resource) -&gt; None:\n\"\"\"Create a new job wait for it to finish registering.\"\"\"\n        try:\n            self.logger.info(f\"Creating Cloud Run Job {self.job_name}\")\n            Job.create(\n                client=client,\n                namespace=self.credentials.project,\n                body=self._jobs_body(),\n            )\n        except googleapiclient.errors.HttpError as exc:\n            self._create_job_error(exc)\n\n        try:\n            self._wait_for_job_creation(client=client, timeout=self.timeout)\n        except Exception:\n            self.logger.exception(\n                \"Encountered an exception while waiting for job run creation\"\n            )\n            if not self.keep_job:\n                self.logger.info(\n                    f\"Deleting Cloud Run Job {self.job_name} from Google Cloud Run.\"\n                )\n                try:\n                    Job.delete(\n                        client=client,\n                        namespace=self.credentials.project,\n                        job_name=self.job_name,\n                    )\n                except Exception:\n                    self.logger.exception(\n                        \"Received an unexpected exception while attempting to delete\"\n                        f\" Cloud Run Job {self.job_name!r}\"\n                    )\n            raise\n\n    def _begin_job_execution(self, client: Resource) -&gt; Execution:\n\"\"\"Submit a job run for execution and return the execution object.\"\"\"\n        try:\n            self.logger.info(\n                f\"Submitting Cloud Run Job {self.job_name!r} for execution.\"\n            )\n            submission = Job.run(\n                client=client,\n                namespace=self.credentials.project,\n                job_name=self.job_name,\n            )\n\n            job_execution = Execution.get(\n                client=client,\n                namespace=submission[\"metadata\"][\"namespace\"],\n                execution_name=submission[\"metadata\"][\"name\"],\n            )\n\n            command = (\n                \" \".join(self.command) if self.command else \"default container command\"\n            )\n\n            self.logger.info(\n                f\"Cloud Run Job {self.job_name!r}: Running command {command!r}\"\n            )\n        except Exception as exc:\n            self._job_run_submission_error(exc)\n\n        return job_execution\n\n    def _watch_job_execution_and_get_result(\n        self, client: Resource, execution: Execution, poll_interval: int\n    ) -&gt; CloudRunJobResult:\n\"\"\"Wait for execution to complete and then return result.\"\"\"\n        try:\n            job_execution = self._watch_job_execution(\n                client=client,\n                job_execution=execution,\n                timeout=self.timeout,\n                poll_interval=poll_interval,\n            )\n        except Exception:\n            self.logger.exception(\n                \"Received an unexpected exception while monitoring Cloud Run Job \"\n                f\"{self.job_name!r}\"\n            )\n            raise\n\n        if job_execution.succeeded():\n            status_code = 0\n            self.logger.info(f\"Job Run {self.job_name} completed successfully\")\n        else:\n            status_code = 1\n            error_msg = job_execution.condition_after_completion()[\"message\"]\n            self.logger.error(\n                f\"Job Run {self.job_name} did not complete successfully. {error_msg}\"\n            )\n\n        self.logger.info(\n            f\"Job Run logs can be found on GCP at: {job_execution.log_uri}\"\n        )\n\n        if not self.keep_job:\n            self.logger.info(\n                f\"Deleting completed Cloud Run Job {self.job_name!r} from Google Cloud\"\n                \" Run...\"\n            )\n            try:\n                Job.delete(\n                    client=client,\n                    namespace=self.credentials.project,\n                    job_name=self.job_name,\n                )\n            except Exception:\n                self.logger.exception(\n                    \"Received an unexpected exception while attempting to delete Cloud\"\n                    f\" Run Job {self.job_name}\"\n                )\n\n        return CloudRunJobResult(identifier=self.job_name, status_code=status_code)\n\n    def _jobs_body(self) -&gt; dict:\n\"\"\"Create properly formatted body used for a Job CREATE request.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs\n        \"\"\"\n        jobs_metadata = {\n            \"name\": self.job_name,\n            \"annotations\": {\n                # See: https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation  # noqa\n                \"run.googleapis.com/launch-stage\": \"BETA\",\n            },\n        }\n        if self.vpc_connector_name:\n            jobs_metadata[\"annotations\"][\n                \"run.googleapis.com/vpc-access-connector\"\n            ] = self.vpc_connector_name\n\n        # env and command here\n        containers = [self._add_container_settings({\"image\": self.image})]\n\n        # apply this timeout to each task\n        timeout_seconds = str(self.timeout)\n\n        body = {\n            \"apiVersion\": \"run.googleapis.com/v1\",\n            \"kind\": \"Job\",\n            \"metadata\": jobs_metadata,\n            \"spec\": {  # JobSpec\n                \"template\": {  # ExecutionTemplateSpec\n                    \"spec\": {  # ExecutionSpec\n                        \"template\": {  # TaskTemplateSpec\n                            \"spec\": {\n                                \"containers\": containers,\n                                \"timeoutSeconds\": timeout_seconds,\n                            }  # TaskSpec\n                        }\n                    },\n                }\n            },\n        }\n        return body\n\n    def preview(self) -&gt; str:\n\"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n        body = self._jobs_body()\n        container_settings = body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n            \"containers\"\n        ][0][\"env\"]\n        body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"env\"] = [\n            container_setting\n            for container_setting in container_settings\n            if container_setting[\"name\"] != \"PREFECT_API_KEY\"\n        ]\n        return json.dumps(body, indent=2)\n\n    def _watch_job_execution(\n        self, client, job_execution: Execution, timeout: int, poll_interval: int = 5\n    ):\n\"\"\"\n        Update job_execution status until it is no longer running or timeout is reached.\n        \"\"\"\n        t0 = time.time()\n        while job_execution.is_running():\n            job_execution = Execution.get(\n                client=client,\n                namespace=job_execution.namespace,\n                execution_name=job_execution.name,\n            )\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while waiting for Cloud Run Job \"\n                    \"execution to complete. Your job may still be running on GCP.\"\n                )\n\n            time.sleep(poll_interval)\n\n        return job_execution\n\n    def _wait_for_job_creation(\n        self, client: Resource, timeout: int, poll_interval: int = 5\n    ):\n\"\"\"Give created job time to register.\"\"\"\n        job = Job.get(\n            client=client, namespace=self.credentials.project, job_name=self.job_name\n        )\n\n        t0 = time.time()\n        while not job.is_ready():\n            ready_condition = (\n                job.ready_condition\n                if job.ready_condition\n                else \"waiting for condition update\"\n            )\n            self.logger.info(\n                f\"Job is not yet ready... Current condition: {ready_condition}\"\n            )\n            job = Job.get(\n                client=client,\n                namespace=self.credentials.project,\n                job_name=self.job_name,\n            )\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while waiting for Cloud Run Job \"\n                    \"execution to complete. Your job may still be running on GCP.\"\n                )\n\n            time.sleep(poll_interval)\n\n    def _get_client(self) -&gt; Resource:\n\"\"\"Get the base client needed for interacting with GCP APIs.\"\"\"\n        # region needed for 'v1' API\n        api_endpoint = f\"https://{self.region}-run.googleapis.com\"\n        gcp_creds = self.credentials.get_credentials_from_service_account()\n        options = ClientOptions(api_endpoint=api_endpoint)\n\n        return discovery.build(\n            \"run\", \"v1\", client_options=options, credentials=gcp_creds\n        ).namespaces()\n\n    # CONTAINER SETTINGS\n    def _add_container_settings(self, base_settings: Dict[str, Any]) -&gt; Dict[str, Any]:\n\"\"\"\n        Add settings related to containers for Cloud Run Jobs to a dictionary.\n        Includes environment variables, entrypoint command, entrypoint arguments,\n        and cpu and memory limits.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container\n        and https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements\n        \"\"\"  # noqa\n        container_settings = base_settings.copy()\n        container_settings.update(self._add_env())\n        container_settings.update(self._add_resources())\n        container_settings.update(self._add_command())\n        container_settings.update(self._add_args())\n        return container_settings\n\n    def _add_args(self) -&gt; dict:\n\"\"\"Set the arguments that will be passed to the entrypoint for a Cloud Run Job.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container\n        \"\"\"  # noqa\n        return {\"args\": self.args} if self.args else {}\n\n    def _add_command(self) -&gt; dict:\n\"\"\"Set the command that a container will run for a Cloud Run Job.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container\n        \"\"\"  # noqa\n        return {\"command\": self.command}\n\n    def _add_resources(self) -&gt; dict:\n\"\"\"Set specified resources limits for a Cloud Run Job.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements\n        See also: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n        \"\"\"  # noqa\n        resources = {\"limits\": {}, \"requests\": {}}\n\n        if self.cpu is not None:\n            cpu = self._cpu_as_k8s_quantity()\n            resources[\"limits\"][\"cpu\"] = cpu\n            resources[\"requests\"][\"cpu\"] = cpu\n        if self.memory_string is not None:\n            resources[\"limits\"][\"memory\"] = self.memory_string\n            resources[\"requests\"][\"memory\"] = self.memory_string\n\n        return {\"resources\": resources} if resources[\"requests\"] else {}\n\n    def _add_env(self) -&gt; dict:\n\"\"\"Add environment variables for a Cloud Run Job.\n\n        Method `self._base_environment()` gets necessary Prefect environment variables\n        from the config.\n\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container#envvar for\n        how environment variables are specified for Cloud Run Jobs.\n        \"\"\"  # noqa\n        env = {**self._base_environment(), **self.env}\n        cloud_run_env = [{\"name\": k, \"value\": v} for k, v in env.items()]\n        return {\"env\": cloud_run_env}\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob-attributes","title":"Attributes","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.args","title":"<code>args: List[str]</code>  <code>pydantic-field</code>","text":"<p>Arguments to be passed to your Cloud Run Job's entrypoint command.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.cpu","title":"<code>cpu: int</code>  <code>pydantic-field</code>","text":"<p>The amount of compute allocated to the Cloud Run Job. The int must be valid based on the rules specified at https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.image","title":"<code>image: str</code>  <code>pydantic-field</code> <code>required</code>","text":"<p>The image to use for a new Cloud Run Job. This value must refer to an image within either Google Container Registry or Google Artifact Registry, like <code>gcr.io/&lt;project_name&gt;/&lt;repo&gt;/</code>.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.job_name","title":"<code>job_name</code>  <code>property</code> <code>readonly</code>","text":"<p>Create a unique and valid job name.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.keep_job","title":"<code>keep_job: bool</code>  <code>pydantic-field</code>","text":"<p>Keep the completed Cloud Run Job on Google Cloud Platform.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory","title":"<code>memory: int</code>  <code>pydantic-field</code>","text":"<p>The amount of memory allocated to the Cloud Run Job.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory_string","title":"<code>memory_string</code>  <code>property</code> <code>readonly</code>","text":"<p>Returns the string expected for memory resources argument.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory_unit","title":"<code>memory_unit: Literal['G', 'Gi', 'M', 'Mi']</code>  <code>pydantic-field</code>","text":"<p>The unit of memory. See https://cloud.google.com/run/docs/configuring/memory-limits#setting for additional details.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.region","title":"<code>region: str</code>  <code>pydantic-field</code> <code>required</code>","text":"<p>The region where the Cloud Run Job resides.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.timeout","title":"<code>timeout: ConstrainedIntValue</code>  <code>pydantic-field</code>","text":"<p>The length of time that Prefect will wait for a Cloud Run Job to complete before raising an exception.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.vpc_connector_name","title":"<code>vpc_connector_name: str</code>  <code>pydantic-field</code>","text":"<p>The name of the VPC connector to use for the Cloud Run Job.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob-methods","title":"Methods","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.kill","title":"<code>kill</code>  <code>async</code>","text":"<p>Kill a task running Cloud Run.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The Cloud Run Job name. This should match a value yielded by CloudRunJob.run.</p> required Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@sync_compatible\nasync def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n\"\"\"\n    Kill a task running Cloud Run.\n\n    Args:\n        identifier: The Cloud Run Job name. This should match a\n            value yielded by CloudRunJob.run.\n    \"\"\"\n    if grace_seconds != 30:\n        self.logger.warning(\n            f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n            \"support dynamic grace period configuration. See here for more info: \"\n            \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\"  # noqa\n        )\n\n    with self._get_client() as client:\n        await run_sync_in_worker_thread(\n            self._kill_job,\n            client=client,\n            namespace=self.credentials.project,\n            job_name=identifier,\n        )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.preview","title":"<code>preview</code>","text":"<p>Generate a preview of the job definition that will be sent to GCP.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def preview(self) -&gt; str:\n\"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n    body = self._jobs_body()\n    container_settings = body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n        \"containers\"\n    ][0][\"env\"]\n    body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"env\"] = [\n        container_setting\n        for container_setting in container_settings\n        if container_setting[\"name\"] != \"PREFECT_API_KEY\"\n    ]\n    return json.dumps(body, indent=2)\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.run","title":"<code>run</code>  <code>async</code>","text":"<p>Run the configured job on a Google Cloud Run Job.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@sync_compatible\nasync def run(self, task_status: Optional[TaskStatus] = None):\n\"\"\"Run the configured job on a Google Cloud Run Job.\"\"\"\n    with self._get_client() as client:\n        await run_sync_in_worker_thread(\n            self._create_job_and_wait_for_registration, client\n        )\n        job_execution = await run_sync_in_worker_thread(\n            self._begin_job_execution, client\n        )\n\n        if task_status:\n            task_status.started(self.job_name)\n\n        result = await run_sync_in_worker_thread(\n            self._watch_job_execution_and_get_result,\n            client,\n            job_execution,\n            5,\n        )\n        return result\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJobResult","title":"<code> CloudRunJobResult            (InfrastructureResult)         </code>  <code>pydantic-model</code>","text":"<p>Result from a Cloud Run Job.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class CloudRunJobResult(InfrastructureResult):\n\"\"\"Result from a Cloud Run Job.\"\"\"\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution","title":"<code> Execution            (BaseModel)         </code>  <code>pydantic-model</code>","text":"<p>Utility class to call GCP <code>executions</code> API and interact with the returned objects.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class Execution(BaseModel):\n\"\"\"\n    Utility class to call GCP `executions` API and\n    interact with the returned objects.\n    \"\"\"\n\n    name: str\n    namespace: str\n    metadata: dict\n    spec: dict\n    status: dict\n    log_uri: str\n\n    def is_running(self) -&gt; bool:\n\"\"\"Returns True if Execution is not completed.\"\"\"\n        return self.status.get(\"completionTime\") is None\n\n    def condition_after_completion(self):\n\"\"\"Returns Execution condition if Execution has completed.\"\"\"\n        for condition in self.status[\"conditions\"]:\n            if condition[\"type\"] == \"Completed\":\n                return condition\n\n    def succeeded(self):\n\"\"\"Whether or not the Execution completed is a successful state.\"\"\"\n        completed_condition = self.condition_after_completion()\n        if completed_condition and completed_condition[\"status\"] == \"True\":\n            return True\n\n        return False\n\n    @classmethod\n    def get(cls, client: Resource, namespace: str, execution_name: str):\n\"\"\"\n        Make a get request to the GCP executions API\n        and return an Execution instance.\n        \"\"\"\n        request = client.executions().get(\n            name=f\"namespaces/{namespace}/executions/{execution_name}\"\n        )\n        response = request.execute()\n\n        return cls(\n            name=response[\"metadata\"][\"name\"],\n            namespace=response[\"metadata\"][\"namespace\"],\n            metadata=response[\"metadata\"],\n            spec=response[\"spec\"],\n            status=response[\"status\"],\n            log_uri=response[\"status\"][\"logUri\"],\n        )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution-methods","title":"Methods","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.condition_after_completion","title":"<code>condition_after_completion</code>","text":"<p>Returns Execution condition if Execution has completed.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def condition_after_completion(self):\n\"\"\"Returns Execution condition if Execution has completed.\"\"\"\n    for condition in self.status[\"conditions\"]:\n        if condition[\"type\"] == \"Completed\":\n            return condition\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.get","title":"<code>get</code>  <code>classmethod</code>","text":"<p>Make a get request to the GCP executions API and return an Execution instance.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@classmethod\ndef get(cls, client: Resource, namespace: str, execution_name: str):\n\"\"\"\n    Make a get request to the GCP executions API\n    and return an Execution instance.\n    \"\"\"\n    request = client.executions().get(\n        name=f\"namespaces/{namespace}/executions/{execution_name}\"\n    )\n    response = request.execute()\n\n    return cls(\n        name=response[\"metadata\"][\"name\"],\n        namespace=response[\"metadata\"][\"namespace\"],\n        metadata=response[\"metadata\"],\n        spec=response[\"spec\"],\n        status=response[\"status\"],\n        log_uri=response[\"status\"][\"logUri\"],\n    )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.is_running","title":"<code>is_running</code>","text":"<p>Returns True if Execution is not completed.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def is_running(self) -&gt; bool:\n\"\"\"Returns True if Execution is not completed.\"\"\"\n    return self.status.get(\"completionTime\") is None\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.succeeded","title":"<code>succeeded</code>","text":"<p>Whether or not the Execution completed is a successful state.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def succeeded(self):\n\"\"\"Whether or not the Execution completed is a successful state.\"\"\"\n    completed_condition = self.condition_after_completion()\n    if completed_condition and completed_condition[\"status\"] == \"True\":\n        return True\n\n    return False\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job","title":"<code> Job            (BaseModel)         </code>  <code>pydantic-model</code>","text":"<p>Utility class to call GCP <code>jobs</code> API and interact with the returned objects.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class Job(BaseModel):\n\"\"\"\n    Utility class to call GCP `jobs` API and\n    interact with the returned objects.\n    \"\"\"\n\n    metadata: dict\n    spec: dict\n    status: dict\n    name: str\n    ready_condition: dict\n    execution_status: dict\n\n    def _is_missing_container(self):\n\"\"\"\n        Check if Job status is not ready because\n        the specified container cannot be found.\n        \"\"\"\n        if (\n            self.ready_condition.get(\"status\") == \"False\"\n            and self.ready_condition.get(\"reason\") == \"ContainerMissing\"\n        ):\n            return True\n        return False\n\n    def is_ready(self) -&gt; bool:\n\"\"\"Whether a job is finished registering and ready to be executed\"\"\"\n        if self._is_missing_container():\n            raise Exception(f\"{self.ready_condition['message']}\")\n        return self.ready_condition.get(\"status\") == \"True\"\n\n    def has_execution_in_progress(self) -&gt; bool:\n\"\"\"See if job has a run in progress.\"\"\"\n        return (\n            self.execution_status == {}\n            or self.execution_status.get(\"completionTimestamp\") is None\n        )\n\n    @staticmethod\n    def _get_ready_condition(job: dict) -&gt; dict:\n\"\"\"Utility to access JSON field containing ready condition.\"\"\"\n        if job[\"status\"].get(\"conditions\"):\n            for condition in job[\"status\"][\"conditions\"]:\n                if condition[\"type\"] == \"Ready\":\n                    return condition\n\n        return {}\n\n    @staticmethod\n    def _get_execution_status(job: dict):\n\"\"\"Utility to access JSON field containing execution status.\"\"\"\n        if job[\"status\"].get(\"latestCreatedExecution\"):\n            return job[\"status\"][\"latestCreatedExecution\"]\n\n        return {}\n\n    @classmethod\n    def get(cls, client: Resource, namespace: str, job_name: str):\n\"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\"\n        request = client.jobs().get(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n        response = request.execute()\n\n        return cls(\n            metadata=response[\"metadata\"],\n            spec=response[\"spec\"],\n            status=response[\"status\"],\n            name=response[\"metadata\"][\"name\"],\n            ready_condition=cls._get_ready_condition(response),\n            execution_status=cls._get_execution_status(response),\n        )\n\n    @staticmethod\n    def create(client: Resource, namespace: str, body: dict):\n\"\"\"Make a create request to the GCP jobs API.\"\"\"\n        request = client.jobs().create(parent=f\"namespaces/{namespace}\", body=body)\n        response = request.execute()\n        return response\n\n    @staticmethod\n    def delete(client: Resource, namespace: str, job_name: str):\n\"\"\"Make a delete request to the GCP jobs API.\"\"\"\n        request = client.jobs().delete(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n        response = request.execute()\n        return response\n\n    @staticmethod\n    def run(client: Resource, namespace: str, job_name: str):\n\"\"\"Make a run request to the GCP jobs API.\"\"\"\n        request = client.jobs().run(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n        response = request.execute()\n        return response\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job-methods","title":"Methods","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.create","title":"<code>create</code>  <code>staticmethod</code>","text":"<p>Make a create request to the GCP jobs API.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@staticmethod\ndef create(client: Resource, namespace: str, body: dict):\n\"\"\"Make a create request to the GCP jobs API.\"\"\"\n    request = client.jobs().create(parent=f\"namespaces/{namespace}\", body=body)\n    response = request.execute()\n    return response\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.delete","title":"<code>delete</code>  <code>staticmethod</code>","text":"<p>Make a delete request to the GCP jobs API.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@staticmethod\ndef delete(client: Resource, namespace: str, job_name: str):\n\"\"\"Make a delete request to the GCP jobs API.\"\"\"\n    request = client.jobs().delete(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n    response = request.execute()\n    return response\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.get","title":"<code>get</code>  <code>classmethod</code>","text":"<p>Make a get request to the GCP jobs API and return a Job instance.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@classmethod\ndef get(cls, client: Resource, namespace: str, job_name: str):\n\"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\"\n    request = client.jobs().get(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n    response = request.execute()\n\n    return cls(\n        metadata=response[\"metadata\"],\n        spec=response[\"spec\"],\n        status=response[\"status\"],\n        name=response[\"metadata\"][\"name\"],\n        ready_condition=cls._get_ready_condition(response),\n        execution_status=cls._get_execution_status(response),\n    )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.has_execution_in_progress","title":"<code>has_execution_in_progress</code>","text":"<p>See if job has a run in progress.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def has_execution_in_progress(self) -&gt; bool:\n\"\"\"See if job has a run in progress.\"\"\"\n    return (\n        self.execution_status == {}\n        or self.execution_status.get(\"completionTimestamp\") is None\n    )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.is_ready","title":"<code>is_ready</code>","text":"<p>Whether a job is finished registering and ready to be executed</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def is_ready(self) -&gt; bool:\n\"\"\"Whether a job is finished registering and ready to be executed\"\"\"\n    if self._is_missing_container():\n        raise Exception(f\"{self.ready_condition['message']}\")\n    return self.ready_condition.get(\"status\") == \"True\"\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.run","title":"<code>run</code>  <code>staticmethod</code>","text":"<p>Make a run request to the GCP jobs API.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@staticmethod\ndef run(client: Resource, namespace: str, job_name: str):\n\"\"\"Make a run request to the GCP jobs API.\"\"\"\n    request = client.jobs().run(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n    response = request.execute()\n    return response\n</code></pre>"},{"location":"cloud_storage/","title":"Cloud Storage","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage","title":"<code>prefect_gcp.cloud_storage</code>","text":"<p>Tasks for interacting with GCP Cloud Storage.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage-classes","title":"Classes","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket","title":"<code> GcsBucket            (WritableDeploymentStorage, WritableFileSystem, ObjectStorageBlock)         </code>  <code>pydantic-model</code>","text":"<p>Block used to store data using GCP Cloud Storage Buckets.</p> <p>Note! <code>GcsBucket</code> in <code>prefect-gcp</code> is a unique block, separate from <code>GCS</code> in core Prefect. <code>GcsBucket</code> does not use <code>gcsfs</code> under the hood, instead using the <code>google-cloud-storage</code> package, and offers more configuration and functionality.</p> <p>Attributes:</p> Name Type Description <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> <code>gcp_credentials</code> <code>GcpCredentials</code> <p>The credentials to authenticate with GCP.</p> <code>bucket_folder</code> <code>str</code> <p>A default path to a folder within the GCS bucket to use for reading and writing objects.</p> <p>Examples:</p> <p>Load stored GCP Cloud Storage Bucket: <pre><code>from prefect_gcp.cloud_storage import GcsBucket\ngcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>class GcsBucket(WritableDeploymentStorage, WritableFileSystem, ObjectStorageBlock):\n\"\"\"\n    Block used to store data using GCP Cloud Storage Buckets.\n\n    Note! `GcsBucket` in `prefect-gcp` is a unique block, separate from `GCS`\n    in core Prefect. `GcsBucket` does not use `gcsfs` under the hood,\n    instead using the `google-cloud-storage` package, and offers more configuration\n    and functionality.\n\n    Attributes:\n        bucket: Name of the bucket.\n        gcp_credentials: The credentials to authenticate with GCP.\n        bucket_folder: A default path to a folder within the GCS bucket to use\n            for reading and writing objects.\n\n    Example:\n        Load stored GCP Cloud Storage Bucket:\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n        gcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\"  # noqa\n    _block_type_name = \"GCS Bucket\"\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/cloud_storage/#prefect_gcp.cloud_storage.GcsBucket\"  # noqa: E501\n\n    bucket: str = Field(..., description=\"Name of the bucket.\")\n    gcp_credentials: GcpCredentials = Field(\n        default_factory=GcpCredentials,\n        description=\"The credentials to authenticate with GCP.\",\n    )\n    bucket_folder: str = Field(\n        default=\"\",\n        description=(\n            \"A default path to a folder within the GCS bucket to use \"\n            \"for reading and writing objects.\"\n        ),\n    )\n\n    @validator(\"bucket_folder\", pre=True, always=True)\n    def _bucket_folder_suffix(cls, value):\n\"\"\"\n        Ensures that the bucket folder is suffixed with a forward slash.\n        \"\"\"\n        if value != \"\" and not value.endswith(\"/\"):\n            value = f\"{value}/\"\n        return value\n\n    def _resolve_path(self, path: str) -&gt; str:\n\"\"\"\n        A helper function used in write_path to join `self.bucket_folder` and `path`.\n\n        Args:\n            path: Name of the key, e.g. \"file1\". Each object in your\n                bucket has a unique key (or key name).\n\n        Returns:\n            The joined path.\n        \"\"\"\n        path = path or str(uuid4())\n\n        # If bucket_folder provided, it means we won't write to the root dir of\n        # the bucket. So we need to add it on the front of the path.\n        path = (\n            str(PurePosixPath(self.bucket_folder, path)) if self.bucket_folder else path\n        )\n        if path == \".\" or path == \"/\":\n            # client.bucket.list_blobs(prefix=None) is the proper way\n            # of specifying the root folder of the bucket\n            path = None\n        return path\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; List[Union[str, Path]]:\n\"\"\"\n        Copies a folder from the configured GCS bucket to a local directory.\n        Defaults to copying the entire contents of the block's bucket_folder\n        to the current working directory.\n\n        Args:\n            from_path: Path in GCS bucket to download from. Defaults to the block's\n                configured bucket_folder.\n            local_path: Local path to download GCS bucket contents to.\n                Defaults to the current working directory.\n\n        Returns:\n            A list of downloaded file paths.\n        \"\"\"\n        from_path = (\n            self.bucket_folder if from_path is None else self._resolve_path(from_path)\n        )\n\n        if local_path is None:\n            local_path = os.path.abspath(\".\")\n        else:\n            local_path = os.path.abspath(os.path.expanduser(local_path))\n\n        project = self.gcp_credentials.project\n        client = self.gcp_credentials.get_cloud_storage_client(project=project)\n\n        blobs = await run_sync_in_worker_thread(\n            client.list_blobs, self.bucket, prefix=from_path\n        )\n\n        file_paths = []\n        for blob in blobs:\n            blob_path = blob.name\n            if blob_path[-1] == \"/\":\n                # object is a folder and will be created if it contains any objects\n                continue\n            local_file_path = os.path.join(local_path, blob_path)\n            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n            with disable_run_logger():\n                file_path = await cloud_storage_download_blob_to_file.fn(\n                    bucket=self.bucket,\n                    blob=blob_path,\n                    path=local_file_path,\n                    gcp_credentials=self.gcp_credentials,\n                )\n                file_paths.append(file_path)\n        return file_paths\n\n    @sync_compatible\n    async def put_directory(\n        self,\n        local_path: Optional[str] = None,\n        to_path: Optional[str] = None,\n        ignore_file: Optional[str] = None,\n    ) -&gt; int:\n\"\"\"\n        Uploads a directory from a given local path to the configured GCS bucket in a\n        given folder.\n\n        Defaults to uploading the entire contents the current working directory to the\n        block's bucket_folder.\n\n        Args:\n            local_path: Path to local directory to upload from.\n            to_path: Path in GCS bucket to upload to. Defaults to block's configured\n                bucket_folder.\n            ignore_file: Path to file containing gitignore style expressions for\n                filepaths to ignore.\n\n        Returns:\n            The number of files uploaded.\n        \"\"\"\n        if local_path is None:\n            local_path = os.path.abspath(\".\")\n        else:\n            local_path = os.path.expanduser(local_path)\n\n        to_path = self.bucket_folder if to_path is None else self._resolve_path(to_path)\n\n        included_files = None\n        if ignore_file:\n            with open(ignore_file, \"r\") as f:\n                ignore_patterns = f.readlines()\n            included_files = filter_files(local_path, ignore_patterns)\n\n        uploaded_file_count = 0\n        for local_file_path in Path(local_path).rglob(\"*\"):\n            if (\n                included_files is not None\n                and local_file_path.name not in included_files\n            ):\n                continue\n            elif not local_file_path.is_dir():\n                remote_file_path = str(\n                    PurePosixPath(to_path, local_file_path.relative_to(local_path))\n                )\n                local_file_content = local_file_path.read_bytes()\n                await self.write_path(remote_file_path, content=local_file_content)\n                uploaded_file_count += 1\n\n        return uploaded_file_count\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n\"\"\"\n        Read specified path from GCS and return contents. Provide the entire\n        path to the key in GCS.\n\n        Args:\n            path: Entire path to (and including) the key.\n\n        Returns:\n            A bytes or string representation of the blob object.\n        \"\"\"\n        path = self._resolve_path(path)\n        with disable_run_logger():\n            contents = await cloud_storage_download_blob_as_bytes.fn(\n                bucket=self.bucket, blob=path, gcp_credentials=self.gcp_credentials\n            )\n        return contents\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n\"\"\"\n        Writes to an GCS bucket.\n\n        Args:\n            path: The key name. Each object in your bucket has a unique\n                key (or key name).\n            content: What you are uploading to GCS Bucket.\n\n        Returns:\n            The path that the contents were written to.\n        \"\"\"\n        path = self._resolve_path(path)\n        with disable_run_logger():\n            await cloud_storage_upload_blob_from_string.fn(\n                data=content,\n                bucket=self.bucket,\n                blob=path,\n                gcp_credentials=self.gcp_credentials,\n            )\n        return path\n\n    # NEW BLOCK INTERFACE METHODS BELOW\n    def _join_bucket_folder(self, bucket_path: str = \"\") -&gt; str:\n\"\"\"\n        Joins the base bucket folder to the bucket path.\n\n        NOTE: If a method reuses another method in this class, be careful to not\n        call this  twice because it'll join the bucket folder twice.\n        See https://github.com/PrefectHQ/prefect-aws/issues/141 for a past issue.\n        \"\"\"\n        bucket_path = str(bucket_path)\n        if self.bucket_folder != \"\" and bucket_path.startswith(self.bucket_folder):\n            self.logger.info(\n                f\"Bucket path {bucket_path!r} is already prefixed with \"\n                f\"bucket folder {self.bucket_folder!r}; is this intentional?\"\n            )\n\n        bucket_path = str(PurePosixPath(self.bucket_folder) / bucket_path)\n        if bucket_path == \".\" or bucket_path == \"/\":\n            # client.bucket.list_blobs(prefix=None) is the proper way\n            # of specifying the root folder of the bucket\n            bucket_path = None\n        return bucket_path\n\n    @sync_compatible\n    async def get_bucket(self) -&gt; \"Bucket\":\n\"\"\"\n        Returns the bucket object.\n\n        Returns:\n            The bucket object.\n\n        Examples:\n            Get the bucket object.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.get_bucket()\n            ```\n        \"\"\"\n        self.logger.info(f\"Getting bucket {self.bucket!r}.\")\n        client = self.gcp_credentials.get_cloud_storage_client()\n        bucket = await run_sync_in_worker_thread(client.get_bucket, self.bucket)\n        return bucket\n\n    @sync_compatible\n    async def list_blobs(self, folder: str = \"\") -&gt; List[\"Blob\"]:\n\"\"\"\n        Lists all blobs in the bucket that are in a folder.\n        Folders are not included in the output.\n\n        Args:\n            folder: The folder to list blobs from.\n\n        Returns:\n            A list of Blob objects.\n\n        Examples:\n            Get all blobs from a folder named \"prefect\".\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.list_blobs(\"prefect\")\n            ```\n        \"\"\"\n        client = self.gcp_credentials.get_cloud_storage_client()\n\n        bucket_path = self._join_bucket_folder(folder)\n        self.logger.info(f\"Listing blobs in bucket {bucket_path}.\")\n        blobs = await run_sync_in_worker_thread(\n            client.list_blobs, self.bucket, prefix=bucket_path\n        )\n\n        # Ignore folders\n        return [blob for blob in blobs if not blob.name.endswith(\"/\")]\n\n    @sync_compatible\n    async def list_folders(self, folder: str = \"\") -&gt; List[str]:\n\"\"\"\n        Lists all folders and subfolders in the bucket.\n\n        Args:\n            folder: List all folders and subfolders inside given folder.\n\n        Returns:\n            A list of folders.\n\n        Examples:\n            Get all folders from a bucket named \"my-bucket\".\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.list_folders()\n            ```\n\n            Get all folders from a folder called years\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.list_folders('years)\n            ```\n        \"\"\"\n\n        bucket_path = self._join_bucket_folder()\n        self.logger.info(f\"Listing folders in bucket {bucket_path}.\")\n\n        blobs = await self.list_blobs(folder)\n        # gets all folders with full path\n        folders = {\n            str(PurePosixPath(blob.name).parent).replace(\".\", \"\") for blob in blobs\n        }\n\n        return list(folders)\n\n    @sync_compatible\n    async def download_object_to_path(\n        self,\n        from_path: str,\n        to_path: Optional[Union[str, Path]] = None,\n        **download_kwargs: Dict[str, Any],\n    ) -&gt; Path:\n\"\"\"\n        Downloads an object from the object storage service to a path.\n\n        Args:\n            from_path: The path to the blob to download; this gets prefixed\n                with the bucket_folder.\n            to_path: The path to download the blob to. If not provided, the\n                blob's name will be used.\n            **download_kwargs: Additional keyword arguments to pass to\n                `Blob.download_to_filename`.\n\n        Returns:\n            The absolute path that the object was downloaded to.\n\n        Examples:\n            Download my_folder/notes.txt object to notes.txt.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n            ```\n        \"\"\"\n        if to_path is None:\n            to_path = Path(from_path).name\n\n        # making path absolute, but converting back to str here\n        # since !r looks nicer that way and filename arg expects str\n        to_path = str(Path(to_path).absolute())\n\n        bucket = await self.get_bucket()\n        bucket_path = self._join_bucket_folder(from_path)\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n            f\"to {to_path!r}.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.download_to_filename, filename=to_path, **download_kwargs\n        )\n        return Path(to_path)\n\n    @sync_compatible\n    async def download_object_to_file_object(\n        self,\n        from_path: str,\n        to_file_object: BinaryIO,\n        **download_kwargs: Dict[str, Any],\n    ) -&gt; BinaryIO:\n\"\"\"\n        Downloads an object from the object storage service to a file-like object,\n        which can be a BytesIO object or a BufferedWriter.\n\n        Args:\n            from_path: The path to the blob to download from; this gets prefixed\n                with the bucket_folder.\n            to_file_object: The file-like object to download the blob to.\n            **download_kwargs: Additional keyword arguments to pass to\n                `Blob.download_to_file`.\n\n        Returns:\n            The file-like object that the object was downloaded to.\n\n        Examples:\n            Download my_folder/notes.txt object to a BytesIO object.\n            ```python\n            from io import BytesIO\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with BytesIO() as buf:\n                gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n            ```\n\n            Download my_folder/notes.txt object to a BufferedWriter.\n            ```python\n                from prefect_gcp.cloud_storage import GcsBucket\n\n                gcs_bucket = GcsBucket.load(\"my-bucket\")\n                with open(\"notes.txt\", \"wb\") as f:\n                    gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n            ```\n        \"\"\"\n        bucket = await self.get_bucket()\n\n        bucket_path = self._join_bucket_folder(from_path)\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n            f\"to file object.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.download_to_file, file_obj=to_file_object, **download_kwargs\n        )\n        return to_file_object\n\n    @sync_compatible\n    async def download_folder_to_path(\n        self,\n        from_folder: str,\n        to_folder: Optional[Union[str, Path]] = None,\n        **download_kwargs: Dict[str, Any],\n    ) -&gt; Path:\n\"\"\"\n        Downloads objects *within* a folder (excluding the folder itself)\n        from the object storage service to a folder.\n\n        Args:\n            from_folder: The path to the folder to download from; this gets prefixed\n                with the bucket_folder.\n            to_folder: The path to download the folder to. If not provided, will default\n                to the current directory.\n            **download_kwargs: Additional keyword arguments to pass to\n                `Blob.download_to_filename`.\n\n        Returns:\n            The absolute path that the folder was downloaded to.\n\n        Examples:\n            Download my_folder to a local folder named my_folder.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n            ```\n        \"\"\"\n        if to_folder is None:\n            to_folder = \"\"\n        to_folder = Path(to_folder).absolute()\n\n        blobs = await self.list_blobs(folder=from_folder)\n        if len(blobs) == 0:\n            self.logger.warning(\n                f\"No blobs were downloaded from \"\n                f\"bucket {self.bucket!r} path {from_folder!r}.\"\n            )\n            return to_folder\n\n        # do not call self._join_bucket_folder for list_blobs\n        # because it's built-in to that method already!\n        # however, we still need to do it because we're using relative_to\n        bucket_folder = self._join_bucket_folder(from_folder)\n\n        async_coros = []\n        for blob in blobs:\n            bucket_path = PurePosixPath(blob.name).relative_to(bucket_folder)\n            if str(bucket_path).endswith(\"/\"):\n                continue\n            to_path = to_folder / bucket_path\n            to_path.parent.mkdir(parents=True, exist_ok=True)\n            self.logger.info(\n                f\"Downloading blob from bucket {self.bucket!r} path \"\n                f\"{str(bucket_path)!r} to {to_path}.\"\n            )\n            async_coros.append(\n                run_sync_in_worker_thread(\n                    blob.download_to_filename, filename=str(to_path), **download_kwargs\n                )\n            )\n        await asyncio.gather(*async_coros)\n\n        return to_folder\n\n    @sync_compatible\n    async def upload_from_path(\n        self,\n        from_path: Union[str, Path],\n        to_path: Optional[str] = None,\n        **upload_kwargs: Dict[str, Any],\n    ) -&gt; str:\n\"\"\"\n        Uploads an object from a path to the object storage service.\n\n        Args:\n            from_path: The path to the file to upload from.\n            to_path: The path to upload the file to. If not provided, will use\n                the file name of from_path; this gets prefixed\n                with the bucket_folder.\n            **upload_kwargs: Additional keyword arguments to pass to\n                `Blob.upload_from_filename`.\n\n        Returns:\n            The path that the object was uploaded to.\n\n        Examples:\n            Upload notes.txt to my_folder/notes.txt.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n            ```\n        \"\"\"\n        if to_path is None:\n            to_path = Path(from_path).name\n\n        bucket_path = self._join_bucket_folder(to_path)\n        bucket = await self.get_bucket()\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Uploading from {from_path!r} to the bucket \"\n            f\"{self.bucket!r} path {bucket_path!r}.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.upload_from_filename, filename=from_path, **upload_kwargs\n        )\n        return bucket_path\n\n    @sync_compatible\n    async def upload_from_file_object(\n        self, from_file_object: BinaryIO, to_path: str, **upload_kwargs\n    ) -&gt; str:\n\"\"\"\n        Uploads an object to the object storage service from a file-like object,\n        which can be a BytesIO object or a BufferedReader.\n\n        Args:\n            from_file_object: The file-like object to upload from.\n            to_path: The path to upload the object to; this gets prefixed\n                with the bucket_folder.\n            **upload_kwargs: Additional keyword arguments to pass to\n                `Blob.upload_from_file`.\n\n        Returns:\n            The path that the object was uploaded to.\n\n        Examples:\n            Upload my_folder/notes.txt object to a BytesIO object.\n            ```python\n            from io import BytesIO\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with open(\"notes.txt\", \"rb\") as f:\n                gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n            ```\n\n            Upload BufferedReader object to my_folder/notes.txt.\n            ```python\n            from io import BufferedReader\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with open(\"notes.txt\", \"rb\") as f:\n                gcs_bucket.upload_from_file_object(\n                    BufferedReader(f), \"my_folder/notes.txt\"\n                )\n            ```\n        \"\"\"\n        bucket = await self.get_bucket()\n\n        bucket_path = self._join_bucket_folder(to_path)\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Uploading from file object to the bucket \"\n            f\"{self.bucket!r} path {bucket_path!r}.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.upload_from_file, from_file_object, **upload_kwargs\n        )\n        return bucket_path\n\n    @sync_compatible\n    async def upload_from_folder(\n        self,\n        from_folder: Union[str, Path],\n        to_folder: Optional[str] = None,\n        **upload_kwargs: Dict[str, Any],\n    ) -&gt; str:\n\"\"\"\n        Uploads files *within* a folder (excluding the folder itself)\n        to the object storage service folder.\n\n        Args:\n            from_folder: The path to the folder to upload from.\n            to_folder: The path to upload the folder to. If not provided, will default\n                to bucket_folder or the base directory of the bucket.\n            **upload_kwargs: Additional keyword arguments to pass to\n                `Blob.upload_from_filename`.\n\n        Returns:\n            The path that the folder was uploaded to.\n\n        Examples:\n            Upload local folder my_folder to the bucket's folder my_folder.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.upload_from_folder(\"my_folder\")\n            ```\n        \"\"\"\n        from_folder = Path(from_folder)\n        # join bucket folder expects string for the first input\n        # when it returns None, we need to convert it back to empty string\n        # so relative_to works\n        bucket_folder = self._join_bucket_folder(to_folder or \"\") or \"\"\n\n        num_uploaded = 0\n        bucket = await self.get_bucket()\n\n        async_coros = []\n        for from_path in from_folder.rglob(\"**/*\"):\n            if from_path.is_dir():\n                continue\n            bucket_path = str(Path(bucket_folder) / from_path.relative_to(from_folder))\n            self.logger.info(\n                f\"Uploading from {str(from_path)!r} to the bucket \"\n                f\"{self.bucket!r} path {bucket_path!r}.\"\n            )\n            blob = bucket.blob(bucket_path)\n            async_coros.append(\n                run_sync_in_worker_thread(\n                    blob.upload_from_filename, filename=from_path, **upload_kwargs\n                )\n            )\n            num_uploaded += 1\n        await asyncio.gather(*async_coros)\n        if num_uploaded == 0:\n            self.logger.warning(f\"No files were uploaded from {from_folder}.\")\n        return bucket_folder\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket-attributes","title":"Attributes","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.bucket","title":"<code>bucket: str</code>  <code>pydantic-field</code> <code>required</code>","text":"<p>Name of the bucket.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.bucket_folder","title":"<code>bucket_folder: str</code>  <code>pydantic-field</code>","text":"<p>A default path to a folder within the GCS bucket to use for reading and writing objects.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.gcp_credentials","title":"<code>gcp_credentials: GcpCredentials</code>  <code>pydantic-field</code>","text":"<p>The credentials to authenticate with GCP.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket-methods","title":"Methods","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_folder_to_path","title":"<code>download_folder_to_path</code>  <code>async</code>","text":"<p>Downloads objects within a folder (excluding the folder itself) from the object storage service to a folder.</p> <p>Parameters:</p> Name Type Description Default <code>from_folder</code> <code>str</code> <p>The path to the folder to download from; this gets prefixed with the bucket_folder.</p> required <code>to_folder</code> <code>Union[str, pathlib.Path]</code> <p>The path to download the folder to. If not provided, will default to the current directory.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>The absolute path that the folder was downloaded to.</p> <p>Examples:</p> <p>Download my_folder to a local folder named my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def download_folder_to_path(\n    self,\n    from_folder: str,\n    to_folder: Optional[Union[str, Path]] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; Path:\n\"\"\"\n    Downloads objects *within* a folder (excluding the folder itself)\n    from the object storage service to a folder.\n\n    Args:\n        from_folder: The path to the folder to download from; this gets prefixed\n            with the bucket_folder.\n        to_folder: The path to download the folder to. If not provided, will default\n            to the current directory.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_filename`.\n\n    Returns:\n        The absolute path that the folder was downloaded to.\n\n    Examples:\n        Download my_folder to a local folder named my_folder.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n        ```\n    \"\"\"\n    if to_folder is None:\n        to_folder = \"\"\n    to_folder = Path(to_folder).absolute()\n\n    blobs = await self.list_blobs(folder=from_folder)\n    if len(blobs) == 0:\n        self.logger.warning(\n            f\"No blobs were downloaded from \"\n            f\"bucket {self.bucket!r} path {from_folder!r}.\"\n        )\n        return to_folder\n\n    # do not call self._join_bucket_folder for list_blobs\n    # because it's built-in to that method already!\n    # however, we still need to do it because we're using relative_to\n    bucket_folder = self._join_bucket_folder(from_folder)\n\n    async_coros = []\n    for blob in blobs:\n        bucket_path = PurePosixPath(blob.name).relative_to(bucket_folder)\n        if str(bucket_path).endswith(\"/\"):\n            continue\n        to_path = to_folder / bucket_path\n        to_path.parent.mkdir(parents=True, exist_ok=True)\n        self.logger.info(\n            f\"Downloading blob from bucket {self.bucket!r} path \"\n            f\"{str(bucket_path)!r} to {to_path}.\"\n        )\n        async_coros.append(\n            run_sync_in_worker_thread(\n                blob.download_to_filename, filename=str(to_path), **download_kwargs\n            )\n        )\n    await asyncio.gather(*async_coros)\n\n    return to_folder\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_object_to_file_object","title":"<code>download_object_to_file_object</code>  <code>async</code>","text":"<p>Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the blob to download from; this gets prefixed with the bucket_folder.</p> required <code>to_file_object</code> <code>BinaryIO</code> <p>The file-like object to download the blob to.</p> required <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_file</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BinaryIO</code> <p>The file-like object that the object was downloaded to.</p> <p>Examples:</p> <p>Download my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith BytesIO() as buf:\n    gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n</code></pre></p> <p>Download my_folder/notes.txt object to a BufferedWriter. <pre><code>    from prefect_gcp.cloud_storage import GcsBucket\n\n    gcs_bucket = GcsBucket.load(\"my-bucket\")\n    with open(\"notes.txt\", \"wb\") as f:\n        gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def download_object_to_file_object(\n    self,\n    from_path: str,\n    to_file_object: BinaryIO,\n    **download_kwargs: Dict[str, Any],\n) -&gt; BinaryIO:\n\"\"\"\n    Downloads an object from the object storage service to a file-like object,\n    which can be a BytesIO object or a BufferedWriter.\n\n    Args:\n        from_path: The path to the blob to download from; this gets prefixed\n            with the bucket_folder.\n        to_file_object: The file-like object to download the blob to.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_file`.\n\n    Returns:\n        The file-like object that the object was downloaded to.\n\n    Examples:\n        Download my_folder/notes.txt object to a BytesIO object.\n        ```python\n        from io import BytesIO\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        with BytesIO() as buf:\n            gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n        ```\n\n        Download my_folder/notes.txt object to a BufferedWriter.\n        ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with open(\"notes.txt\", \"wb\") as f:\n                gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n        ```\n    \"\"\"\n    bucket = await self.get_bucket()\n\n    bucket_path = self._join_bucket_folder(from_path)\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n        f\"to file object.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.download_to_file, file_obj=to_file_object, **download_kwargs\n    )\n    return to_file_object\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_object_to_path","title":"<code>download_object_to_path</code>  <code>async</code>","text":"<p>Downloads an object from the object storage service to a path.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the blob to download; this gets prefixed with the bucket_folder.</p> required <code>to_path</code> <code>Union[str, pathlib.Path]</code> <p>The path to download the blob to. If not provided, the blob's name will be used.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>The absolute path that the object was downloaded to.</p> <p>Examples:</p> <p>Download my_folder/notes.txt object to notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def download_object_to_path(\n    self,\n    from_path: str,\n    to_path: Optional[Union[str, Path]] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; Path:\n\"\"\"\n    Downloads an object from the object storage service to a path.\n\n    Args:\n        from_path: The path to the blob to download; this gets prefixed\n            with the bucket_folder.\n        to_path: The path to download the blob to. If not provided, the\n            blob's name will be used.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_filename`.\n\n    Returns:\n        The absolute path that the object was downloaded to.\n\n    Examples:\n        Download my_folder/notes.txt object to notes.txt.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n        ```\n    \"\"\"\n    if to_path is None:\n        to_path = Path(from_path).name\n\n    # making path absolute, but converting back to str here\n    # since !r looks nicer that way and filename arg expects str\n    to_path = str(Path(to_path).absolute())\n\n    bucket = await self.get_bucket()\n    bucket_path = self._join_bucket_folder(from_path)\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n        f\"to {to_path!r}.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.download_to_filename, filename=to_path, **download_kwargs\n    )\n    return Path(to_path)\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.get_bucket","title":"<code>get_bucket</code>  <code>async</code>","text":"<p>Returns the bucket object.</p> <p>Returns:</p> Type Description <code>Bucket</code> <p>The bucket object.</p> <p>Examples:</p> <p>Get the bucket object. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.get_bucket()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def get_bucket(self) -&gt; \"Bucket\":\n\"\"\"\n    Returns the bucket object.\n\n    Returns:\n        The bucket object.\n\n    Examples:\n        Get the bucket object.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.get_bucket()\n        ```\n    \"\"\"\n    self.logger.info(f\"Getting bucket {self.bucket!r}.\")\n    client = self.gcp_credentials.get_cloud_storage_client()\n    bucket = await run_sync_in_worker_thread(client.get_bucket, self.bucket)\n    return bucket\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>Optional[str]</code> <p>Path in GCS bucket to download from. Defaults to the block's configured bucket_folder.</p> <code>None</code> <code>local_path</code> <code>Optional[str]</code> <p>Local path to download GCS bucket contents to. Defaults to the current working directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Union[str, pathlib.Path]]</code> <p>A list of downloaded file paths.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; List[Union[str, Path]]:\n\"\"\"\n    Copies a folder from the configured GCS bucket to a local directory.\n    Defaults to copying the entire contents of the block's bucket_folder\n    to the current working directory.\n\n    Args:\n        from_path: Path in GCS bucket to download from. Defaults to the block's\n            configured bucket_folder.\n        local_path: Local path to download GCS bucket contents to.\n            Defaults to the current working directory.\n\n    Returns:\n        A list of downloaded file paths.\n    \"\"\"\n    from_path = (\n        self.bucket_folder if from_path is None else self._resolve_path(from_path)\n    )\n\n    if local_path is None:\n        local_path = os.path.abspath(\".\")\n    else:\n        local_path = os.path.abspath(os.path.expanduser(local_path))\n\n    project = self.gcp_credentials.project\n    client = self.gcp_credentials.get_cloud_storage_client(project=project)\n\n    blobs = await run_sync_in_worker_thread(\n        client.list_blobs, self.bucket, prefix=from_path\n    )\n\n    file_paths = []\n    for blob in blobs:\n        blob_path = blob.name\n        if blob_path[-1] == \"/\":\n            # object is a folder and will be created if it contains any objects\n            continue\n        local_file_path = os.path.join(local_path, blob_path)\n        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n        with disable_run_logger():\n            file_path = await cloud_storage_download_blob_to_file.fn(\n                bucket=self.bucket,\n                blob=blob_path,\n                path=local_file_path,\n                gcp_credentials=self.gcp_credentials,\n            )\n            file_paths.append(file_path)\n    return file_paths\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.list_blobs","title":"<code>list_blobs</code>  <code>async</code>","text":"<p>Lists all blobs in the bucket that are in a folder. Folders are not included in the output.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder to list blobs from.</p> <code>''</code> <p>Returns:</p> Type Description <code>List[Blob]</code> <p>A list of Blob objects.</p> <p>Examples:</p> <p>Get all blobs from a folder named \"prefect\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_blobs(\"prefect\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def list_blobs(self, folder: str = \"\") -&gt; List[\"Blob\"]:\n\"\"\"\n    Lists all blobs in the bucket that are in a folder.\n    Folders are not included in the output.\n\n    Args:\n        folder: The folder to list blobs from.\n\n    Returns:\n        A list of Blob objects.\n\n    Examples:\n        Get all blobs from a folder named \"prefect\".\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.list_blobs(\"prefect\")\n        ```\n    \"\"\"\n    client = self.gcp_credentials.get_cloud_storage_client()\n\n    bucket_path = self._join_bucket_folder(folder)\n    self.logger.info(f\"Listing blobs in bucket {bucket_path}.\")\n    blobs = await run_sync_in_worker_thread(\n        client.list_blobs, self.bucket, prefix=bucket_path\n    )\n\n    # Ignore folders\n    return [blob for blob in blobs if not blob.name.endswith(\"/\")]\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.list_folders","title":"<code>list_folders</code>  <code>async</code>","text":"<p>Lists all folders and subfolders in the bucket.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>List all folders and subfolders inside given folder.</p> <code>''</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of folders.</p> <p>Examples:</p> <p>Get all folders from a bucket named \"my-bucket\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders()\n</code></pre></p> <p>Get all folders from a folder called years <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders('years)\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def list_folders(self, folder: str = \"\") -&gt; List[str]:\n\"\"\"\n    Lists all folders and subfolders in the bucket.\n\n    Args:\n        folder: List all folders and subfolders inside given folder.\n\n    Returns:\n        A list of folders.\n\n    Examples:\n        Get all folders from a bucket named \"my-bucket\".\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.list_folders()\n        ```\n\n        Get all folders from a folder called years\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.list_folders('years)\n        ```\n    \"\"\"\n\n    bucket_path = self._join_bucket_folder()\n    self.logger.info(f\"Listing folders in bucket {bucket_path}.\")\n\n    blobs = await self.list_blobs(folder)\n    # gets all folders with full path\n    folders = {\n        str(PurePosixPath(blob.name).parent).replace(\".\", \"\") for blob in blobs\n    }\n\n    return list(folders)\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Uploads a directory from a given local path to the configured GCS bucket in a given folder.</p> <p>Defaults to uploading the entire contents the current working directory to the block's bucket_folder.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>Optional[str]</code> <p>Path to local directory to upload from.</p> <code>None</code> <code>to_path</code> <code>Optional[str]</code> <p>Path in GCS bucket to upload to. Defaults to block's configured bucket_folder.</p> <code>None</code> <code>ignore_file</code> <code>Optional[str]</code> <p>Path to file containing gitignore style expressions for filepaths to ignore.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of files uploaded.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self,\n    local_path: Optional[str] = None,\n    to_path: Optional[str] = None,\n    ignore_file: Optional[str] = None,\n) -&gt; int:\n\"\"\"\n    Uploads a directory from a given local path to the configured GCS bucket in a\n    given folder.\n\n    Defaults to uploading the entire contents the current working directory to the\n    block's bucket_folder.\n\n    Args:\n        local_path: Path to local directory to upload from.\n        to_path: Path in GCS bucket to upload to. Defaults to block's configured\n            bucket_folder.\n        ignore_file: Path to file containing gitignore style expressions for\n            filepaths to ignore.\n\n    Returns:\n        The number of files uploaded.\n    \"\"\"\n    if local_path is None:\n        local_path = os.path.abspath(\".\")\n    else:\n        local_path = os.path.expanduser(local_path)\n\n    to_path = self.bucket_folder if to_path is None else self._resolve_path(to_path)\n\n    included_files = None\n    if ignore_file:\n        with open(ignore_file, \"r\") as f:\n            ignore_patterns = f.readlines()\n        included_files = filter_files(local_path, ignore_patterns)\n\n    uploaded_file_count = 0\n    for local_file_path in Path(local_path).rglob(\"*\"):\n        if (\n            included_files is not None\n            and local_file_path.name not in included_files\n        ):\n            continue\n        elif not local_file_path.is_dir():\n            remote_file_path = str(\n                PurePosixPath(to_path, local_file_path.relative_to(local_path))\n            )\n            local_file_content = local_file_path.read_bytes()\n            await self.write_path(remote_file_path, content=local_file_content)\n            uploaded_file_count += 1\n\n    return uploaded_file_count\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.read_path","title":"<code>read_path</code>  <code>async</code>","text":"<p>Read specified path from GCS and return contents. Provide the entire path to the key in GCS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Entire path to (and including) the key.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>A bytes or string representation of the blob object.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def read_path(self, path: str) -&gt; bytes:\n\"\"\"\n    Read specified path from GCS and return contents. Provide the entire\n    path to the key in GCS.\n\n    Args:\n        path: Entire path to (and including) the key.\n\n    Returns:\n        A bytes or string representation of the blob object.\n    \"\"\"\n    path = self._resolve_path(path)\n    with disable_run_logger():\n        contents = await cloud_storage_download_blob_as_bytes.fn(\n            bucket=self.bucket, blob=path, gcp_credentials=self.gcp_credentials\n        )\n    return contents\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_file_object","title":"<code>upload_from_file_object</code>  <code>async</code>","text":"<p>Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader.</p> <p>Parameters:</p> Name Type Description Default <code>from_file_object</code> <code>BinaryIO</code> <p>The file-like object to upload from.</p> required <code>to_path</code> <code>str</code> <p>The path to upload the object to; this gets prefixed with the bucket_folder.</p> required <code>**upload_kwargs</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_file</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path that the object was uploaded to.</p> <p>Examples:</p> <p>Upload my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n</code></pre></p> <p>Upload BufferedReader object to my_folder/notes.txt. <pre><code>from io import BufferedReader\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(\n        BufferedReader(f), \"my_folder/notes.txt\"\n    )\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def upload_from_file_object(\n    self, from_file_object: BinaryIO, to_path: str, **upload_kwargs\n) -&gt; str:\n\"\"\"\n    Uploads an object to the object storage service from a file-like object,\n    which can be a BytesIO object or a BufferedReader.\n\n    Args:\n        from_file_object: The file-like object to upload from.\n        to_path: The path to upload the object to; this gets prefixed\n            with the bucket_folder.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_file`.\n\n    Returns:\n        The path that the object was uploaded to.\n\n    Examples:\n        Upload my_folder/notes.txt object to a BytesIO object.\n        ```python\n        from io import BytesIO\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        with open(\"notes.txt\", \"rb\") as f:\n            gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n        ```\n\n        Upload BufferedReader object to my_folder/notes.txt.\n        ```python\n        from io import BufferedReader\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        with open(\"notes.txt\", \"rb\") as f:\n            gcs_bucket.upload_from_file_object(\n                BufferedReader(f), \"my_folder/notes.txt\"\n            )\n        ```\n    \"\"\"\n    bucket = await self.get_bucket()\n\n    bucket_path = self._join_bucket_folder(to_path)\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Uploading from file object to the bucket \"\n        f\"{self.bucket!r} path {bucket_path!r}.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.upload_from_file, from_file_object, **upload_kwargs\n    )\n    return bucket_path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_folder","title":"<code>upload_from_folder</code>  <code>async</code>","text":"<p>Uploads files within a folder (excluding the folder itself) to the object storage service folder.</p> <p>Parameters:</p> Name Type Description Default <code>from_folder</code> <code>Union[str, pathlib.Path]</code> <p>The path to the folder to upload from.</p> required <code>to_folder</code> <code>Optional[str]</code> <p>The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path that the folder was uploaded to.</p> <p>Examples:</p> <p>Upload local folder my_folder to the bucket's folder my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_folder(\"my_folder\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def upload_from_folder(\n    self,\n    from_folder: Union[str, Path],\n    to_folder: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n\"\"\"\n    Uploads files *within* a folder (excluding the folder itself)\n    to the object storage service folder.\n\n    Args:\n        from_folder: The path to the folder to upload from.\n        to_folder: The path to upload the folder to. If not provided, will default\n            to bucket_folder or the base directory of the bucket.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_filename`.\n\n    Returns:\n        The path that the folder was uploaded to.\n\n    Examples:\n        Upload local folder my_folder to the bucket's folder my_folder.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.upload_from_folder(\"my_folder\")\n        ```\n    \"\"\"\n    from_folder = Path(from_folder)\n    # join bucket folder expects string for the first input\n    # when it returns None, we need to convert it back to empty string\n    # so relative_to works\n    bucket_folder = self._join_bucket_folder(to_folder or \"\") or \"\"\n\n    num_uploaded = 0\n    bucket = await self.get_bucket()\n\n    async_coros = []\n    for from_path in from_folder.rglob(\"**/*\"):\n        if from_path.is_dir():\n            continue\n        bucket_path = str(Path(bucket_folder) / from_path.relative_to(from_folder))\n        self.logger.info(\n            f\"Uploading from {str(from_path)!r} to the bucket \"\n            f\"{self.bucket!r} path {bucket_path!r}.\"\n        )\n        blob = bucket.blob(bucket_path)\n        async_coros.append(\n            run_sync_in_worker_thread(\n                blob.upload_from_filename, filename=from_path, **upload_kwargs\n            )\n        )\n        num_uploaded += 1\n    await asyncio.gather(*async_coros)\n    if num_uploaded == 0:\n        self.logger.warning(f\"No files were uploaded from {from_folder}.\")\n    return bucket_folder\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_path","title":"<code>upload_from_path</code>  <code>async</code>","text":"<p>Uploads an object from a path to the object storage service.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>Union[str, pathlib.Path]</code> <p>The path to the file to upload from.</p> required <code>to_path</code> <code>Optional[str]</code> <p>The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path that the object was uploaded to.</p> <p>Examples:</p> <p>Upload notes.txt to my_folder/notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def upload_from_path(\n    self,\n    from_path: Union[str, Path],\n    to_path: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n\"\"\"\n    Uploads an object from a path to the object storage service.\n\n    Args:\n        from_path: The path to the file to upload from.\n        to_path: The path to upload the file to. If not provided, will use\n            the file name of from_path; this gets prefixed\n            with the bucket_folder.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_filename`.\n\n    Returns:\n        The path that the object was uploaded to.\n\n    Examples:\n        Upload notes.txt to my_folder/notes.txt.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n        ```\n    \"\"\"\n    if to_path is None:\n        to_path = Path(from_path).name\n\n    bucket_path = self._join_bucket_folder(to_path)\n    bucket = await self.get_bucket()\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Uploading from {from_path!r} to the bucket \"\n        f\"{self.bucket!r} path {bucket_path!r}.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.upload_from_filename, filename=from_path, **upload_kwargs\n    )\n    return bucket_path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.write_path","title":"<code>write_path</code>  <code>async</code>","text":"<p>Writes to an GCS bucket.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The key name. Each object in your bucket has a unique key (or key name).</p> required <code>content</code> <code>bytes</code> <p>What you are uploading to GCS Bucket.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path that the contents were written to.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def write_path(self, path: str, content: bytes) -&gt; str:\n\"\"\"\n    Writes to an GCS bucket.\n\n    Args:\n        path: The key name. Each object in your bucket has a unique\n            key (or key name).\n        content: What you are uploading to GCS Bucket.\n\n    Returns:\n        The path that the contents were written to.\n    \"\"\"\n    path = self._resolve_path(path)\n    with disable_run_logger():\n        await cloud_storage_upload_blob_from_string.fn(\n            data=content,\n            bucket=self.bucket,\n            blob=path,\n            gcp_credentials=self.gcp_credentials,\n        )\n    return path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage-functions","title":"Functions","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_copy_blob","title":"<code>cloud_storage_copy_blob</code>  <code>async</code>","text":"<p>Copies data from one Google Cloud Storage bucket to another, without downloading it locally.</p> <p>Parameters:</p> Name Type Description Default <code>source_bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>dest_bucket</code> <code>str</code> <p>Destination bucket name.</p> required <code>source_blob</code> <code>str</code> <p>Source blob name.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>dest_blob</code> <code>Optional[str]</code> <p>Destination blob name; if not provided, defaults to source_blob.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**copy_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Bucket.copy_blob</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Destination blob name.</p> <p>Examples:</p> <p>Copies blob from one bucket to another. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_copy_blob\n\n@flow()\ndef example_cloud_storage_copy_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_copy_blob(\n        \"source_bucket\",\n        \"dest_bucket\",\n        \"source_blob\",\n        gcp_credentials\n    )\n    return blob\n\nexample_cloud_storage_copy_blob_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_copy_blob(\n    source_bucket: str,\n    dest_bucket: str,\n    source_blob: str,\n    gcp_credentials: GcpCredentials,\n    dest_blob: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **copy_kwargs: Dict[str, Any],\n) -&gt; str:\n\"\"\"\n    Copies data from one Google Cloud Storage bucket to another,\n    without downloading it locally.\n\n    Args:\n        source_bucket: Source bucket name.\n        dest_bucket: Destination bucket name.\n        source_blob: Source blob name.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        dest_blob: Destination blob name; if not provided, defaults to source_blob.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **copy_kwargs: Additional keyword arguments to pass to\n            `Bucket.copy_blob`.\n\n    Returns:\n        Destination blob name.\n\n    Example:\n        Copies blob from one bucket to another.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_copy_blob\n\n        @flow()\n        def example_cloud_storage_copy_blob_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            blob = cloud_storage_copy_blob(\n                \"source_bucket\",\n                \"dest_bucket\",\n                \"source_blob\",\n                gcp_credentials\n            )\n            return blob\n\n        example_cloud_storage_copy_blob_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\n        \"Copying blob named %s from the %s bucket to the %s bucket\",\n        source_blob,\n        source_bucket,\n        dest_bucket,\n    )\n\n    source_bucket_obj = await _get_bucket(\n        source_bucket, gcp_credentials, project=project\n    )\n\n    dest_bucket_obj = await _get_bucket(dest_bucket, gcp_credentials, project=project)\n    if dest_blob is None:\n        dest_blob = source_blob\n\n    source_blob_obj = source_bucket_obj.blob(source_blob)\n    await run_sync_in_worker_thread(\n        source_bucket_obj.copy_blob,\n        blob=source_blob_obj,\n        destination_bucket=dest_bucket_obj,\n        new_name=dest_blob,\n        timeout=timeout,\n        **copy_kwargs,\n    )\n\n    return dest_blob\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_create_bucket","title":"<code>cloud_storage_create_bucket</code>  <code>async</code>","text":"<p>Creates a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>location</code> <code>Optional[str]</code> <p>Location of the bucket.</p> <code>None</code> <code>**create_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>client.create_bucket</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The bucket name.</p> <p>Examples:</p> <p>Creates a bucket named \"prefect\". <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_create_bucket\n\n@flow()\ndef example_cloud_storage_create_bucket_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials)\n\nexample_cloud_storage_create_bucket_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_create_bucket(\n    bucket: str,\n    gcp_credentials: GcpCredentials,\n    project: Optional[str] = None,\n    location: Optional[str] = None,\n    **create_kwargs: Dict[str, Any],\n) -&gt; str:\n\"\"\"\n    Creates a bucket.\n\n    Args:\n        bucket: Name of the bucket.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        location: Location of the bucket.\n        **create_kwargs: Additional keyword arguments to pass to `client.create_bucket`.\n\n    Returns:\n        The bucket name.\n\n    Example:\n        Creates a bucket named \"prefect\".\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_create_bucket\n\n        @flow()\n        def example_cloud_storage_create_bucket_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials)\n\n        example_cloud_storage_create_bucket_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Creating %s bucket\", bucket)\n\n    client = gcp_credentials.get_cloud_storage_client(project=project)\n    await run_sync_in_worker_thread(\n        client.create_bucket, bucket, location=location, **create_kwargs\n    )\n    return bucket\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_as_bytes","title":"<code>cloud_storage_download_blob_as_bytes</code>  <code>async</code>","text":"<p>Downloads a blob as bytes.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>chunk_size</code> <code>int</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_as_bytes</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bytes</code> <p>A bytes or string representation of the blob object.</p> <p>Examples:</p> <p>Downloads blob from bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes\n\n@flow()\ndef example_cloud_storage_download_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    contents = cloud_storage_download_blob_as_bytes(\n        \"bucket\", \"blob\", gcp_credentials)\n    return contents\n\nexample_cloud_storage_download_blob_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_download_blob_as_bytes(\n    bucket: str,\n    blob: str,\n    gcp_credentials: GcpCredentials,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; bytes:\n\"\"\"\n    Downloads a blob as bytes.\n\n    Args:\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        chunk_size (int, optional): The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_as_bytes`.\n\n    Returns:\n        A bytes or string representation of the blob object.\n\n    Example:\n        Downloads blob from bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes\n\n        @flow()\n        def example_cloud_storage_download_blob_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            contents = cloud_storage_download_blob_as_bytes(\n                \"bucket\", \"blob\", gcp_credentials)\n            return contents\n\n        example_cloud_storage_download_blob_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Downloading blob named %s from the %s bucket\", blob, bucket)\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    contents = await run_sync_in_worker_thread(\n        blob_obj.download_as_bytes, timeout=timeout, **download_kwargs\n    )\n    return contents\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_to_file","title":"<code>cloud_storage_download_blob_to_file</code>  <code>async</code>","text":"<p>Downloads a blob to a file path.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>path</code> <code>Union[str, pathlib.Path]</code> <p>Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>chunk_size</code> <code>int</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, pathlib.Path]</code> <p>The path to the blob object.</p> <p>Examples:</p> <p>Downloads blob from bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file\n\n@flow()\ndef example_cloud_storage_download_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    path = cloud_storage_download_blob_to_file(\n        \"bucket\", \"blob\", \"file_path\", gcp_credentials)\n    return path\n\nexample_cloud_storage_download_blob_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_download_blob_to_file(\n    bucket: str,\n    blob: str,\n    path: Union[str, Path],\n    gcp_credentials: GcpCredentials,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; Union[str, Path]:\n\"\"\"\n    Downloads a blob to a file path.\n\n    Args:\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        path: Downloads the contents to the provided file path;\n            if the path is a directory, automatically joins the blob name.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        chunk_size (int, optional): The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_filename`.\n\n    Returns:\n        The path to the blob object.\n\n    Example:\n        Downloads blob from bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file\n\n        @flow()\n        def example_cloud_storage_download_blob_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            path = cloud_storage_download_blob_to_file(\n                \"bucket\", \"blob\", \"file_path\", gcp_credentials)\n            return path\n\n        example_cloud_storage_download_blob_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\n        \"Downloading blob named %s from the %s bucket to %s\", blob, bucket, path\n    )\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    if os.path.isdir(path):\n        if isinstance(path, Path):\n            path = path.joinpath(blob)  # keep as Path if Path is passed\n        else:\n            path = os.path.join(path, blob)  # keep as str if a str is passed\n\n    await run_sync_in_worker_thread(\n        blob_obj.download_to_filename, path, timeout=timeout, **download_kwargs\n    )\n    return path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_file","title":"<code>cloud_storage_upload_blob_from_file</code>  <code>async</code>","text":"<p>Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, pathlib.Path, _io.BytesIO]</code> <p>Path to data or file like object to upload.</p> required <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>content_type</code> <code>Optional[str]</code> <p>Type of content being uploaded.</p> <code>None</code> <code>chunk_size</code> <code>Optional[int]</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_file</code> or <code>Blob.upload_from_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The blob name.</p> <p>Examples:</p> <p>Uploads blob to bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file\n\n@flow()\ndef example_cloud_storage_upload_blob_from_file_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_upload_blob_from_file(\n        \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials)\n    return blob\n\nexample_cloud_storage_upload_blob_from_file_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_upload_blob_from_file(\n    file: Union[str, Path, BytesIO],\n    bucket: str,\n    blob: str,\n    gcp_credentials: GcpCredentials,\n    content_type: Optional[str] = None,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n\"\"\"\n    Uploads a blob from file path or file-like object. Usage for passing in\n    file-like object is if the data was downloaded from the web;\n    can bypass writing to disk and directly upload to Cloud Storage.\n\n    Args:\n        file: Path to data or file like object to upload.\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        content_type: Type of content being uploaded.\n        chunk_size: The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_file` or `Blob.upload_from_filename`.\n\n    Returns:\n        The blob name.\n\n    Example:\n        Uploads blob to bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file\n\n        @flow()\n        def example_cloud_storage_upload_blob_from_file_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            blob = cloud_storage_upload_blob_from_file(\n                \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials)\n            return blob\n\n        example_cloud_storage_upload_blob_from_file_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Uploading blob named %s to the %s bucket\", blob, bucket)\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    if isinstance(file, BytesIO):\n        await run_sync_in_worker_thread(\n            blob_obj.upload_from_file,\n            file,\n            content_type=content_type,\n            timeout=timeout,\n            **upload_kwargs,\n        )\n    else:\n        await run_sync_in_worker_thread(\n            blob_obj.upload_from_filename,\n            file,\n            content_type=content_type,\n            timeout=timeout,\n            **upload_kwargs,\n        )\n    return blob\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_string","title":"<code>cloud_storage_upload_blob_from_string</code>  <code>async</code>","text":"<p>Uploads a blob from a string or bytes representation of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, bytes]</code> <p>String or bytes representation of data to upload.</p> required <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>content_type</code> <code>Optional[str]</code> <p>Type of content being uploaded.</p> <code>None</code> <code>chunk_size</code> <code>Optional[int]</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_string</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The blob name.</p> <p>Examples:</p> <p>Uploads blob to bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string\n\n@flow()\ndef example_cloud_storage_upload_blob_from_string_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_upload_blob_from_string(\n        \"data\", \"bucket\", \"blob\", gcp_credentials)\n    return blob\n\nexample_cloud_storage_upload_blob_from_string_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_upload_blob_from_string(\n    data: Union[str, bytes],\n    bucket: str,\n    blob: str,\n    gcp_credentials: GcpCredentials,\n    content_type: Optional[str] = None,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n\"\"\"\n    Uploads a blob from a string or bytes representation of data.\n\n    Args:\n        data: String or bytes representation of data to upload.\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        content_type: Type of content being uploaded.\n        chunk_size: The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_string`.\n\n    Returns:\n        The blob name.\n\n    Example:\n        Uploads blob to bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string\n\n        @flow()\n        def example_cloud_storage_upload_blob_from_string_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            blob = cloud_storage_upload_blob_from_string(\n                \"data\", \"bucket\", \"blob\", gcp_credentials)\n            return blob\n\n        example_cloud_storage_upload_blob_from_string_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Uploading blob named %s to the %s bucket\", blob, bucket)\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    await run_sync_in_worker_thread(\n        blob_obj.upload_from_string,\n        data,\n        content_type=content_type,\n        timeout=timeout,\n        **upload_kwargs,\n    )\n    return blob\n</code></pre>"},{"location":"credentials/","title":"Credentials","text":""},{"location":"credentials/#prefect_gcp.credentials","title":"<code>prefect_gcp.credentials</code>","text":"<p>Module handling GCP credentials.</p>"},{"location":"credentials/#prefect_gcp.credentials-classes","title":"Classes","text":""},{"location":"credentials/#prefect_gcp.credentials.ClientType","title":"<code> ClientType            (Enum)         </code>","text":"<p>An enumeration.</p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>class ClientType(Enum):\n\n    CLOUD_STORAGE = \"cloud_storage\"\n    BIGQUERY = \"bigquery\"\n    SECRET_MANAGER = \"secret_manager\"\n    AIPLATFORM = \"job_service\"  # vertex ai\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials","title":"<code> GcpCredentials            (CredentialsBlock)         </code>  <code>pydantic-model</code>","text":"<p>Block used to manage authentication with GCP. Google authentication is handled via the <code>google.oauth2</code> module or through the CLI. Specify either one of service <code>account_file</code> or <code>service_account_info</code>; if both are not specified, the client will try to detect the credentials following Google's Application Default Credentials. See Google's Authentication documentation for details on inference and recommended authentication patterns.</p> <p>Attributes:</p> Name Type Description <code>service_account_file</code> <code>Optional[pathlib.Path]</code> <p>Path to the service account JSON keyfile.</p> <code>service_account_info</code> <code>Optional[prefect.blocks.fields.SecretDict]</code> <p>The contents of the keyfile as a dict.</p> <p>Examples:</p> <p>Load GCP credentials stored in a <code>GCP Credentials</code> Block: <pre><code>from prefect_gcp import GcpCredentials\ngcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>class GcpCredentials(CredentialsBlock):\n\"\"\"\n    Block used to manage authentication with GCP. Google authentication is\n    handled via the `google.oauth2` module or through the CLI.\n    Specify either one of service `account_file` or `service_account_info`; if both\n    are not specified, the client will try to detect the credentials following Google's\n    [Application Default Credentials](https://cloud.google.com/docs/authentication/application-default-credentials).\n    See Google's [Authentication documentation](https://cloud.google.com/docs/authentication#service-accounts)\n    for details on inference and recommended authentication patterns.\n\n    Attributes:\n        service_account_file: Path to the service account JSON keyfile.\n        service_account_info: The contents of the keyfile as a dict.\n\n    Example:\n        Load GCP credentials stored in a `GCP Credentials` Block:\n        ```python\n        from prefect_gcp import GcpCredentials\n        gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"  # noqa\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\"  # noqa\n    _block_type_name = \"GCP Credentials\"\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/credentials/#prefect_gcp.credentials.GcpCredentials\"  # noqa: E501\n\n    service_account_file: Optional[Path] = Field(\n        default=None, description=\"Path to the service account JSON keyfile.\"\n    )\n    service_account_info: Optional[SecretDict] = Field(\n        default=None, description=\"The contents of the keyfile as a dict.\"\n    )\n    project: Optional[str] = Field(\n        default=None, description=\"The GCP project to use for the client.\"\n    )\n\n    _service_account_email: Optional[str] = None\n\n    @root_validator\n    def _provide_one_service_account_source(cls, values):\n\"\"\"\n        Ensure that only a service account file or service account info ias provided.\n        \"\"\"\n        both_service_account = (\n            values.get(\"service_account_info\") is not None\n            and values.get(\"service_account_file\") is not None\n        )\n        if both_service_account:\n            raise ValueError(\n                \"Only one of service_account_info or service_account_file \"\n                \"can be specified at once\"\n            )\n        return values\n\n    @validator(\"service_account_file\")\n    def _check_service_account_file(cls, file):\n\"\"\"Get full path of provided file and make sure that it exists.\"\"\"\n        if not file:\n            return file\n\n        service_account_file = Path(file).expanduser()\n        if not service_account_file.exists():\n            raise ValueError(\"The provided path to the service account is invalid\")\n        return service_account_file\n\n    @validator(\"service_account_info\", pre=True)\n    def _convert_json_string_json_service_account_info(cls, value):\n\"\"\"\n        Converts service account info provided as a json formatted string\n        to a dictionary\n        \"\"\"\n        if isinstance(value, str):\n            try:\n                service_account_info = json.loads(value)\n                return service_account_info\n            except Exception:\n                raise ValueError(\"Unable to decode service_account_info\")\n        else:\n            return value\n\n    def block_initialization(self):\n        credentials = self.get_credentials_from_service_account()\n        if self.project is None:\n            if self.service_account_info or self.service_account_file:\n                credentials_project = credentials.project_id\n            else:  # google.auth.default using gcloud auth application-default login\n                credentials_project = credentials.quota_project_id\n            self.project = credentials_project\n\n        if hasattr(credentials, \"service_account_email\"):\n            self._service_account_email = credentials.service_account_email\n\n    def get_credentials_from_service_account(self) -&gt; Credentials:\n\"\"\"\n        Helper method to serialize credentials by using either\n        service_account_file or service_account_info.\n        \"\"\"\n        if self.service_account_info:\n            credentials = Credentials.from_service_account_info(\n                self.service_account_info.get_secret_value(),\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n        elif self.service_account_file:\n            credentials = Credentials.from_service_account_file(\n                self.service_account_file,\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n        else:\n            credentials, _ = google.auth.default()\n        return credentials\n\n    @sync_compatible\n    async def get_access_token(self):\n\"\"\"\n        See: https://stackoverflow.com/a/69107745\n        Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/\n        \"\"\"  # noqa\n        request = google.auth.transport.requests.Request()\n        credentials = self.get_credentials_from_service_account()\n        await run_sync_in_worker_thread(credentials.refresh, request)\n        return credentials.token\n\n    def get_client(\n        self,\n        client_type: Union[str, ClientType],\n        **get_client_kwargs: Dict[str, Any],\n    ) -&gt; Any:\n\"\"\"\n        Helper method to dynamically get a client type.\n\n        Args:\n            client_type: The name of the client to get.\n            **get_client_kwargs: Additional keyword arguments to pass to the\n                `get_*_client` method.\n\n        Returns:\n            An authenticated client.\n\n        Raises:\n            ValueError: if the client is not supported.\n        \"\"\"\n        if isinstance(client_type, str):\n            client_type = ClientType(client_type)\n        client_type = client_type.value\n        get_client_method = getattr(self, f\"get_{client_type}_client\")\n        return get_client_method(**get_client_kwargs)\n\n    @_raise_help_msg(\"cloud_storage\")\n    def get_cloud_storage_client(\n        self, project: Optional[str] = None\n    ) -&gt; \"StorageClient\":\n\"\"\"\n        Gets an authenticated Cloud Storage client.\n\n        Args:\n            project: Name of the project to use; overrides the base\n                class's project if provided.\n\n        Returns:\n            An authenticated Cloud Storage client.\n\n        Examples:\n            Gets a GCP Cloud Storage client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_cloud_storage_client()\n            example_get_client_flow()\n            ```\n\n            Gets a GCP Cloud Storage client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_cloud_storage_client()\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n\n        # override class project if method project is provided\n        project = project or self.project\n        storage_client = StorageClient(credentials=credentials, project=project)\n        return storage_client\n\n    @_raise_help_msg(\"bigquery\")\n    def get_bigquery_client(\n        self, project: str = None, location: str = None\n    ) -&gt; \"BigQueryClient\":\n\"\"\"\n        Gets an authenticated BigQuery client.\n\n        Args:\n            project: Name of the project to use; overrides the base\n                class's project if provided.\n            location: Location to use.\n\n        Returns:\n            An authenticated BigQuery client.\n\n        Examples:\n            Gets a GCP BigQuery client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_bigquery_client()\n            example_get_client_flow()\n            ```\n\n            Gets a GCP BigQuery client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_bigquery_client()\n\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n\n        # override class project if method project is provided\n        project = project or self.project\n        big_query_client = BigQueryClient(\n            credentials=credentials, project=project, location=location\n        )\n        return big_query_client\n\n    @_raise_help_msg(\"secret_manager\")\n    def get_secret_manager_client(self) -&gt; \"SecretManagerServiceClient\":\n\"\"\"\n        Gets an authenticated Secret Manager Service client.\n\n        Returns:\n            An authenticated Secret Manager Service client.\n\n        Examples:\n            Gets a GCP Secret Manager client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_secret_manager_client()\n            example_get_client_flow()\n            ```\n\n            Gets a GCP Cloud Storage client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_secret_manager_client()\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n\n        # doesn't accept project; must pass in project in tasks\n        secret_manager_client = SecretManagerServiceClient(credentials=credentials)\n        return secret_manager_client\n\n    @_raise_help_msg(\"aiplatform\")\n    def get_job_service_client(\n        self, client_options: Dict[str, Any] = None\n    ) -&gt; \"JobServiceClient\":\n\"\"\"\n        Gets an authenticated Job Service client for Vertex AI.\n\n        Returns:\n            An authenticated Job Service client.\n\n        Examples:\n            Gets a GCP Job Service client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_job_service_client()\n\n            example_get_client_flow()\n            ```\n\n            Gets a GCP Cloud Storage client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_job_service_client()\n\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n        job_service_client = JobServiceClient(\n            credentials=credentials, client_options=client_options\n        )\n        return job_service_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials-attributes","title":"Attributes","text":""},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.project","title":"<code>project: str</code>  <code>pydantic-field</code>","text":"<p>The GCP project to use for the client.</p>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.service_account_file","title":"<code>service_account_file: Path</code>  <code>pydantic-field</code>","text":"<p>Path to the service account JSON keyfile.</p>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.service_account_info","title":"<code>service_account_info: SecretDict</code>  <code>pydantic-field</code>","text":"<p>The contents of the keyfile as a dict.</p>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials-methods","title":"Methods","text":""},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_access_token","title":"<code>get_access_token</code>  <code>async</code>","text":"Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@sync_compatible\nasync def get_access_token(self):\n\"\"\"\n    See: https://stackoverflow.com/a/69107745\n    Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/\n    \"\"\"  # noqa\n    request = google.auth.transport.requests.Request()\n    credentials = self.get_credentials_from_service_account()\n    await run_sync_in_worker_thread(credentials.refresh, request)\n    return credentials.token\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_bigquery_client","title":"<code>get_bigquery_client</code>","text":"<p>Gets an authenticated BigQuery client.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>Name of the project to use; overrides the base class's project if provided.</p> <code>None</code> <code>location</code> <code>str</code> <p>Location to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>BigQueryClient</code> <p>An authenticated BigQuery client.</p> <p>Examples:</p> <p>Gets a GCP BigQuery client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_bigquery_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP BigQuery client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_bigquery_client()\n\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"bigquery\")\ndef get_bigquery_client(\n    self, project: str = None, location: str = None\n) -&gt; \"BigQueryClient\":\n\"\"\"\n    Gets an authenticated BigQuery client.\n\n    Args:\n        project: Name of the project to use; overrides the base\n            class's project if provided.\n        location: Location to use.\n\n    Returns:\n        An authenticated BigQuery client.\n\n    Examples:\n        Gets a GCP BigQuery client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_bigquery_client()\n        example_get_client_flow()\n        ```\n\n        Gets a GCP BigQuery client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_bigquery_client()\n\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n\n    # override class project if method project is provided\n    project = project or self.project\n    big_query_client = BigQueryClient(\n        credentials=credentials, project=project, location=location\n    )\n    return big_query_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_client","title":"<code>get_client</code>","text":"<p>Helper method to dynamically get a client type.</p> <p>Parameters:</p> Name Type Description Default <code>client_type</code> <code>Union[str, prefect_gcp.credentials.ClientType]</code> <p>The name of the client to get.</p> required <code>**get_client_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to the <code>get_*_client</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>An authenticated client.</p> <p>Exceptions:</p> Type Description <code>ValueError</code> <p>if the client is not supported.</p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>def get_client(\n    self,\n    client_type: Union[str, ClientType],\n    **get_client_kwargs: Dict[str, Any],\n) -&gt; Any:\n\"\"\"\n    Helper method to dynamically get a client type.\n\n    Args:\n        client_type: The name of the client to get.\n        **get_client_kwargs: Additional keyword arguments to pass to the\n            `get_*_client` method.\n\n    Returns:\n        An authenticated client.\n\n    Raises:\n        ValueError: if the client is not supported.\n    \"\"\"\n    if isinstance(client_type, str):\n        client_type = ClientType(client_type)\n    client_type = client_type.value\n    get_client_method = getattr(self, f\"get_{client_type}_client\")\n    return get_client_method(**get_client_kwargs)\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_cloud_storage_client","title":"<code>get_cloud_storage_client</code>","text":"<p>Gets an authenticated Cloud Storage client.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the base class's project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>StorageClient</code> <p>An authenticated Cloud Storage client.</p> <p>Examples:</p> <p>Gets a GCP Cloud Storage client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"cloud_storage\")\ndef get_cloud_storage_client(\n    self, project: Optional[str] = None\n) -&gt; \"StorageClient\":\n\"\"\"\n    Gets an authenticated Cloud Storage client.\n\n    Args:\n        project: Name of the project to use; overrides the base\n            class's project if provided.\n\n    Returns:\n        An authenticated Cloud Storage client.\n\n    Examples:\n        Gets a GCP Cloud Storage client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_cloud_storage_client()\n        example_get_client_flow()\n        ```\n\n        Gets a GCP Cloud Storage client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_cloud_storage_client()\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n\n    # override class project if method project is provided\n    project = project or self.project\n    storage_client = StorageClient(credentials=credentials, project=project)\n    return storage_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_credentials_from_service_account","title":"<code>get_credentials_from_service_account</code>","text":"<p>Helper method to serialize credentials by using either service_account_file or service_account_info.</p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>def get_credentials_from_service_account(self) -&gt; Credentials:\n\"\"\"\n    Helper method to serialize credentials by using either\n    service_account_file or service_account_info.\n    \"\"\"\n    if self.service_account_info:\n        credentials = Credentials.from_service_account_info(\n            self.service_account_info.get_secret_value(),\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n        )\n    elif self.service_account_file:\n        credentials = Credentials.from_service_account_file(\n            self.service_account_file,\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n        )\n    else:\n        credentials, _ = google.auth.default()\n    return credentials\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_job_service_client","title":"<code>get_job_service_client</code>","text":"<p>Gets an authenticated Job Service client for Vertex AI.</p> <p>Returns:</p> Type Description <code>JobServiceClient</code> <p>An authenticated Job Service client.</p> <p>Examples:</p> <p>Gets a GCP Job Service client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"aiplatform\")\ndef get_job_service_client(\n    self, client_options: Dict[str, Any] = None\n) -&gt; \"JobServiceClient\":\n\"\"\"\n    Gets an authenticated Job Service client for Vertex AI.\n\n    Returns:\n        An authenticated Job Service client.\n\n    Examples:\n        Gets a GCP Job Service client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_job_service_client()\n\n        example_get_client_flow()\n        ```\n\n        Gets a GCP Cloud Storage client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_job_service_client()\n\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n    job_service_client = JobServiceClient(\n        credentials=credentials, client_options=client_options\n    )\n    return job_service_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_secret_manager_client","title":"<code>get_secret_manager_client</code>","text":"<p>Gets an authenticated Secret Manager Service client.</p> <p>Returns:</p> Type Description <code>SecretManagerServiceClient</code> <p>An authenticated Secret Manager Service client.</p> <p>Examples:</p> <p>Gets a GCP Secret Manager client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"secret_manager\")\ndef get_secret_manager_client(self) -&gt; \"SecretManagerServiceClient\":\n\"\"\"\n    Gets an authenticated Secret Manager Service client.\n\n    Returns:\n        An authenticated Secret Manager Service client.\n\n    Examples:\n        Gets a GCP Secret Manager client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_secret_manager_client()\n        example_get_client_flow()\n        ```\n\n        Gets a GCP Cloud Storage client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_secret_manager_client()\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n\n    # doesn't accept project; must pass in project in tasks\n    secret_manager_client = SecretManagerServiceClient(credentials=credentials)\n    return secret_manager_client\n</code></pre>"},{"location":"examples_catalog/","title":"Examples Catalog","text":"<p>Below is a list of examples for <code>prefect-gcp</code>.</p>"},{"location":"examples_catalog/#aiplatform-module","title":"Aiplatform Module","text":"<p>Run a job using Vertex AI Custom Training: <pre><code>from prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.run()\n</code></pre></p> <p>Run a job that runs the command <code>echo hello world</code> using Google Cloud Run Jobs: <pre><code>from prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    command=[\"echo\", \"hello world\"],\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.run()\n</code></pre></p> <p>Preview job specs: <pre><code>from prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    command=[\"echo\", \"hello world\"],\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.preview()\n</code></pre></p>"},{"location":"examples_catalog/#bigquery-module","title":"Bigquery Module","text":"<p>Execute operation with parameters: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        CREATE TABLE mydataset.trips AS (\n        SELECT\n            bikeid,\n            start_time,\n            duration_minutes\n        FROM\n            bigquery-public-data.austin_bikeshare.bikeshare_trips\n        LIMIT %(limit)s\n        );\n    '''\n    warehouse.execute(operation, parameters={\"limit\": 5})\n</code></pre> Create mytable in mydataset and insert two rows into it: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"bigquery\") as warehouse:\n    create_operation = '''\n    CREATE TABLE IF NOT EXISTS mydataset.mytable (\n        col1 STRING,\n        col2 INTEGER,\n        col3 BOOLEAN\n    )\n    '''\n    warehouse.execute(create_operation)\n    insert_operation = '''\n    INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n    '''\n    seq_of_parameters = [\n        (\"a\", 1, True),\n        (\"b\", 2, False),\n    ]\n    warehouse.execute_many(\n        insert_operation,\n        seq_of_parameters=seq_of_parameters\n    )\n</code></pre> Execute operation with parameters, fetching all rows: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    result = warehouse.fetch_all(operation, parameters=parameters)\n</code></pre> Execute operation with parameters, fetching one new row at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_one(operation, parameters=parameters)\n        print(result)\n</code></pre> Execute operation with parameters, fetching two new rows at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 6;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_many(\n            operation,\n            parameters=parameters,\n            size=2\n        )\n        print(result)\n</code></pre></p>"},{"location":"examples_catalog/#cloud-run-module","title":"Cloud Run Module","text":"<p>Run a job using Google Cloud Run Jobs: <pre><code>CloudRunJob(\n    image=\"gcr.io/my-project/my-image\",\n    region=\"us-east1\",\n    credentials=my_gcp_credentials\n).run()\n</code></pre></p> <p>Run a job that runs the command <code>echo hello world</code> using Google Cloud Run Jobs: <pre><code>CloudRunJob(\n    image=\"gcr.io/my-project/my-image\",\n    region=\"us-east1\",\n    credentials=my_gcp_credentials\n    command=[\"echo\", \"hello world\"]\n).run()\n</code></pre></p>"},{"location":"examples_catalog/#cloud-storage-module","title":"Cloud Storage Module","text":"<p>Download my_folder to a local folder named my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n</code></pre> Download my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith BytesIO() as buf:\n    gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n</code></pre></p> <p>Download my_folder/notes.txt object to a BufferedWriter. <pre><code>    from prefect_gcp.cloud_storage import GcsBucket\n\n    gcs_bucket = GcsBucket.load(\"my-bucket\")\n    with open(\"notes.txt\", \"wb\") as f:\n        gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n</code></pre> Upload my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n</code></pre></p> <p>Upload BufferedReader object to my_folder/notes.txt. <pre><code>from io import BufferedReader\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(\n        BufferedReader(f), \"my_folder/notes.txt\"\n    )\n</code></pre> Get all blobs from a folder named \"prefect\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_blobs(\"prefect\")\n</code></pre> Download my_folder/notes.txt object to notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n</code></pre> Get the bucket object. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.get_bucket()\n</code></pre> Upload notes.txt to my_folder/notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n</code></pre> Get all folders from a bucket named \"my-bucket\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders()\n</code></pre></p> <p>Get all folders from a folder called years <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders('years)\n</code></pre> Upload local folder my_folder to the bucket's folder my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_folder(\"my_folder\")\n</code></pre></p>"},{"location":"examples_catalog/#credentials-module","title":"Credentials Module","text":"<p>Gets a GCP BigQuery client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_bigquery_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP BigQuery client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_bigquery_client()\n\nexample_get_client_flow()\n</code></pre> Gets a GCP Secret Manager client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre> Gets a GCP Cloud Storage client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre> Gets a GCP Job Service client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p>"},{"location":"secret_manager/","title":"Secret Manager","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager","title":"<code>prefect_gcp.secret_manager</code>","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager-classes","title":"Classes","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret","title":"<code> GcpSecret            (SecretBlock)         </code>  <code>pydantic-model</code>","text":"<p>Manages a secret in Google Cloud Platform's Secret Manager.</p> <p>Attributes:</p> Name Type Description <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> <code>secret_name</code> <code>str</code> <p>Name of the secret to manage.</p> <code>secret_version</code> <code>str</code> <p>Version number of the secret to use, or \"latest\".</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>class GcpSecret(SecretBlock):\n\"\"\"\n    Manages a secret in Google Cloud Platform's Secret Manager.\n\n    Attributes:\n        gcp_credentials: Credentials to use for authentication with GCP.\n        secret_name: Name of the secret to manage.\n        secret_version: Version number of the secret to use, or \"latest\".\n    \"\"\"\n\n    _logo_url = \"https://images.ctfassets.net/gm98wzqotmnx/4CD4wwbiIKPkZDt4U3TEuW/c112fe85653da054b6d5334ef662bec4/gcp.png?h=250\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/secret_manager/#prefect_gcp.secret_manager.GcpSecret\"  # noqa: E501\n\n    gcp_credentials: GcpCredentials\n    secret_name: str = Field(default=..., description=\"Name of the secret to manage.\")\n    secret_version: str = Field(\n        default=\"latest\", description=\"Version number of the secret to use.\"\n    )\n\n    @sync_compatible\n    async def read_secret(self) -&gt; bytes:\n\"\"\"\n        Reads the secret data from the secret storage service.\n\n        Returns:\n            The secret data as bytes.\n        \"\"\"\n        client = self.gcp_credentials.get_secret_manager_client()\n        project = self.gcp_credentials.project\n        name = f\"projects/{project}/secrets/{self.secret_name}/versions/{self.secret_version}\"  # noqa\n        request = AccessSecretVersionRequest(name=name)\n\n        self.logger.debug(f\"Preparing to read secret data from {name!r}.\")\n        response = await run_sync_in_worker_thread(\n            client.access_secret_version, request=request\n        )\n        secret = response.payload.data.decode(\"UTF-8\")\n        self.logger.info(f\"The secret {name!r} data was successfully read.\")\n        return secret\n\n    @sync_compatible\n    async def write_secret(self, secret_data: bytes) -&gt; str:\n\"\"\"\n        Writes the secret data to the secret storage service; if it doesn't exist\n        it will be created.\n\n        Args:\n            secret_data: The secret to write.\n\n        Returns:\n            The path that the secret was written to.\n        \"\"\"\n        client = self.gcp_credentials.get_secret_manager_client()\n        project = self.gcp_credentials.project\n        parent = f\"projects/{project}/secrets/{self.secret_name}\"\n        payload = SecretPayload(data=secret_data)\n        add_request = AddSecretVersionRequest(parent=parent, payload=payload)\n\n        self.logger.debug(f\"Preparing to write secret data to {parent!r}.\")\n        try:\n            response = await run_sync_in_worker_thread(\n                client.add_secret_version, request=add_request\n            )\n        except NotFound:\n            self.logger.info(\n                f\"The secret {parent!r} does not exist yet, creating it now.\"\n            )\n            create_parent = f\"projects/{project}\"\n            secret_id = self.secret_name\n            secret = Secret(replication=Replication(automatic=Replication.Automatic()))\n            create_request = CreateSecretRequest(\n                parent=create_parent, secret_id=secret_id, secret=secret\n            )\n            await run_sync_in_worker_thread(\n                client.create_secret, request=create_request\n            )\n\n            self.logger.debug(f\"Preparing to write secret data to {parent!r} again.\")\n            response = await run_sync_in_worker_thread(\n                client.add_secret_version, request=add_request\n            )\n\n        self.logger.info(f\"The secret data was written successfully to {parent!r}.\")\n        return response.name\n\n    @sync_compatible\n    async def delete_secret(self) -&gt; str:\n\"\"\"\n        Deletes the secret from the secret storage service.\n\n        Returns:\n            The path that the secret was deleted from.\n        \"\"\"\n        client = self.gcp_credentials.get_secret_manager_client()\n        project = self.gcp_credentials.project\n\n        name = f\"projects/{project}/secrets/{self.secret_name}\"\n        request = DeleteSecretRequest(name=name)\n\n        self.logger.debug(f\"Preparing to delete the secret {name!r}.\")\n        await run_sync_in_worker_thread(client.delete_secret, request=request)\n        self.logger.info(f\"The secret {name!r} was successfully deleted.\")\n        return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret-attributes","title":"Attributes","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.secret_name","title":"<code>secret_name: str</code>  <code>pydantic-field</code> <code>required</code>","text":"<p>Name of the secret to manage.</p>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.secret_version","title":"<code>secret_version: str</code>  <code>pydantic-field</code>","text":"<p>Version number of the secret to use.</p>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret-methods","title":"Methods","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.delete_secret","title":"<code>delete_secret</code>  <code>async</code>","text":"<p>Deletes the secret from the secret storage service.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path that the secret was deleted from.</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@sync_compatible\nasync def delete_secret(self) -&gt; str:\n\"\"\"\n    Deletes the secret from the secret storage service.\n\n    Returns:\n        The path that the secret was deleted from.\n    \"\"\"\n    client = self.gcp_credentials.get_secret_manager_client()\n    project = self.gcp_credentials.project\n\n    name = f\"projects/{project}/secrets/{self.secret_name}\"\n    request = DeleteSecretRequest(name=name)\n\n    self.logger.debug(f\"Preparing to delete the secret {name!r}.\")\n    await run_sync_in_worker_thread(client.delete_secret, request=request)\n    self.logger.info(f\"The secret {name!r} was successfully deleted.\")\n    return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.read_secret","title":"<code>read_secret</code>  <code>async</code>","text":"<p>Reads the secret data from the secret storage service.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>The secret data as bytes.</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@sync_compatible\nasync def read_secret(self) -&gt; bytes:\n\"\"\"\n    Reads the secret data from the secret storage service.\n\n    Returns:\n        The secret data as bytes.\n    \"\"\"\n    client = self.gcp_credentials.get_secret_manager_client()\n    project = self.gcp_credentials.project\n    name = f\"projects/{project}/secrets/{self.secret_name}/versions/{self.secret_version}\"  # noqa\n    request = AccessSecretVersionRequest(name=name)\n\n    self.logger.debug(f\"Preparing to read secret data from {name!r}.\")\n    response = await run_sync_in_worker_thread(\n        client.access_secret_version, request=request\n    )\n    secret = response.payload.data.decode(\"UTF-8\")\n    self.logger.info(f\"The secret {name!r} data was successfully read.\")\n    return secret\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.write_secret","title":"<code>write_secret</code>  <code>async</code>","text":"<p>Writes the secret data to the secret storage service; if it doesn't exist it will be created.</p> <p>Parameters:</p> Name Type Description Default <code>secret_data</code> <code>bytes</code> <p>The secret to write.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path that the secret was written to.</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@sync_compatible\nasync def write_secret(self, secret_data: bytes) -&gt; str:\n\"\"\"\n    Writes the secret data to the secret storage service; if it doesn't exist\n    it will be created.\n\n    Args:\n        secret_data: The secret to write.\n\n    Returns:\n        The path that the secret was written to.\n    \"\"\"\n    client = self.gcp_credentials.get_secret_manager_client()\n    project = self.gcp_credentials.project\n    parent = f\"projects/{project}/secrets/{self.secret_name}\"\n    payload = SecretPayload(data=secret_data)\n    add_request = AddSecretVersionRequest(parent=parent, payload=payload)\n\n    self.logger.debug(f\"Preparing to write secret data to {parent!r}.\")\n    try:\n        response = await run_sync_in_worker_thread(\n            client.add_secret_version, request=add_request\n        )\n    except NotFound:\n        self.logger.info(\n            f\"The secret {parent!r} does not exist yet, creating it now.\"\n        )\n        create_parent = f\"projects/{project}\"\n        secret_id = self.secret_name\n        secret = Secret(replication=Replication(automatic=Replication.Automatic()))\n        create_request = CreateSecretRequest(\n            parent=create_parent, secret_id=secret_id, secret=secret\n        )\n        await run_sync_in_worker_thread(\n            client.create_secret, request=create_request\n        )\n\n        self.logger.debug(f\"Preparing to write secret data to {parent!r} again.\")\n        response = await run_sync_in_worker_thread(\n            client.add_secret_version, request=add_request\n        )\n\n    self.logger.info(f\"The secret data was written successfully to {parent!r}.\")\n    return response.name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager-functions","title":"Functions","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager.create_secret","title":"<code>create_secret</code>  <code>async</code>","text":"<p>Creates a secret in Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the created secret.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import create_secret\n\n@flow()\ndef example_cloud_storage_create_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_path = create_secret(\"secret_name\", gcp_credentials)\n    return secret_path\n\nexample_cloud_storage_create_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def create_secret(\n    secret_name: str,\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n\"\"\"\n    Creates a secret in Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the created secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import create_secret\n\n        @flow()\n        def example_cloud_storage_create_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_path = create_secret(\"secret_name\", gcp_credentials)\n            return secret_path\n\n        example_cloud_storage_create_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Creating the %s secret\", secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    parent = f\"projects/{project}\"\n    secret_settings = {\"replication\": {\"automatic\": {}}}\n\n    partial_create = partial(\n        client.create_secret,\n        parent=parent,\n        secret_id=secret_name,\n        secret=secret_settings,\n        timeout=timeout,\n    )\n    response = await to_thread.run_sync(partial_create)\n    return response.name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret","title":"<code>delete_secret</code>  <code>async</code>","text":"<p>Deletes the specified secret from Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to delete.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the deleted secret.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import delete_secret\n\n@flow()\ndef example_cloud_storage_delete_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_path = delete_secret(\"secret_name\", gcp_credentials)\n    return secret_path\n\nexample_cloud_storage_delete_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def delete_secret(\n    secret_name: str,\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n\"\"\"\n    Deletes the specified secret from Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to delete.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the deleted secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import delete_secret\n\n        @flow()\n        def example_cloud_storage_delete_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_path = delete_secret(\"secret_name\", gcp_credentials)\n            return secret_path\n\n        example_cloud_storage_delete_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Deleting %s secret\", secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    name = f\"projects/{project}/secrets/{secret_name}/\"\n    partial_delete = partial(client.delete_secret, name=name, timeout=timeout)\n    await to_thread.run_sync(partial_delete)\n    return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret_version","title":"<code>delete_secret_version</code>  <code>async</code>","text":"<p>Deletes a version of a given secret from Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>version_id</code> <code>int</code> <p>Version number of the secret to use; \"latest\" can NOT be used.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the deleted secret version.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import delete_secret_version\n\n@flow()\ndef example_cloud_storage_delete_secret_version_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials)\n    return secret_value\n\nexample_cloud_storage_delete_secret_version_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def delete_secret_version(\n    secret_name: str,\n    version_id: int,\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n\"\"\"\n    Deletes a version of a given secret from Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        version_id: Version number of the secret to use; \"latest\" can NOT be used.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the deleted secret version.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import delete_secret_version\n\n        @flow()\n        def example_cloud_storage_delete_secret_version_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials)\n            return secret_value\n\n        example_cloud_storage_delete_secret_version_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Reading %s version of %s secret\", version_id, secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    if version_id == \"latest\":\n        raise ValueError(\"The version_id cannot be 'latest'\")\n\n    name = f\"projects/{project}/secrets/{secret_name}/versions/{version_id}\"\n    partial_destroy = partial(client.destroy_secret_version, name=name, timeout=timeout)\n    await to_thread.run_sync(partial_destroy)\n    return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.read_secret","title":"<code>read_secret</code>  <code>async</code>","text":"<p>Reads the value of a given secret from Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Contents of the specified secret.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import read_secret\n\n@flow()\ndef example_cloud_storage_read_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1)\n    return secret_value\n\nexample_cloud_storage_read_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def read_secret(\n    secret_name: str,\n    gcp_credentials: \"GcpCredentials\",\n    version_id: Union[str, int] = \"latest\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n\"\"\"\n    Reads the value of a given secret from Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        Contents of the specified secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import read_secret\n\n        @flow()\n        def example_cloud_storage_read_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1)\n            return secret_value\n\n        example_cloud_storage_read_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Reading %s version of %s secret\", version_id, secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    name = f\"projects/{project}/secrets/{secret_name}/versions/{version_id}\"\n    partial_access = partial(client.access_secret_version, name=name, timeout=timeout)\n    response = await to_thread.run_sync(partial_access)\n    secret = response.payload.data.decode(\"UTF-8\")\n    return secret\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.update_secret","title":"<code>update_secret</code>  <code>async</code>","text":"<p>Updates a secret in Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>secret_value</code> <code>Union[str, bytes]</code> <p>Desired value of the secret. Can be either <code>str</code> or <code>bytes</code>.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the updated secret.</p> <p>Examples:</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import update_secret\n\n@flow()\ndef example_cloud_storage_update_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials)\n    return secret_path\n\nexample_cloud_storage_update_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def update_secret(\n    secret_name: str,\n    secret_value: Union[str, bytes],\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n\"\"\"\n    Updates a secret in Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        secret_value: Desired value of the secret. Can be either `str` or `bytes`.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the updated secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import update_secret\n\n        @flow()\n        def example_cloud_storage_update_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials)\n            return secret_path\n\n        example_cloud_storage_update_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Updating the %s secret\", secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    parent = f\"projects/{project}/secrets/{secret_name}\"\n    if isinstance(secret_value, str):\n        secret_value = secret_value.encode(\"UTF-8\")\n    partial_add = partial(\n        client.add_secret_version,\n        parent=parent,\n        payload={\"data\": secret_value},\n        timeout=timeout,\n    )\n    response = await to_thread.run_sync(partial_add)\n    return response.name\n</code></pre>"}]}
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Coordinate and use GCP in your dataflow with <code>prefect-gcp</code>","text":"<p>The <code>prefect-gcp</code> collection makes it easy to leverage the capabilities of Google Cloud Platform (GCP) in your flows, featuring support for Vertex AI, Cloud Run, BigQuery, Cloud Storage, and Secret Manager.</p>"},{"location":"#getting-started","title":"Getting Started","text":""},{"location":"#saving-credentials-to-a-block","title":"Saving credentials to a block","text":"<p>You will need to first install prefect-gcp and authenticate with a service account in order to use <code>prefect-gcp</code>.</p> <p><code>prefect-gcp</code> is able to safely save and load the service account, so they can be reused across the collection! Simply follow the steps below.</p> <ol> <li>Refer to the GCP service account documentation on how to create and download a service account key file.</li> <li>Copy the JSON contents.</li> <li>Create a short script, replacing the placeholders with your information.</li> </ol> <pre><code>from prefect_gcp import GcpCredentials\n\n# replace this PLACEHOLDER dict with your own service account info\nservice_account_info = {\n  \"type\": \"service_account\",\n  \"project_id\": \"PROJECT_ID\",\n  \"private_key_id\": \"KEY_ID\",\n  \"private_key\": \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n  \"client_email\": \"SERVICE_ACCOUNT_EMAIL\",\n  \"client_id\": \"CLIENT_ID\",\n  \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n  \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n  \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n  \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\"\n}\n\nGcpCredentials(\n    service_account_info=service_account_info\n).save(\"BLOCK-NAME-PLACEHOLDER\")\n</code></pre> <p><code>service_account_info</code> vs <code>service_account_file</code></p> <p>The advantage of using <code>service_account_info</code>, instead of <code>service_account_file</code>, is that it is accessible across containers.</p> <p>If <code>service_account_file</code> is used, the provided file path must be available in the container executing the flow.</p> <p>Congrats! You can now easily load the saved block, which holds your credentials:</p> <pre><code>from prefect_gcp import GcpCredentials\nGcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\")\n</code></pre> <p>Registering blocks</p> <p>Register blocks in this module to view and edit them on Prefect Cloud:</p> <pre><code>prefect block register -m prefect_gcp\n</code></pre>"},{"location":"#using-prefect-with-google-cloud-run","title":"Using Prefect with Google Cloud Run","text":"<p>Is your local computer or server running out of memory or taking too long to complete a job?</p> <p><code>prefect_gcp</code> can offers a solution by enabling you to execute your Prefect flows remotely, on-demand thru Google Cloud Run.</p> <p>The following code snippets demonstrate how <code>prefect_gcp</code> can be used to run a job on Cloud Run, either as part of a Prefect deployment's infrastructure or within a flow.</p>"},{"location":"#as-infrastructure","title":"As Infrastructure","text":"<p>Below is a simple walkthrough for how to use Google Cloud Run as infrastructure for a deployment.</p>"},{"location":"#set-variables","title":"Set variables","text":"<p>To expedite copy/paste without the needing to update placeholders manually, update and execute the following.</p> <pre><code>export CREDENTIALS_BLOCK_NAME=\"BLOCK-NAME-PLACEHOLDER\"\nexport CLOUD_RUN_JOB_BLOCK_NAME=\"cloud-run-job-example\"\nexport CLOUD_RUN_JOB_REGION=\"us-central1\"\nexport GCS_BUCKET_BLOCK_NAME=\"cloud-run-job-bucket-example\"\nexport GCP_PROJECT_ID=$(gcloud config get-value project)\n</code></pre>"},{"location":"#build-an-image","title":"Build an image","text":"<p>First, find an existing image within the Google Artifact Registry. Ensure it has Python and <code>prefect-gcp[cloud_storage]</code> installed, or follow the instructions below to set one up.</p> <p>Create a <code>Dockerfile</code>.</p> <pre><code>FROM prefecthq/prefect:2-python3.11\nRUN pip install \"prefect-gcp[cloud_storage]\"\n</code></pre> <p>Then push to the Google Artifact Registry.</p> <pre><code>gcloud artifacts repositories create test-example-repository --repository-format=docker --location=us\ngcloud auth configure-docker us-docker.pkg.dev\ndocker build -t us-docker.pkg.dev/${GCP_PROJECT_ID}/test-example-repository/prefect-gcp:2-python3.11 .\ndocker push us-docker.pkg.dev/${GCP_PROJECT_ID}/test-example-repository/prefect-gcp:2-python3.11\n</code></pre>"},{"location":"#save-an-infrastructure-and-storage-block","title":"Save an infrastructure and storage block","text":"<p>Save a custom infrastructure and storage block by executing the following snippet.</p> <pre><code>import os\nfrom prefect_gcp import GcpCredentials, CloudRunJob, GcsBucket\n\ngcp_credentials = GcpCredentials.load(os.environ[\"CREDENTIALS_BLOCK_NAME\"])\n\n# must be from GCR and have Python + Prefect\nimage = f\"us-docker.pkg.dev/{os.environ['GCP_PROJECT_ID']}/test-example-repository/prefect-gcp:2-python3.11\"  # noqa\n\ncloud_run_job = CloudRunJob(\n    image=image,\n    credentials=gcp_credentials,\n    region=os.environ[\"CLOUD_RUN_JOB_REGION\"],\n)\ncloud_run_job.save(os.environ[\"CLOUD_RUN_JOB_BLOCK_NAME\"], overwrite=True)\n\nbucket_name = \"cloud-run-job-bucket\"\ncloud_storage_client = gcp_credentials.get_cloud_storage_client()\ncloud_storage_client.create_bucket(bucket_name)\ngcs_bucket = GcsBucket(\n    bucket=bucket_name,\n    gcp_credentials=gcp_credentials,\n)\ngcs_bucket.save(os.environ[\"GCS_BUCKET_BLOCK_NAME\"], overwrite=True)\n</code></pre>"},{"location":"#write-a-flow","title":"Write a flow","text":"<p>Then, use an existing flow to create a deployment with, or use the flow below if you don't have an existing flow handy.</p> <pre><code>from prefect import flow\n\n@flow(log_prints=True)\ndef cloud_run_job_flow():\n    print(\"Hello, Prefect!\")\n\nif __name__ == \"__main__\":\n    cloud_run_job_flow()\n</code></pre>"},{"location":"#create-a-deployment","title":"Create a deployment","text":"<p>If the script was named \"cloud_run_job_script.py\", build a deployment manifest with the following command.</p> <pre><code>prefect deployment build cloud_run_job_script.py:cloud_run_job_flow \\\n    -n cloud-run-deployment \\\n    -ib cloud-run-job/${CLOUD_RUN_JOB_BLOCK_NAME} \\\n    -sb gcs-bucket/${GCS_BUCKET_BLOCK_NAME}\n</code></pre> <p>Now apply the deployment!</p> <pre><code>prefect deployment apply cloud_run_job_flow-deployment.yaml\n</code></pre>"},{"location":"#test-the-deployment","title":"Test the deployment","text":"<p>Start up an agent in a separate terminal. The agent will poll the Prefect API for scheduled flow runs that are ready to run.</p> <pre><code>prefect agent start -q 'default'\n</code></pre> <p>Run the deployment once to test.</p> <pre><code>prefect deployment run cloud-run-job-flow/cloud-run-deployment\n</code></pre> <p>Once the flow run has completed, you will see <code>Hello, Prefect!</code> logged in the Prefect UI.</p> <p>No class found for dispatch key</p> <p>If you encounter an error message like <code>KeyError: \"No class found for dispatch key 'cloud-run-job' in registry for type 'Block'.\"</code>, ensure <code>prefect-gcp</code> is installed in the environment that your agent is running!</p>"},{"location":"#within-flow","title":"Within Flow","text":"<p>You can execute commands through Cloud Run Job directly within a Prefect flow.</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_run import CloudRunJob\n\n@flow\ndef cloud_run_job_flow():\n    cloud_run_job = CloudRunJob(\n        image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n        credentials=GcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\"),\n        region=\"us-central1\",\n        command=[\"echo\", \"Hello, Prefect!\"],\n    )\n    return cloud_run_job.run()\n</code></pre>"},{"location":"#using-prefect-with-google-vertex-ai","title":"Using Prefect with Google Vertex AI","text":"<p><code>prefect_gcp</code> can enable you to execute your Prefect flows remotely, on-demand using Google Vertex AI too!</p> <p>Be sure to additionally install the AI Platform extra!</p> <p>Setting up a Vertex AI job is extremely similar to setting up a Cloud Run Job, but replace <code>CloudRunJob</code> with the following snippet.</p> <pre><code>from prefect_gcp import GcpCredentials, VertexAICustomTrainingJob, GcsBucket\n\ngcp_credentials = GcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\")\n\nvertex_ai_job = VertexAICustomTrainingJob(\n    image=\"IMAGE-NAME-PLACEHOLDER\",  # must be from GCR and have Python + Prefect\n    credentials=gcp_credentials,\n    region=\"us-central1\",\n)\nvertex_ai_job.save(\"test-example\")\n</code></pre> <p>Cloud Run Job vs Vertex AI</p> <p>With Vertex AI, you can allocate computational resources on-the-fly for your executions, much like Cloud Run.</p> <p>However, unlike Cloud Run, you have the flexibility to provision instances with higher CPU, GPU, TPU, and RAM capacities.</p> <p>Additionally, jobs can run for up to 7 days, which is significantly longer than the maximum duration allowed on Cloud Run.</p>"},{"location":"#using-prefect-with-google-bigquery","title":"Using Prefect with Google BigQuery","text":"<p>Got big data in BigQuery? <code>prefect_gcp</code> allows you to steadily stream data from and write to Google BigQuery within your Prefect flows!</p> <p>Be sure to install <code>prefect-gcp</code> with the BigQuery extra!</p> <p>The provided code snippet shows how you can use <code>prefect_gcp</code> to create a new dataset in BigQuery, define a table, insert rows, and fetch data from the table.</p> <pre><code>from prefect import flow\nfrom prefect_gcp.bigquery import GcpCredentials, BigQueryWarehouse\n\n@flow\ndef bigquery_flow():\n    all_rows = []\n    gcp_credentials = GcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\")\n\n    client = gcp_credentials.get_bigquery_client()\n    client.create_dataset(\"test_example\", exists_ok=True)\n\n    with BigQueryWarehouse(gcp_credentials=gcp_credentials) as warehouse:\n        warehouse.execute(\n            \"CREATE TABLE IF NOT EXISTS test_example.customers (name STRING, address STRING);\"\n        )\n        warehouse.execute_many(\n            \"INSERT INTO test_example.customers (name, address) VALUES (%(name)s, %(address)s);\",\n            seq_of_parameters=[\n                {\"name\": \"Marvin\", \"address\": \"Highway 42\"},\n                {\"name\": \"Ford\", \"address\": \"Highway 42\"},\n                {\"name\": \"Unknown\", \"address\": \"Highway 42\"},\n            ],\n        )\n        while True:\n            # Repeated fetch* calls using the same operation will\n            # skip re-executing and instead return the next set of results\n            new_rows = warehouse.fetch_many(\"SELECT * FROM test_example.customers\", size=2)\n            if len(new_rows) == 0:\n                break\n            all_rows.extend(new_rows)\n    return all_rows\n\nbigquery_flow()\n</code></pre>"},{"location":"#using-prefect-with-google-cloud-storage","title":"Using Prefect with Google Cloud Storage","text":"<p>With <code>prefect_gcp</code>, you can have peace of mind that your Prefect flows have not only seamlessly uploaded and downloaded objects to Google Cloud Storage, but also have these actions logged.</p> <p>Be sure to additionally install <code>prefect-gcp</code> with the Cloud Storage extra!</p> <p>The provided code snippet shows how you can use <code>prefect_gcp</code> to upload a file to a Google Cloud Storage bucket and download the same file under a different file name.</p> <pre><code>from pathlib import Path\nfrom prefect import flow\nfrom prefect_gcp import GcpCredentials, GcsBucket\n\n\n@flow\ndef cloud_storage_flow():\n    # create a dummy file to upload\n    file_path = Path(\"test-example.txt\")\n    file_path.write_text(\"Hello, Prefect!\")\n\n    gcp_credentials = GcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\")\n    gcs_bucket = GcsBucket(\n        bucket=\"BUCKET-NAME-PLACEHOLDER\",\n        gcp_credentials=gcp_credentials\n    )\n\n    gcs_bucket_path = gcs_bucket.upload_from_path(file_path)\n    downloaded_file_path = gcs_bucket.download_object_to_path(\n        gcs_bucket_path, \"downloaded-test-example.txt\"\n    )\n    return downloaded_file_path.read_text()\n\n\ncloud_storage_flow()\n</code></pre> <p>Upload and download directories</p> <p><code>GcsBucket</code> supports uploading and downloading entire directories. To view examples, check out the Examples Catalog!</p>"},{"location":"#using-prefect-with-google-secret-manager","title":"Using Prefect with Google Secret Manager","text":"<p>Do you already have secrets available on Google Secret Manager? There's no need to migrate them!</p> <p><code>prefect_gcp</code> allows you to read and write secrets with Google Secret Manager within your Prefect flows.</p> <p>Be sure to install <code>prefect-gcp</code> with the Secret Manager extra!</p> <p>The provided code snippet shows how you can use <code>prefect_gcp</code> to write a secret to the Secret Manager, read the secret data, delete the secret, and finally return the secret data.</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials, GcpSecret\n\n\n@flow\ndef secret_manager_flow():\n    gcp_credentials = GcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\")\n    gcp_secret = GcpSecret(secret_name=\"test-example\", gcp_credentials=gcp_credentials)\n    gcp_secret.write_secret(secret_data=b\"Hello, Prefect!\")\n    secret_data = gcp_secret.read_secret()\n    gcp_secret.delete_secret()\n    return secret_data\n\nsecret_manager_flow()\n</code></pre>"},{"location":"#accessing-google-credentials-or-clients-from-gcpcredentials","title":"Accessing Google credentials or clients from GcpCredentials","text":"<p>In the case that <code>prefect-gcp</code> is missing a feature, feel free to submit an issue.</p> <p>In the meantime, you may want to access the underlying Google Cloud credentials or clients, which <code>prefect-gcp</code> exposes via the <code>GcpCredentials</code> block.</p> <p>The provided code snippet shows how you can use <code>prefect_gcp</code> to instantiate a Google Cloud client, like <code>bigquery.Client</code>.</p> <p>Note a <code>GcpCredentials</code> object is NOT a valid input to the underlying BigQuery client--use the <code>get_credentials_from_service_account</code> method to access and pass an actual <code>google.auth.Credentials</code> object.</p> <pre><code>import google.cloud.bigquery\nfrom prefect import flow\nfrom prefect_gcp import GcpCredentials\n\n@flow\ndef create_bigquery_client():\n    gcp_credentials = GcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\")\n    google_auth_credentials = gcp_credentials.get_credentials_from_service_account()\n    bigquery_client = bigquery.Client(credentials=google_auth_credentials)\n</code></pre> <p>If you simply want to access the underlying client, <code>prefect-gcp</code> exposes a <code>get_client</code> method from <code>GcpCredentials</code>.</p> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\n\n@flow\ndef create_bigquery_client():\n    gcp_credentials = GcpCredentials.load(\"BLOCK-NAME-PLACEHOLDER\")\n    bigquery_client = gcp_credentials.get_client(\"bigquery\")\n</code></pre>"},{"location":"#resources","title":"Resources","text":"<p>For more tips on how to use tasks and flows in a Collection, check out Using Collections!</p>"},{"location":"#installation","title":"Installation","text":"<p>To use <code>prefect-gcp</code> and Cloud Run:</p> <pre><code>pip install prefect-gcp\n</code></pre> <p>To use Cloud Storage: <pre><code>pip install \"prefect-gcp[cloud_storage]\"\n</code></pre></p> <p>To use BigQuery:</p> <pre><code>pip install \"prefect-gcp[bigquery]\"\n</code></pre> <p>To use Secret Manager: <pre><code>pip install \"prefect-gcp[secret_manager]\"\n</code></pre></p> <p>To use Vertex AI: <pre><code>pip install \"prefect-gcp[aiplatform]\"\n</code></pre></p> <p>A list of available blocks in <code>prefect-gcp</code> and their setup instructions can be found here.</p> <p>Requires an installation of Python 3.7+.</p> <p>We recommend using a Python virtual environment manager such as pipenv, conda or virtualenv.</p> <p>These tasks are designed to work with Prefect 2. For more information about how to use Prefect, please refer to the Prefect documentation.</p>"},{"location":"#feedback","title":"Feedback","text":"<p>If you encounter any bugs while using <code>prefect-gcp</code>, feel free to open an issue in the <code>prefect-gcp</code> repository.</p> <p>If you have any questions or issues while using <code>prefect-gcp</code>, you can find help in either the Prefect Discourse forum or the Prefect Slack community.</p> <p>Feel free to star or watch <code>prefect-gcp</code> for updates too!</p>"},{"location":"aiplatform/","title":"AI Platform","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform","title":"<code>prefect_gcp.aiplatform</code>","text":"<p>Integrations with Google AI Platform.</p> <p>Note this module is experimental. The intefaces within may change without notice.</p> <p>Examples:</p> <pre><code>Run a job using Vertex AI Custom Training:\n```python\nfrom prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.run()\n```\n\nRun a job that runs the command `echo hello world` using Google Cloud Run Jobs:\n```python\nfrom prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    command=[\"echo\", \"hello world\"],\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.run()\n```\n\nPreview job specs:\n```python\nfrom prefect_gcp.credentials import GcpCredentials\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\ngcp_credentials = GcpCredentials.load(\"BLOCK_NAME\")\njob = VertexAICustomTrainingJob(\n    command=[\"echo\", \"hello world\"],\n    region=\"us-east1\",\n    image=\"us-docker.pkg.dev/cloudrun/container/job:latest\",\n    gcp_credentials=gcp_credentials,\n)\njob.preview()\n```\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform-classes","title":"Classes","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob","title":"<code>VertexAICustomTrainingJob</code>","text":"<p>             Bases: <code>Infrastructure</code></p> <p>Infrastructure block used to run Vertex AI custom training jobs.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>class VertexAICustomTrainingJob(Infrastructure):\n    \"\"\"\n    Infrastructure block used to run Vertex AI custom training jobs.\n    \"\"\"\n\n    _block_type_name = \"Vertex AI Custom Training Job\"\n    _block_type_slug = \"vertex-ai-custom-training-job\"\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob\"  # noqa: E501\n\n    type: Literal[\"vertex-ai-custom-training-job\"] = Field(\n        \"vertex-ai-custom-training-job\", description=\"The slug for this task type.\"\n    )\n\n    gcp_credentials: GcpCredentials = Field(\n        default_factory=GcpCredentials,\n        description=(\n            \"GCP credentials to use when running the configured Vertex AI custom \"\n            \"training job. If not provided, credentials will be inferred from the \"\n            \"environment. See `GcpCredentials` for details.\"\n        ),\n    )\n    region: str = Field(\n        default=...,\n        description=\"The region where the Vertex AI custom training job resides.\",\n    )\n    image: str = Field(\n        default=...,\n        title=\"Image Name\",\n        description=(\n            \"The image to use for a new Vertex AI custom training job. This value must \"\n            \"refer to an image within either Google Container Registry \"\n            \"or Google Artifact Registry, like `gcr.io/&lt;project_name&gt;/&lt;repo&gt;/`.\"\n        ),\n    )\n    env: Dict[str, str] = Field(\n        default_factory=dict,\n        title=\"Environment Variables\",\n        description=\"Environment variables to be passed to your Cloud Run Job.\",\n    )\n    machine_type: str = Field(\n        default=\"n1-standard-4\",\n        description=\"The machine type to use for the run, which controls the available \"\n        \"CPU and memory.\",\n    )\n    accelerator_type: Optional[str] = Field(\n        default=None, description=\"The type of accelerator to attach to the machine.\"\n    )\n    accelerator_count: Optional[int] = Field(\n        default=None, description=\"The number of accelerators to attach to the machine.\"\n    )\n    boot_disk_type: str = Field(\n        default=\"pd-ssd\",\n        title=\"Boot Disk Type\",\n        description=\"The type of boot disk to attach to the machine.\",\n    )\n    boot_disk_size_gb: int = Field(\n        default=100,\n        title=\"Boot Disk Size\",\n        description=\"The size of the boot disk to attach to the machine, in gigabytes.\",\n    )\n    maximum_run_time: datetime.timedelta = Field(\n        default=datetime.timedelta(days=7), description=\"The maximum job running time.\"\n    )\n    network: Optional[str] = Field(\n        default=None,\n        description=\"The full name of the Compute Engine network\"\n        \"to which the Job should be peered. Private services access must \"\n        \"already be configured for the network. If left unspecified, the job \"\n        \"is not peered with any network.\",\n    )\n    reserved_ip_ranges: Optional[List[str]] = Field(\n        default=None,\n        description=\"A list of names for the reserved ip ranges under the VPC \"\n        \"network that can be used for this job. If set, we will deploy the job \"\n        \"within the provided ip ranges. Otherwise, the job will be deployed to \"\n        \"any ip ranges under the provided VPC network.\",\n    )\n    service_account: Optional[str] = Field(\n        default=None,\n        description=(\n            \"Specifies the service account to use \"\n            \"as the run-as account in Vertex AI. The agent submitting jobs must have \"\n            \"act-as permission on this run-as account. If unspecified, the AI \"\n            \"Platform Custom Code Service Agent for the CustomJob's project is \"\n            \"used. Takes precedence over the service account found in gcp_credentials, \"\n            \"and required if a service account cannot be detected in gcp_credentials.\"\n        ),\n    )\n    job_watch_poll_interval: float = Field(\n        default=5.0,\n        description=(\n            \"The amount of time to wait between GCP API calls while monitoring the \"\n            \"state of a Vertex AI Job.\"\n        ),\n    )\n\n    @property\n    def job_name(self):\n        \"\"\"\n        The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference:\n        https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name\n        \"\"\"  # noqa\n        try:\n            base_name = self.name or self.image.split(\"/\")[2]\n            return f\"{base_name}-{uuid4().hex}\"\n        except IndexError:\n            raise ValueError(\n                \"The provided image must be from either Google Container Registry \"\n                \"or Google Artifact Registry\"\n            )\n\n    def _get_compatible_labels(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Ensures labels are compatible with GCP label requirements.\n        https://cloud.google.com/resource-manager/docs/creating-managing-labels\n\n        Ex: the Prefect provided key of prefect.io/flow-name -&gt; prefect-io_flow-name\n        \"\"\"\n        compatible_labels = {}\n        for key, val in self.labels.items():\n            new_key = slugify(\n                key,\n                lowercase=True,\n                replacements=[(\"/\", \"_\"), (\".\", \"-\")],\n                max_length=63,\n                regex_pattern=_DISALLOWED_GCP_LABEL_CHARACTERS,\n            )\n            compatible_labels[new_key] = slugify(\n                val,\n                lowercase=True,\n                replacements=[(\"/\", \"_\"), (\".\", \"-\")],\n                max_length=63,\n                regex_pattern=_DISALLOWED_GCP_LABEL_CHARACTERS,\n            )\n        return compatible_labels\n\n    def preview(self) -&gt; str:\n        \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n        job_spec = self._build_job_spec()\n        custom_job = CustomJob(\n            display_name=self.job_name,\n            job_spec=job_spec,\n            labels=self._get_compatible_labels(),\n        )\n        return str(custom_job)  # outputs a json string\n\n    def _build_job_spec(self) -&gt; \"CustomJobSpec\":\n        \"\"\"\n        Builds a job spec by gathering details.\n        \"\"\"\n        # gather worker pool spec\n        env_list = [\n            {\"name\": name, \"value\": value}\n            for name, value in {\n                **self._base_environment(),\n                **self.env,\n            }.items()\n        ]\n        container_spec = ContainerSpec(\n            image_uri=self.image, command=self.command, args=[], env=env_list\n        )\n        machine_spec = MachineSpec(\n            machine_type=self.machine_type,\n            accelerator_type=self.accelerator_type,\n            accelerator_count=self.accelerator_count,\n        )\n        worker_pool_spec = WorkerPoolSpec(\n            container_spec=container_spec,\n            machine_spec=machine_spec,\n            replica_count=1,\n            disk_spec=DiskSpec(\n                boot_disk_type=self.boot_disk_type,\n                boot_disk_size_gb=self.boot_disk_size_gb,\n            ),\n        )\n        # look for service account\n        service_account = (\n            self.service_account or self.gcp_credentials._service_account_email\n        )\n        if service_account is None:\n            raise ValueError(\n                \"A service account is required for the Vertex job. \"\n                \"A service account could not be detected in the attached credentials; \"\n                \"please set a service account explicitly, e.g. \"\n                '`VertexAICustomTrainingJob(service_acount=\"...\")`'\n            )\n\n        # build custom job specs\n        timeout = Duration().FromTimedelta(td=self.maximum_run_time)\n        scheduling = Scheduling(timeout=timeout)\n        job_spec = CustomJobSpec(\n            worker_pool_specs=[worker_pool_spec],\n            service_account=service_account,\n            scheduling=scheduling,\n            network=self.network,\n            reserved_ip_ranges=self.reserved_ip_ranges,\n        )\n        return job_spec\n\n    async def _create_and_begin_job(\n        self, job_spec: \"CustomJobSpec\", job_service_client: \"JobServiceClient\"\n    ) -&gt; \"CustomJob\":\n        \"\"\"\n        Builds a custom job and begins running it.\n        \"\"\"\n        # create custom job\n        custom_job = CustomJob(\n            display_name=self.job_name,\n            job_spec=job_spec,\n            labels=self._get_compatible_labels(),\n        )\n\n        # run job\n        self.logger.info(\n            f\"{self._log_prefix}: Job {self.job_name!r} starting to run \"\n            f\"the command {' '.join(self.command)!r} in region \"\n            f\"{self.region!r} using image {self.image!r}\"\n        )\n\n        project = self.gcp_credentials.project\n        resource_name = f\"projects/{project}/locations/{self.region}\"\n\n        retry_policy = retry(\n            stop=stop_after_attempt(3), wait=wait_fixed(1) + wait_random(0, 3)\n        )\n\n        custom_job_run = await run_sync_in_worker_thread(\n            retry_policy(job_service_client.create_custom_job),\n            parent=resource_name,\n            custom_job=custom_job,\n        )\n\n        self.logger.info(\n            f\"{self._log_prefix}: Job {self.job_name!r} has successfully started; \"\n            f\"the full job name is {custom_job_run.name!r}\"\n        )\n\n        return custom_job_run\n\n    async def _watch_job_run(\n        self,\n        full_job_name: str,  # different from self.job_name\n        job_service_client: \"JobServiceClient\",\n        current_state: \"JobState\",\n        until_states: Tuple[\"JobState\"],\n        timeout: int = None,\n    ) -&gt; \"CustomJob\":\n        \"\"\"\n        Polls job run to see if status changed.\n        \"\"\"\n        state = JobState.JOB_STATE_UNSPECIFIED\n        last_state = current_state\n        t0 = time.time()\n\n        while state not in until_states:\n            job_run = await run_sync_in_worker_thread(\n                job_service_client.get_custom_job,\n                name=full_job_name,\n            )\n            state = job_run.state\n            if state != last_state:\n                state_label = (\n                    state.name.replace(\"_\", \" \")\n                    .lower()\n                    .replace(\"state\", \"state is now:\")\n                )\n                # results in \"New job state is now: succeeded\"\n                self.logger.info(\n                    f\"{self._log_prefix}: {self.job_name} has new {state_label}\"\n                )\n                last_state = state\n            else:\n                # Intermittently, the job will not be described. We want to respect the\n                # watch timeout though.\n                self.logger.debug(f\"{self._log_prefix}: Job not found.\")\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while watching job for states \"\n                    \"{until_states!r}\"\n                )\n            time.sleep(self.job_watch_poll_interval)\n\n        return job_run\n\n    @sync_compatible\n    async def run(\n        self, task_status: Optional[\"TaskStatus\"] = None\n    ) -&gt; VertexAICustomTrainingJobResult:\n        \"\"\"\n        Run the configured task on VertexAI.\n\n        Args:\n            task_status: An optional `TaskStatus` to update when the container starts.\n\n        Returns:\n            The `VertexAICustomTrainingJobResult`.\n        \"\"\"\n        client_options = ClientOptions(\n            api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n        )\n\n        job_spec = self._build_job_spec()\n        with self.gcp_credentials.get_job_service_client(\n            client_options=client_options\n        ) as job_service_client:\n            job_run = await self._create_and_begin_job(job_spec, job_service_client)\n\n            if task_status:\n                task_status.started(self.job_name)\n\n            final_job_run = await self._watch_job_run(\n                full_job_name=job_run.name,\n                job_service_client=job_service_client,\n                current_state=job_run.state,\n                until_states=(\n                    JobState.JOB_STATE_SUCCEEDED,\n                    JobState.JOB_STATE_FAILED,\n                    JobState.JOB_STATE_CANCELLED,\n                    JobState.JOB_STATE_EXPIRED,\n                ),\n                timeout=self.maximum_run_time.total_seconds(),\n            )\n\n        error_msg = final_job_run.error.message\n        if error_msg:\n            raise RuntimeError(f\"{self._log_prefix}: {error_msg}\")\n\n        status_code = 0 if final_job_run.state == JobState.JOB_STATE_SUCCEEDED else 1\n\n        return VertexAICustomTrainingJobResult(\n            identifier=final_job_run.display_name, status_code=status_code\n        )\n\n    @sync_compatible\n    async def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n        \"\"\"\n        Kill a job running Cloud Run.\n\n        Args:\n            identifier: The Vertex AI full job name, formatted like\n                \"projects/{project}/locations/{location}/customJobs/{custom_job}\".\n\n        Returns:\n            The `VertexAICustomTrainingJobResult`.\n        \"\"\"\n        client_options = ClientOptions(\n            api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n        )\n        with self.gcp_credentials.get_job_service_client(\n            client_options=client_options\n        ) as job_service_client:\n            await run_sync_in_worker_thread(\n                self._kill_job,\n                job_service_client=job_service_client,\n                full_job_name=identifier,\n            )\n            self.logger.info(f\"Requested to cancel {identifier}...\")\n\n    def _kill_job(\n        self, job_service_client: \"JobServiceClient\", full_job_name: str\n    ) -&gt; None:\n        \"\"\"\n        Thin wrapper around Job.delete, wrapping a try/except since\n        Job is an independent class that doesn't have knowledge of\n        CloudRunJob and its associated logic.\n        \"\"\"\n        cancel_custom_job_request = CancelCustomJobRequest(name=full_job_name)\n        try:\n            job_service_client.cancel_custom_job(\n                request=cancel_custom_job_request,\n            )\n        except Exception as exc:\n            if \"does not exist\" in str(exc):\n                raise InfrastructureNotFound(\n                    f\"Cannot stop Vertex AI job; the job name {full_job_name!r} \"\n                    \"could not be found.\"\n                ) from exc\n            raise\n\n    @property\n    def _log_prefix(self) -&gt; str:\n        \"\"\"\n        Internal property for generating a prefix for logs where `name` may be null\n        \"\"\"\n        if self.name is not None:\n            return f\"VertexAICustomTrainingJob {self.name!r}\"\n        else:\n            return \"VertexAICustomTrainingJob\"\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob-attributes","title":"Attributes","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.job_name","title":"<code>job_name</code>  <code>property</code>","text":"<p>The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name</p>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob-functions","title":"Functions","text":""},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.kill","title":"<code>kill</code>  <code>async</code>","text":"<p>Kill a job running Cloud Run.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The Vertex AI full job name, formatted like \"projects/{project}/locations/{location}/customJobs/{custom_job}\".</p> required <p>Returns:</p> Type Description <code>None</code> <p>The <code>VertexAICustomTrainingJobResult</code>.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>@sync_compatible\nasync def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n    \"\"\"\n    Kill a job running Cloud Run.\n\n    Args:\n        identifier: The Vertex AI full job name, formatted like\n            \"projects/{project}/locations/{location}/customJobs/{custom_job}\".\n\n    Returns:\n        The `VertexAICustomTrainingJobResult`.\n    \"\"\"\n    client_options = ClientOptions(\n        api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n    )\n    with self.gcp_credentials.get_job_service_client(\n        client_options=client_options\n    ) as job_service_client:\n        await run_sync_in_worker_thread(\n            self._kill_job,\n            job_service_client=job_service_client,\n            full_job_name=identifier,\n        )\n        self.logger.info(f\"Requested to cancel {identifier}...\")\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.preview","title":"<code>preview</code>","text":"<p>Generate a preview of the job definition that will be sent to GCP.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>def preview(self) -&gt; str:\n    \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n    job_spec = self._build_job_spec()\n    custom_job = CustomJob(\n        display_name=self.job_name,\n        job_spec=job_spec,\n        labels=self._get_compatible_labels(),\n    )\n    return str(custom_job)  # outputs a json string\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJob.run","title":"<code>run</code>  <code>async</code>","text":"<p>Run the configured task on VertexAI.</p> <p>Parameters:</p> Name Type Description Default <code>task_status</code> <code>Optional[TaskStatus]</code> <p>An optional <code>TaskStatus</code> to update when the container starts.</p> <code>None</code> <p>Returns:</p> Type Description <code>VertexAICustomTrainingJobResult</code> <p>The <code>VertexAICustomTrainingJobResult</code>.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>@sync_compatible\nasync def run(\n    self, task_status: Optional[\"TaskStatus\"] = None\n) -&gt; VertexAICustomTrainingJobResult:\n    \"\"\"\n    Run the configured task on VertexAI.\n\n    Args:\n        task_status: An optional `TaskStatus` to update when the container starts.\n\n    Returns:\n        The `VertexAICustomTrainingJobResult`.\n    \"\"\"\n    client_options = ClientOptions(\n        api_endpoint=f\"{self.region}-aiplatform.googleapis.com\"\n    )\n\n    job_spec = self._build_job_spec()\n    with self.gcp_credentials.get_job_service_client(\n        client_options=client_options\n    ) as job_service_client:\n        job_run = await self._create_and_begin_job(job_spec, job_service_client)\n\n        if task_status:\n            task_status.started(self.job_name)\n\n        final_job_run = await self._watch_job_run(\n            full_job_name=job_run.name,\n            job_service_client=job_service_client,\n            current_state=job_run.state,\n            until_states=(\n                JobState.JOB_STATE_SUCCEEDED,\n                JobState.JOB_STATE_FAILED,\n                JobState.JOB_STATE_CANCELLED,\n                JobState.JOB_STATE_EXPIRED,\n            ),\n            timeout=self.maximum_run_time.total_seconds(),\n        )\n\n    error_msg = final_job_run.error.message\n    if error_msg:\n        raise RuntimeError(f\"{self._log_prefix}: {error_msg}\")\n\n    status_code = 0 if final_job_run.state == JobState.JOB_STATE_SUCCEEDED else 1\n\n    return VertexAICustomTrainingJobResult(\n        identifier=final_job_run.display_name, status_code=status_code\n    )\n</code></pre>"},{"location":"aiplatform/#prefect_gcp.aiplatform.VertexAICustomTrainingJobResult","title":"<code>VertexAICustomTrainingJobResult</code>","text":"<p>             Bases: <code>InfrastructureResult</code></p> <p>Result from a Vertex AI custom training job.</p> Source code in <code>prefect_gcp/aiplatform.py</code> <pre><code>class VertexAICustomTrainingJobResult(InfrastructureResult):\n    \"\"\"Result from a Vertex AI custom training job.\"\"\"\n</code></pre>"},{"location":"bigquery/","title":"BigQuery","text":""},{"location":"bigquery/#prefect_gcp.bigquery","title":"<code>prefect_gcp.bigquery</code>","text":"<p>Tasks for interacting with GCP BigQuery</p>"},{"location":"bigquery/#prefect_gcp.bigquery-classes","title":"Classes","text":""},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse","title":"<code>BigQueryWarehouse</code>","text":"<p>             Bases: <code>DatabaseBlock</code></p> <p>A block for querying a database with BigQuery.</p> <p>Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called.</p> <p>It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited.</p> <p>It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost.</p> <p>Attributes:</p> Name Type Description <code>gcp_credentials</code> <code>GcpCredentials</code> <p>The credentials to use to authenticate.</p> <code>fetch_size</code> <code>int</code> <p>The number of rows to fetch at a time when calling fetch_many. Note, this parameter is executed on the client side and is not passed to the database. To limit on the server side, add the <code>LIMIT</code> clause, or the dialect's equivalent clause, like <code>TOP</code>, to the query.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>class BigQueryWarehouse(DatabaseBlock):\n    \"\"\"\n    A block for querying a database with BigQuery.\n\n    Upon instantiating, a connection to BigQuery is established\n    and maintained for the life of the object until the close method is called.\n\n    It is recommended to use this block as a context manager, which will automatically\n    close the connection and its cursors when the context is exited.\n\n    It is also recommended that this block is loaded and consumed within a single task\n    or flow because if the block is passed across separate tasks and flows,\n    the state of the block's connection and cursor could be lost.\n\n    Attributes:\n        gcp_credentials: The credentials to use to authenticate.\n        fetch_size: The number of rows to fetch at a time when calling fetch_many.\n            Note, this parameter is executed on the client side and is not\n            passed to the database. To limit on the server side, add the `LIMIT`\n            clause, or the dialect's equivalent clause, like `TOP`, to the query.\n    \"\"\"  # noqa\n\n    _block_type_name = \"BigQuery Warehouse\"\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/bigquery/#prefect_gcp.bigquery.BigQueryWarehouse\"  # noqa: E501\n\n    gcp_credentials: GcpCredentials\n    fetch_size: int = Field(\n        default=1, description=\"The number of rows to fetch at a time.\"\n    )\n\n    _connection: Optional[\"Connection\"] = None\n    _unique_cursors: Dict[str, \"Cursor\"] = None\n\n    def _start_connection(self):\n        \"\"\"\n        Starts a connection.\n        \"\"\"\n        with self.gcp_credentials.get_bigquery_client() as client:\n            self._connection = Connection(client=client)\n\n    def block_initialization(self) -&gt; None:\n        super().block_initialization()\n        if self._connection is None:\n            self._start_connection()\n\n        if self._unique_cursors is None:\n            self._unique_cursors = {}\n\n    def get_connection(self) -&gt; \"Connection\":\n        \"\"\"\n        Get the opened connection to BigQuery.\n        \"\"\"\n        return self._connection\n\n    def _get_cursor(self, inputs: Dict[str, Any]) -&gt; Tuple[bool, \"Cursor\"]:\n        \"\"\"\n        Get a BigQuery cursor.\n\n        Args:\n            inputs: The inputs to generate a unique hash, used to decide\n                whether a new cursor should be used.\n\n        Returns:\n            Whether a cursor is new and a BigQuery cursor.\n        \"\"\"\n        input_hash = hash_objects(inputs)\n        assert input_hash is not None, (\n            \"We were not able to hash your inputs, \"\n            \"which resulted in an unexpected data return; \"\n            \"please open an issue with a reproducible example.\"\n        )\n        if input_hash not in self._unique_cursors.keys():\n            new_cursor = self._connection.cursor()\n            self._unique_cursors[input_hash] = new_cursor\n            return True, new_cursor\n        else:\n            existing_cursor = self._unique_cursors[input_hash]\n            return False, existing_cursor\n\n    def reset_cursors(self) -&gt; None:\n        \"\"\"\n        Tries to close all opened cursors.\n        \"\"\"\n        input_hashes = tuple(self._unique_cursors.keys())\n        for input_hash in input_hashes:\n            cursor = self._unique_cursors.pop(input_hash)\n            try:\n                cursor.close()\n            except Exception as exc:\n                self.logger.warning(\n                    f\"Failed to close cursor for input hash {input_hash!r}: {exc}\"\n                )\n\n    @sync_compatible\n    async def fetch_one(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; \"Row\":\n        \"\"\"\n        Fetch a single result from the database.\n\n        Repeated calls using the same inputs to *any* of the fetch methods of this\n        block will skip executing the operation again, and instead,\n        return the next set of results from the previous execution,\n        until the reset_cursors method is called.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Returns:\n            A tuple containing the data returned by the database,\n                where each row is a tuple and each column is a value in the tuple.\n\n        Examples:\n            Execute operation with parameters, fetching one new row at a time:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    SELECT word, word_count\n                    FROM `bigquery-public-data.samples.shakespeare`\n                    WHERE corpus = %(corpus)s\n                    AND word_count &gt;= %(min_word_count)s\n                    ORDER BY word_count DESC\n                    LIMIT 3;\n                '''\n                parameters = {\n                    \"corpus\": \"romeoandjuliet\",\n                    \"min_word_count\": 250,\n                }\n                for _ in range(0, 3):\n                    result = warehouse.fetch_one(operation, parameters=parameters)\n                    print(result)\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        new, cursor = self._get_cursor(inputs)\n        if new:\n            await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n        result = await run_sync_in_worker_thread(cursor.fetchone)\n        return result\n\n    @sync_compatible\n    async def fetch_many(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        size: Optional[int] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; List[\"Row\"]:\n        \"\"\"\n        Fetch a limited number of results from the database.\n\n        Repeated calls using the same inputs to *any* of the fetch methods of this\n        block will skip executing the operation again, and instead,\n        return the next set of results from the previous execution,\n        until the reset_cursors method is called.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            size: The number of results to return; if None or 0, uses the value of\n                `fetch_size` configured on the block.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Returns:\n            A list of tuples containing the data returned by the database,\n                where each row is a tuple and each column is a value in the tuple.\n\n        Examples:\n            Execute operation with parameters, fetching two new rows at a time:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    SELECT word, word_count\n                    FROM `bigquery-public-data.samples.shakespeare`\n                    WHERE corpus = %(corpus)s\n                    AND word_count &gt;= %(min_word_count)s\n                    ORDER BY word_count DESC\n                    LIMIT 6;\n                '''\n                parameters = {\n                    \"corpus\": \"romeoandjuliet\",\n                    \"min_word_count\": 250,\n                }\n                for _ in range(0, 3):\n                    result = warehouse.fetch_many(\n                        operation,\n                        parameters=parameters,\n                        size=2\n                    )\n                    print(result)\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        new, cursor = self._get_cursor(inputs)\n        if new:\n            await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n        size = size or self.fetch_size\n        result = await run_sync_in_worker_thread(cursor.fetchmany, size=size)\n        return result\n\n    @sync_compatible\n    async def fetch_all(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; List[\"Row\"]:\n        \"\"\"\n        Fetch all results from the database.\n\n        Repeated calls using the same inputs to *any* of the fetch methods of this\n        block will skip executing the operation again, and instead,\n        return the next set of results from the previous execution,\n        until the reset_cursors method is called.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Returns:\n            A list of tuples containing the data returned by the database,\n                where each row is a tuple and each column is a value in the tuple.\n\n        Examples:\n            Execute operation with parameters, fetching all rows:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    SELECT word, word_count\n                    FROM `bigquery-public-data.samples.shakespeare`\n                    WHERE corpus = %(corpus)s\n                    AND word_count &gt;= %(min_word_count)s\n                    ORDER BY word_count DESC\n                    LIMIT 3;\n                '''\n                parameters = {\n                    \"corpus\": \"romeoandjuliet\",\n                    \"min_word_count\": 250,\n                }\n                result = warehouse.fetch_all(operation, parameters=parameters)\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        new, cursor = self._get_cursor(inputs)\n        if new:\n            await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n        result = await run_sync_in_worker_thread(cursor.fetchall)\n        return result\n\n    @sync_compatible\n    async def execute(\n        self,\n        operation: str,\n        parameters: Optional[Dict[str, Any]] = None,\n        **execution_options: Dict[str, Any],\n    ) -&gt; None:\n        \"\"\"\n        Executes an operation on the database. This method is intended to be used\n        for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n        Unlike the fetch methods, this method will always execute the operation\n        upon calling.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            parameters: The parameters for the operation.\n            **execution_options: Additional options to pass to `connection.execute`.\n\n        Examples:\n            Execute operation with parameters:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n                operation = '''\n                    CREATE TABLE mydataset.trips AS (\n                    SELECT\n                        bikeid,\n                        start_time,\n                        duration_minutes\n                    FROM\n                        bigquery-public-data.austin_bikeshare.bikeshare_trips\n                    LIMIT %(limit)s\n                    );\n                '''\n                warehouse.execute(operation, parameters={\"limit\": 5})\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            parameters=parameters,\n            **execution_options,\n        )\n        cursor = self._get_cursor(inputs)[1]\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    @sync_compatible\n    async def execute_many(\n        self,\n        operation: str,\n        seq_of_parameters: List[Dict[str, Any]],\n    ) -&gt; None:\n        \"\"\"\n        Executes many operations on the database. This method is intended to be used\n        for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n        Unlike the fetch methods, this method will always execute the operations\n        upon calling.\n\n        Args:\n            operation: The SQL query or other operation to be executed.\n            seq_of_parameters: The sequence of parameters for the operation.\n\n        Examples:\n            Create mytable in mydataset and insert two rows into it:\n            ```python\n            from prefect_gcp.bigquery import BigQueryWarehouse\n\n            with BigQueryWarehouse.load(\"bigquery\") as warehouse:\n                create_operation = '''\n                CREATE TABLE IF NOT EXISTS mydataset.mytable (\n                    col1 STRING,\n                    col2 INTEGER,\n                    col3 BOOLEAN\n                )\n                '''\n                warehouse.execute(create_operation)\n                insert_operation = '''\n                INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n                '''\n                seq_of_parameters = [\n                    (\"a\", 1, True),\n                    (\"b\", 2, False),\n                ]\n                warehouse.execute_many(\n                    insert_operation,\n                    seq_of_parameters=seq_of_parameters\n                )\n            ```\n        \"\"\"\n        inputs = dict(\n            operation=operation,\n            seq_of_parameters=seq_of_parameters,\n        )\n        cursor = self._get_cursor(inputs)[1]\n        await run_sync_in_worker_thread(cursor.executemany, **inputs)\n\n    def close(self):\n        \"\"\"\n        Closes connection and its cursors.\n        \"\"\"\n        try:\n            self.reset_cursors()\n        finally:\n            if self._connection is not None:\n                self._connection.close()\n                self._connection = None\n\n    def __enter__(self):\n        \"\"\"\n        Start a connection upon entry.\n        \"\"\"\n        return self\n\n    def __exit__(self, *args):\n        \"\"\"\n        Closes connection and its cursors upon exit.\n        \"\"\"\n        self.close()\n\n    def __getstate__(self):\n        \"\"\" \"\"\"\n        data = self.__dict__.copy()\n        data.update({k: None for k in {\"_connection\", \"_unique_cursors\"}})\n        return data\n\n    def __setstate__(self, data: dict):\n        \"\"\" \"\"\"\n        self.__dict__.update(data)\n        self._unique_cursors = {}\n        self._start_connection()\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse-functions","title":"Functions","text":""},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.close","title":"<code>close</code>","text":"<p>Closes connection and its cursors.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>def close(self):\n    \"\"\"\n    Closes connection and its cursors.\n    \"\"\"\n    try:\n        self.reset_cursors()\n    finally:\n        if self._connection is not None:\n            self._connection.close()\n            self._connection = None\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.execute","title":"<code>execute</code>  <code>async</code>","text":"<p>Executes an operation on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE.</p> <p>Unlike the fetch methods, this method will always execute the operation upon calling.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Examples:</p> <p>Execute operation with parameters: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        CREATE TABLE mydataset.trips AS (\n        SELECT\n            bikeid,\n            start_time,\n            duration_minutes\n        FROM\n            bigquery-public-data.austin_bikeshare.bikeshare_trips\n        LIMIT %(limit)s\n        );\n    '''\n    warehouse.execute(operation, parameters={\"limit\": 5})\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def execute(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; None:\n    \"\"\"\n    Executes an operation on the database. This method is intended to be used\n    for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n    Unlike the fetch methods, this method will always execute the operation\n    upon calling.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Examples:\n        Execute operation with parameters:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                CREATE TABLE mydataset.trips AS (\n                SELECT\n                    bikeid,\n                    start_time,\n                    duration_minutes\n                FROM\n                    bigquery-public-data.austin_bikeshare.bikeshare_trips\n                LIMIT %(limit)s\n                );\n            '''\n            warehouse.execute(operation, parameters={\"limit\": 5})\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    cursor = self._get_cursor(inputs)[1]\n    await run_sync_in_worker_thread(cursor.execute, **inputs)\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.execute_many","title":"<code>execute_many</code>  <code>async</code>","text":"<p>Executes many operations on the database. This method is intended to be used for operations that do not return data, such as INSERT, UPDATE, or DELETE.</p> <p>Unlike the fetch methods, this method will always execute the operations upon calling.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>seq_of_parameters</code> <code>List[Dict[str, Any]]</code> <p>The sequence of parameters for the operation.</p> required <p>Examples:</p> <p>Create mytable in mydataset and insert two rows into it: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"bigquery\") as warehouse:\n    create_operation = '''\n    CREATE TABLE IF NOT EXISTS mydataset.mytable (\n        col1 STRING,\n        col2 INTEGER,\n        col3 BOOLEAN\n    )\n    '''\n    warehouse.execute(create_operation)\n    insert_operation = '''\n    INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n    '''\n    seq_of_parameters = [\n        (\"a\", 1, True),\n        (\"b\", 2, False),\n    ]\n    warehouse.execute_many(\n        insert_operation,\n        seq_of_parameters=seq_of_parameters\n    )\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def execute_many(\n    self,\n    operation: str,\n    seq_of_parameters: List[Dict[str, Any]],\n) -&gt; None:\n    \"\"\"\n    Executes many operations on the database. This method is intended to be used\n    for operations that do not return data, such as INSERT, UPDATE, or DELETE.\n\n    Unlike the fetch methods, this method will always execute the operations\n    upon calling.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        seq_of_parameters: The sequence of parameters for the operation.\n\n    Examples:\n        Create mytable in mydataset and insert two rows into it:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"bigquery\") as warehouse:\n            create_operation = '''\n            CREATE TABLE IF NOT EXISTS mydataset.mytable (\n                col1 STRING,\n                col2 INTEGER,\n                col3 BOOLEAN\n            )\n            '''\n            warehouse.execute(create_operation)\n            insert_operation = '''\n            INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n            '''\n            seq_of_parameters = [\n                (\"a\", 1, True),\n                (\"b\", 2, False),\n            ]\n            warehouse.execute_many(\n                insert_operation,\n                seq_of_parameters=seq_of_parameters\n            )\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        seq_of_parameters=seq_of_parameters,\n    )\n    cursor = self._get_cursor(inputs)[1]\n    await run_sync_in_worker_thread(cursor.executemany, **inputs)\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_all","title":"<code>fetch_all</code>  <code>async</code>","text":"<p>Fetch all results from the database.</p> <p>Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Row]</code> <p>A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple.</p> <p>Examples:</p> <p>Execute operation with parameters, fetching all rows: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    result = warehouse.fetch_all(operation, parameters=parameters)\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def fetch_all(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; List[\"Row\"]:\n    \"\"\"\n    Fetch all results from the database.\n\n    Repeated calls using the same inputs to *any* of the fetch methods of this\n    block will skip executing the operation again, and instead,\n    return the next set of results from the previous execution,\n    until the reset_cursors method is called.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Returns:\n        A list of tuples containing the data returned by the database,\n            where each row is a tuple and each column is a value in the tuple.\n\n    Examples:\n        Execute operation with parameters, fetching all rows:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = %(corpus)s\n                AND word_count &gt;= %(min_word_count)s\n                ORDER BY word_count DESC\n                LIMIT 3;\n            '''\n            parameters = {\n                \"corpus\": \"romeoandjuliet\",\n                \"min_word_count\": 250,\n            }\n            result = warehouse.fetch_all(operation, parameters=parameters)\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    new, cursor = self._get_cursor(inputs)\n    if new:\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    result = await run_sync_in_worker_thread(cursor.fetchall)\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_many","title":"<code>fetch_many</code>  <code>async</code>","text":"<p>Fetch a limited number of results from the database.</p> <p>Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>size</code> <code>Optional[int]</code> <p>The number of results to return; if None or 0, uses the value of <code>fetch_size</code> configured on the block.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Row]</code> <p>A list of tuples containing the data returned by the database, where each row is a tuple and each column is a value in the tuple.</p> <p>Examples:</p> <p>Execute operation with parameters, fetching two new rows at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 6;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_many(\n            operation,\n            parameters=parameters,\n            size=2\n        )\n        print(result)\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def fetch_many(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    size: Optional[int] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; List[\"Row\"]:\n    \"\"\"\n    Fetch a limited number of results from the database.\n\n    Repeated calls using the same inputs to *any* of the fetch methods of this\n    block will skip executing the operation again, and instead,\n    return the next set of results from the previous execution,\n    until the reset_cursors method is called.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        size: The number of results to return; if None or 0, uses the value of\n            `fetch_size` configured on the block.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Returns:\n        A list of tuples containing the data returned by the database,\n            where each row is a tuple and each column is a value in the tuple.\n\n    Examples:\n        Execute operation with parameters, fetching two new rows at a time:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = %(corpus)s\n                AND word_count &gt;= %(min_word_count)s\n                ORDER BY word_count DESC\n                LIMIT 6;\n            '''\n            parameters = {\n                \"corpus\": \"romeoandjuliet\",\n                \"min_word_count\": 250,\n            }\n            for _ in range(0, 3):\n                result = warehouse.fetch_many(\n                    operation,\n                    parameters=parameters,\n                    size=2\n                )\n                print(result)\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    new, cursor = self._get_cursor(inputs)\n    if new:\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    size = size or self.fetch_size\n    result = await run_sync_in_worker_thread(cursor.fetchmany, size=size)\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.fetch_one","title":"<code>fetch_one</code>  <code>async</code>","text":"<p>Fetch a single result from the database.</p> <p>Repeated calls using the same inputs to any of the fetch methods of this block will skip executing the operation again, and instead, return the next set of results from the previous execution, until the reset_cursors method is called.</p> <p>Parameters:</p> Name Type Description Default <code>operation</code> <code>str</code> <p>The SQL query or other operation to be executed.</p> required <code>parameters</code> <code>Optional[Dict[str, Any]]</code> <p>The parameters for the operation.</p> <code>None</code> <code>**execution_options</code> <code>Dict[str, Any]</code> <p>Additional options to pass to <code>connection.execute</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Row</code> <p>A tuple containing the data returned by the database, where each row is a tuple and each column is a value in the tuple.</p> <p>Examples:</p> <p>Execute operation with parameters, fetching one new row at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_one(operation, parameters=parameters)\n        print(result)\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@sync_compatible\nasync def fetch_one(\n    self,\n    operation: str,\n    parameters: Optional[Dict[str, Any]] = None,\n    **execution_options: Dict[str, Any],\n) -&gt; \"Row\":\n    \"\"\"\n    Fetch a single result from the database.\n\n    Repeated calls using the same inputs to *any* of the fetch methods of this\n    block will skip executing the operation again, and instead,\n    return the next set of results from the previous execution,\n    until the reset_cursors method is called.\n\n    Args:\n        operation: The SQL query or other operation to be executed.\n        parameters: The parameters for the operation.\n        **execution_options: Additional options to pass to `connection.execute`.\n\n    Returns:\n        A tuple containing the data returned by the database,\n            where each row is a tuple and each column is a value in the tuple.\n\n    Examples:\n        Execute operation with parameters, fetching one new row at a time:\n        ```python\n        from prefect_gcp.bigquery import BigQueryWarehouse\n\n        with BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n            operation = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = %(corpus)s\n                AND word_count &gt;= %(min_word_count)s\n                ORDER BY word_count DESC\n                LIMIT 3;\n            '''\n            parameters = {\n                \"corpus\": \"romeoandjuliet\",\n                \"min_word_count\": 250,\n            }\n            for _ in range(0, 3):\n                result = warehouse.fetch_one(operation, parameters=parameters)\n                print(result)\n        ```\n    \"\"\"\n    inputs = dict(\n        operation=operation,\n        parameters=parameters,\n        **execution_options,\n    )\n    new, cursor = self._get_cursor(inputs)\n    if new:\n        await run_sync_in_worker_thread(cursor.execute, **inputs)\n\n    result = await run_sync_in_worker_thread(cursor.fetchone)\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.get_connection","title":"<code>get_connection</code>","text":"<p>Get the opened connection to BigQuery.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>def get_connection(self) -&gt; \"Connection\":\n    \"\"\"\n    Get the opened connection to BigQuery.\n    \"\"\"\n    return self._connection\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.BigQueryWarehouse.reset_cursors","title":"<code>reset_cursors</code>","text":"<p>Tries to close all opened cursors.</p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>def reset_cursors(self) -&gt; None:\n    \"\"\"\n    Tries to close all opened cursors.\n    \"\"\"\n    input_hashes = tuple(self._unique_cursors.keys())\n    for input_hash in input_hashes:\n        cursor = self._unique_cursors.pop(input_hash)\n        try:\n            cursor.close()\n        except Exception as exc:\n            self.logger.warning(\n                f\"Failed to close cursor for input hash {input_hash!r}: {exc}\"\n            )\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery-functions","title":"Functions","text":""},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_create_table","title":"<code>bigquery_create_table</code>  <code>async</code>","text":"<p>Creates table in BigQuery. Args:     dataset: Name of a dataset in that the table will be created.     table: Name of a table to create.     schema: Schema to use when creating the table.     gcp_credentials: Credentials to use for authentication with GCP.     clustering_fields: List of fields to cluster the table by.     time_partitioning: <code>bigquery.TimePartitioning</code> object specifying a partitioning         of the newly created table     project: Project to initialize the BigQuery Client with; if         not provided, will default to the one inferred from your credentials.     location: The location of the dataset that will be written to.     external_config: The external data source.  # noqa Returns:     Table name. Example:     <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_create_table\nfrom google.cloud.bigquery import SchemaField\n@flow\ndef example_bigquery_create_table_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    schema = [\n        SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"),\n        SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"),\n        SchemaField(\"bool\", field_type=\"BOOLEAN\")\n    ]\n    result = bigquery_create_table(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        schema=schema,\n        gcp_credentials=gcp_credentials\n    )\n    return result\nexample_bigquery_create_table_flow()\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_create_table(\n    dataset: str,\n    table: str,\n    gcp_credentials: GcpCredentials,\n    schema: Optional[List[\"SchemaField\"]] = None,\n    clustering_fields: List[str] = None,\n    time_partitioning: \"TimePartitioning\" = None,\n    project: Optional[str] = None,\n    location: str = \"US\",\n    external_config: Optional[\"ExternalConfig\"] = None,\n) -&gt; str:\n    \"\"\"\n    Creates table in BigQuery.\n    Args:\n        dataset: Name of a dataset in that the table will be created.\n        table: Name of a table to create.\n        schema: Schema to use when creating the table.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        clustering_fields: List of fields to cluster the table by.\n        time_partitioning: `bigquery.TimePartitioning` object specifying a partitioning\n            of the newly created table\n        project: Project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: The location of the dataset that will be written to.\n        external_config: The [external data source](https://registry.terraform.io/providers/hashicorp/google/latest/docs/resources/bigquery_table#nested_external_data_configuration).  # noqa\n    Returns:\n        Table name.\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_create_table\n        from google.cloud.bigquery import SchemaField\n        @flow\n        def example_bigquery_create_table_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            schema = [\n                SchemaField(\"number\", field_type=\"INTEGER\", mode=\"REQUIRED\"),\n                SchemaField(\"text\", field_type=\"STRING\", mode=\"REQUIRED\"),\n                SchemaField(\"bool\", field_type=\"BOOLEAN\")\n            ]\n            result = bigquery_create_table(\n                dataset=\"dataset\",\n                table=\"test_table\",\n                schema=schema,\n                gcp_credentials=gcp_credentials\n            )\n            return result\n        example_bigquery_create_table_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Creating %s.%s\", dataset, table)\n\n    if not external_config and not schema:\n        raise ValueError(\"Either a schema or an external config must be provided.\")\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n    try:\n        partial_get_dataset = partial(client.get_dataset, dataset)\n        dataset_ref = await to_thread.run_sync(partial_get_dataset)\n    except NotFound:\n        logger.debug(\"Dataset %s not found, creating\", dataset)\n        partial_create_dataset = partial(client.create_dataset, dataset)\n        dataset_ref = await to_thread.run_sync(partial_create_dataset)\n\n    table_ref = dataset_ref.table(table)\n    try:\n        partial_get_table = partial(client.get_table, table_ref)\n        await to_thread.run_sync(partial_get_table)\n        logger.info(\"%s.%s already exists\", dataset, table)\n    except NotFound:\n        logger.debug(\"Table %s not found, creating\", table)\n        table_obj = Table(table_ref, schema=schema)\n\n        # external data configuration\n        if external_config:\n            table_obj.external_data_configuration = external_config\n\n        # cluster for optimal data sorting/access\n        if clustering_fields:\n            table_obj.clustering_fields = clustering_fields\n\n        # partitioning\n        if time_partitioning:\n            table_obj.time_partitioning = time_partitioning\n\n        partial_create_table = partial(client.create_table, table_obj)\n        await to_thread.run_sync(partial_create_table)\n\n    return table\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_insert_stream","title":"<code>bigquery_insert_stream</code>  <code>async</code>","text":"<p>Insert records in a Google BigQuery table via the streaming API.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>Name of a dataset where the records will be written to.</p> required <code>table</code> <code>str</code> <p>Name of a table to write to.</p> required <code>records</code> <code>List[dict]</code> <p>The list of records to insert as rows into the BigQuery table; each item in the list should be a dictionary whose keys correspond to columns in the table.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>project</code> <code>Optional[str]</code> <p>The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>location</code> <code>str</code> <p>Location of the dataset that will be written to.</p> <code>'US'</code> <p>Returns:</p> Type Description <code>List</code> <p>List of inserted rows.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_insert_stream\nfrom google.cloud.bigquery import SchemaField\n\n@flow\ndef example_bigquery_insert_stream_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    records = [\n        {\"number\": 1, \"text\": \"abc\", \"bool\": True},\n        {\"number\": 2, \"text\": \"def\", \"bool\": False},\n    ]\n    result = bigquery_insert_stream(\n        dataset=\"integrations\",\n        table=\"test_table\",\n        records=records,\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_insert_stream_flow()\n</code></pre> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_insert_stream(\n    dataset: str,\n    table: str,\n    records: List[dict],\n    gcp_credentials: GcpCredentials,\n    project: Optional[str] = None,\n    location: str = \"US\",\n) -&gt; List:\n    \"\"\"\n    Insert records in a Google BigQuery table via the [streaming\n    API](https://cloud.google.com/bigquery/streaming-data-into-bigquery).\n\n    Args:\n        dataset: Name of a dataset where the records will be written to.\n        table: Name of a table to write to.\n        records: The list of records to insert as rows into the BigQuery table;\n            each item in the list should be a dictionary whose keys correspond to\n            columns in the table.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        project: The project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: Location of the dataset that will be written to.\n\n    Returns:\n        List of inserted rows.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_insert_stream\n        from google.cloud.bigquery import SchemaField\n\n        @flow\n        def example_bigquery_insert_stream_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            records = [\n                {\"number\": 1, \"text\": \"abc\", \"bool\": True},\n                {\"number\": 2, \"text\": \"def\", \"bool\": False},\n            ]\n            result = bigquery_insert_stream(\n                dataset=\"integrations\",\n                table=\"test_table\",\n                records=records,\n                gcp_credentials=gcp_credentials\n            )\n            return result\n\n        example_bigquery_insert_stream_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Inserting into %s.%s as a stream\", dataset, table)\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n    table_ref = client.dataset(dataset).table(table)\n    partial_insert = partial(\n        client.insert_rows_json, table=table_ref, json_rows=records\n    )\n    response = await to_thread.run_sync(partial_insert)\n\n    errors = []\n    output = []\n    for row in response:\n        output.append(row)\n        if \"errors\" in row:\n            errors.append(row[\"errors\"])\n\n    if errors:\n        raise ValueError(errors)\n\n    return output\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_cloud_storage","title":"<code>bigquery_load_cloud_storage</code>  <code>async</code>","text":"<p>Run method for this Task.  Invoked by calling this Task within a Flow context, after initialization. Args:     uri: GCS path to load data from.     dataset: The id of a destination dataset to write the records to.     table: The name of a destination table to write the records to.     gcp_credentials: Credentials to use for authentication with GCP.     schema: The schema to use when creating the table.     job_config: Dictionary of job configuration parameters;         note that the parameters provided here must be pickleable         (e.g., dataset references will be rejected).     project: The project to initialize the BigQuery Client with; if         not provided, will default to the one inferred from your credentials.     location: Location of the dataset that will be written to.</p> <p>Returns:</p> Type Description <code>LoadJob</code> <p>The response from <code>load_table_from_uri</code>.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_load_cloud_storage\n\n@flow\ndef example_bigquery_load_cloud_storage_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    result = bigquery_load_cloud_storage(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        uri=\"uri\",\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_load_cloud_storage_flow()\n</code></pre> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_load_cloud_storage(\n    dataset: str,\n    table: str,\n    uri: str,\n    gcp_credentials: GcpCredentials,\n    schema: Optional[List[\"SchemaField\"]] = None,\n    job_config: Optional[dict] = None,\n    project: Optional[str] = None,\n    location: str = \"US\",\n) -&gt; \"LoadJob\":\n    \"\"\"\n    Run method for this Task.  Invoked by _calling_ this\n    Task within a Flow context, after initialization.\n    Args:\n        uri: GCS path to load data from.\n        dataset: The id of a destination dataset to write the records to.\n        table: The name of a destination table to write the records to.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        schema: The schema to use when creating the table.\n        job_config: Dictionary of job configuration parameters;\n            note that the parameters provided here must be pickleable\n            (e.g., dataset references will be rejected).\n        project: The project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: Location of the dataset that will be written to.\n\n    Returns:\n        The response from `load_table_from_uri`.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_load_cloud_storage\n\n        @flow\n        def example_bigquery_load_cloud_storage_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            result = bigquery_load_cloud_storage(\n                dataset=\"dataset\",\n                table=\"test_table\",\n                uri=\"uri\",\n                gcp_credentials=gcp_credentials\n            )\n            return result\n\n        example_bigquery_load_cloud_storage_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Loading into %s.%s from cloud storage\", dataset, table)\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n    table_ref = client.dataset(dataset).table(table)\n\n    job_config = job_config or {}\n    if \"autodetect\" not in job_config:\n        job_config[\"autodetect\"] = True\n    job_config = LoadJobConfig(**job_config)\n    if schema:\n        job_config.schema = schema\n\n    result = None\n    try:\n        partial_load = partial(\n            _result_sync,\n            client.load_table_from_uri,\n            uri,\n            table_ref,\n            job_config=job_config,\n        )\n        result = await to_thread.run_sync(partial_load)\n    except Exception as exception:\n        logger.exception(exception)\n        if result is not None and result.errors is not None:\n            for error in result.errors:\n                logger.exception(error)\n        raise\n\n    if result is not None:\n        # remove unpickleable attributes\n        result._client = None\n        result._completion_lock = None\n\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_load_file","title":"<code>bigquery_load_file</code>  <code>async</code>","text":"<p>Loads file into BigQuery.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>str</code> <p>ID of a destination dataset to write the records to; if not provided here, will default to the one provided at initialization.</p> required <code>table</code> <code>str</code> <p>Name of a destination table to write the records to; if not provided here, will default to the one provided at initialization.</p> required <code>path</code> <code>Union[str, Path]</code> <p>A string or path-like object of the file to be loaded.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>schema</code> <code>Optional[List[SchemaField]]</code> <p>Schema to use when creating the table.</p> <code>None</code> <code>job_config</code> <code>Optional[dict]</code> <p>An optional dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected).</p> <code>None</code> <code>rewind</code> <code>bool</code> <p>if True, seek to the beginning of the file handle before reading the file.</p> <code>False</code> <code>size</code> <code>Optional[int]</code> <p>Number of bytes to read from the file handle. If size is None or large, resumable upload will be used. Otherwise, multipart upload will be used.</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>Project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>location</code> <code>str</code> <p>location of the dataset that will be written to.</p> <code>'US'</code> <p>Returns:</p> Type Description <code>LoadJob</code> <p>The response from <code>load_table_from_file</code>.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_load_file\nfrom google.cloud.bigquery import SchemaField\n\n@flow\ndef example_bigquery_load_file_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    result = bigquery_load_file(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        path=\"path\",\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_load_file_flow()\n</code></pre> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_load_file(\n    dataset: str,\n    table: str,\n    path: Union[str, Path],\n    gcp_credentials: GcpCredentials,\n    schema: Optional[List[\"SchemaField\"]] = None,\n    job_config: Optional[dict] = None,\n    rewind: bool = False,\n    size: Optional[int] = None,\n    project: Optional[str] = None,\n    location: str = \"US\",\n) -&gt; \"LoadJob\":\n    \"\"\"\n    Loads file into BigQuery.\n\n    Args:\n        dataset: ID of a destination dataset to write the records to;\n            if not provided here, will default to the one provided at initialization.\n        table: Name of a destination table to write the records to;\n            if not provided here, will default to the one provided at initialization.\n        path: A string or path-like object of the file to be loaded.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        schema: Schema to use when creating the table.\n        job_config: An optional dictionary of job configuration parameters;\n            note that the parameters provided here must be pickleable\n            (e.g., dataset references will be rejected).\n        rewind: if True, seek to the beginning of the file handle\n            before reading the file.\n        size: Number of bytes to read from the file handle. If size is None or large,\n            resumable upload will be used. Otherwise, multipart upload will be used.\n        project: Project to initialize the BigQuery Client with; if\n            not provided, will default to the one inferred from your credentials.\n        location: location of the dataset that will be written to.\n\n    Returns:\n        The response from `load_table_from_file`.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_load_file\n        from google.cloud.bigquery import SchemaField\n\n        @flow\n        def example_bigquery_load_file_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            result = bigquery_load_file(\n                dataset=\"dataset\",\n                table=\"test_table\",\n                path=\"path\",\n                gcp_credentials=gcp_credentials\n            )\n            return result\n\n        example_bigquery_load_file_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Loading into %s.%s from file\", dataset, table)\n\n    if not os.path.exists(path):\n        raise ValueError(f\"{path} does not exist\")\n    elif not os.path.isfile(path):\n        raise ValueError(f\"{path} is not a file\")\n\n    client = gcp_credentials.get_bigquery_client(project=project)\n    table_ref = client.dataset(dataset).table(table)\n\n    job_config = job_config or {}\n    if \"autodetect\" not in job_config:\n        job_config[\"autodetect\"] = True\n        # TODO: test if autodetect is needed when schema is passed\n    job_config = LoadJobConfig(**job_config)\n    if schema:\n        # TODO: test if schema can be passed directly in job_config\n        job_config.schema = schema\n\n    try:\n        with open(path, \"rb\") as file_obj:\n            partial_load = partial(\n                _result_sync,\n                client.load_table_from_file,\n                file_obj,\n                table_ref,\n                rewind=rewind,\n                size=size,\n                location=location,\n                job_config=job_config,\n            )\n            result = await to_thread.run_sync(partial_load)\n    except IOError:\n        logger.exception(f\"Could not open and read from {path}\")\n        raise\n\n    if result is not None:\n        # remove unpickleable attributes\n        result._client = None\n        result._completion_lock = None\n\n    return result\n</code></pre>"},{"location":"bigquery/#prefect_gcp.bigquery.bigquery_query","title":"<code>bigquery_query</code>  <code>async</code>","text":"<p>Runs a BigQuery query.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>String of the query to execute.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>query_params</code> <code>Optional[List[tuple]]</code> <p>List of 3-tuples specifying BigQuery query parameters; currently only scalar query parameters are supported.  See the Google documentation for more details on how both the query and the query parameters should be formatted.</p> <code>None</code> <code>dry_run_max_bytes</code> <code>Optional[int]</code> <p>If provided, the maximum number of bytes the query is allowed to process; this will be determined by executing a dry run and raising a <code>ValueError</code> if the maximum is exceeded.</p> <code>None</code> <code>dataset</code> <code>Optional[str]</code> <p>Name of a destination dataset to write the query results to, if you don't want them returned; if provided, <code>table</code> must also be provided.</p> <code>None</code> <code>table</code> <code>Optional[str]</code> <p>Name of a destination table to write the query results to, if you don't want them returned; if provided, <code>dataset</code> must also be provided.</p> <code>None</code> <code>to_dataframe</code> <code>bool</code> <p>If provided, returns the results of the query as a pandas dataframe instead of a list of <code>bigquery.table.Row</code> objects.</p> <code>False</code> <code>job_config</code> <code>Optional[dict]</code> <p>Dictionary of job configuration parameters; note that the parameters provided here must be pickleable (e.g., dataset references will be rejected).</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>The project to initialize the BigQuery Client with; if not provided, will default to the one inferred from your credentials.</p> <code>None</code> <code>result_transformer</code> <code>Optional[Callable[[List[Row]], Any]]</code> <p>Function that can be passed to transform the result of a query before returning. The function will be passed the list of rows returned by BigQuery for the given query.</p> <code>None</code> <code>location</code> <code>str</code> <p>Location of the dataset that will be queried.</p> <code>'US'</code> <p>Returns:</p> Type Description <code>Any</code> <p>A list of rows, or pandas DataFrame if to_dataframe,</p> <code>Any</code> <p>matching the query criteria.</p> Example <p>Queries the public names database, returning 10 results. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_query\n\n@flow\ndef example_bigquery_query_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\",\n        project=\"project\"\n    )\n    query = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = @corpus\n        AND word_count &gt;= @min_word_count\n        ORDER BY word_count DESC;\n    '''\n    query_params = [\n        (\"corpus\", \"STRING\", \"romeoandjuliet\"),\n        (\"min_word_count\", \"INT64\", 250)\n    ]\n    result = bigquery_query(\n        query, gcp_credentials, query_params=query_params\n    )\n    return result\n\nexample_bigquery_query_flow()\n</code></pre></p> Source code in <code>prefect_gcp/bigquery.py</code> <pre><code>@task\nasync def bigquery_query(\n    query: str,\n    gcp_credentials: GcpCredentials,\n    query_params: Optional[List[tuple]] = None,  # 3-tuples\n    dry_run_max_bytes: Optional[int] = None,\n    dataset: Optional[str] = None,\n    table: Optional[str] = None,\n    to_dataframe: bool = False,\n    job_config: Optional[dict] = None,\n    project: Optional[str] = None,\n    result_transformer: Optional[Callable[[List[\"Row\"]], Any]] = None,\n    location: str = \"US\",\n) -&gt; Any:\n    \"\"\"\n    Runs a BigQuery query.\n\n    Args:\n        query: String of the query to execute.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        query_params: List of 3-tuples specifying BigQuery query parameters; currently\n            only scalar query parameters are supported.  See the\n            [Google documentation](https://cloud.google.com/bigquery/docs/parameterized-queries#bigquery-query-params-python)\n            for more details on how both the query and the query parameters should be formatted.\n        dry_run_max_bytes: If provided, the maximum number of bytes the query\n            is allowed to process; this will be determined by executing a dry run\n            and raising a `ValueError` if the maximum is exceeded.\n        dataset: Name of a destination dataset to write the query results to,\n            if you don't want them returned; if provided, `table` must also be provided.\n        table: Name of a destination table to write the query results to,\n            if you don't want them returned; if provided, `dataset` must also be provided.\n        to_dataframe: If provided, returns the results of the query as a pandas\n            dataframe instead of a list of `bigquery.table.Row` objects.\n        job_config: Dictionary of job configuration parameters;\n            note that the parameters provided here must be pickleable\n            (e.g., dataset references will be rejected).\n        project: The project to initialize the BigQuery Client with; if not\n            provided, will default to the one inferred from your credentials.\n        result_transformer: Function that can be passed to transform the result of a query before returning. The function will be passed the list of rows returned by BigQuery for the given query.\n        location: Location of the dataset that will be queried.\n\n    Returns:\n        A list of rows, or pandas DataFrame if to_dataframe,\n        matching the query criteria.\n\n    Example:\n        Queries the public names database, returning 10 results.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.bigquery import bigquery_query\n\n        @flow\n        def example_bigquery_query_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\",\n                project=\"project\"\n            )\n            query = '''\n                SELECT word, word_count\n                FROM `bigquery-public-data.samples.shakespeare`\n                WHERE corpus = @corpus\n                AND word_count &gt;= @min_word_count\n                ORDER BY word_count DESC;\n            '''\n            query_params = [\n                (\"corpus\", \"STRING\", \"romeoandjuliet\"),\n                (\"min_word_count\", \"INT64\", 250)\n            ]\n            result = bigquery_query(\n                query, gcp_credentials, query_params=query_params\n            )\n            return result\n\n        example_bigquery_query_flow()\n        ```\n    \"\"\"  # noqa\n    logger = get_run_logger()\n    logger.info(\"Running BigQuery query\")\n\n    client = gcp_credentials.get_bigquery_client(project=project, location=location)\n\n    # setup job config\n    job_config = QueryJobConfig(**job_config or {})\n    if query_params is not None:\n        job_config.query_parameters = [ScalarQueryParameter(*qp) for qp in query_params]\n\n    # perform dry_run if requested\n    if dry_run_max_bytes is not None:\n        saved_info = dict(\n            dry_run=job_config.dry_run, use_query_cache=job_config.use_query_cache\n        )\n        job_config.dry_run = True\n        job_config.use_query_cache = False\n        partial_query = partial(client.query, query, job_config=job_config)\n        response = await to_thread.run_sync(partial_query)\n        total_bytes_processed = response.total_bytes_processed\n        if total_bytes_processed &gt; dry_run_max_bytes:\n            raise RuntimeError(\n                f\"Query will process {total_bytes_processed} bytes which is above \"\n                f\"the set maximum of {dry_run_max_bytes} for this task.\"\n            )\n        job_config.dry_run = saved_info[\"dry_run\"]\n        job_config.use_query_cache = saved_info[\"use_query_cache\"]\n\n    # if writing to a destination table\n    if dataset is not None:\n        table_ref = client.dataset(dataset).table(table)\n        job_config.destination = table_ref\n\n    partial_query = partial(\n        _result_sync,\n        client.query,\n        query,\n        job_config=job_config,\n    )\n    result = await to_thread.run_sync(partial_query)\n\n    if to_dataframe:\n        return result.to_dataframe()\n    else:\n        if result_transformer:\n            return result_transformer(result)\n        else:\n            return list(result)\n</code></pre>"},{"location":"blocks_catalog/","title":"Blocks Catalog","text":"<p>Below is a list of Blocks available for registration in <code>prefect-gcp</code>.</p> <p>To register blocks in this module to view and edit them on Prefect Cloud, first install the required packages, then <pre><code>prefect block register -m prefect_gcp\n</code></pre> Note, to use the <code>load</code> method on Blocks, you must already have a block document saved through code or saved through the UI.</p>"},{"location":"blocks_catalog/#credentials-module","title":"Credentials Module","text":"<p>GcpCredentials</p> <p>Block used to manage authentication with GCP. Google authentication is handled via the <code>google.oauth2</code> module or through the CLI. Specify either one of service <code>account_file</code> or <code>service_account_info</code>; if both are not specified, the client will try to detect the credentials following Google's Application Default Credentials. See Google's Authentication documentation for details on inference and recommended authentication patterns.</p> <p>To load the GcpCredentials: <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow\ndef my_flow():\n    my_block = GcpCredentials.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Credentials Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#aiplatform-module","title":"Aiplatform Module","text":"<p>VertexAICustomTrainingJob</p> <p>Infrastructure block used to run Vertex AI custom training jobs.</p> <p>To load the VertexAICustomTrainingJob: <pre><code>from prefect import flow\nfrom prefect_gcp.aiplatform import VertexAICustomTrainingJob\n\n@flow\ndef my_flow():\n    my_block = VertexAICustomTrainingJob.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Aiplatform Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#bigquery-module","title":"Bigquery Module","text":"<p>BigQueryWarehouse</p> <p>A block for querying a database with BigQuery.</p> <p>Upon instantiating, a connection to BigQuery is established and maintained for the life of the object until the close method is called.</p> <p>It is recommended to use this block as a context manager, which will automatically close the connection and its cursors when the context is exited.</p> <p>It is also recommended that this block is loaded and consumed within a single task or flow because if the block is passed across separate tasks and flows, the state of the block's connection and cursor could be lost.</p> <p>To load the BigQueryWarehouse: <pre><code>from prefect import flow\nfrom prefect_gcp.bigquery import BigQueryWarehouse\n\n@flow\ndef my_flow():\n    my_block = BigQueryWarehouse.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Bigquery Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#cloud-run-module","title":"Cloud Run Module","text":"<p>CloudRunJob</p> <p>Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.</p> <p>To load the CloudRunJob: <pre><code>from prefect import flow\nfrom prefect_gcp.cloud_run import CloudRunJob\n\n@flow\ndef my_flow():\n    my_block = CloudRunJob.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Cloud Run Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#cloud-storage-module","title":"Cloud Storage Module","text":"<p>GcsBucket</p> <p>Block used to store data using GCP Cloud Storage Buckets.</p> <p>Note! <code>GcsBucket</code> in <code>prefect-gcp</code> is a unique block, separate from <code>GCS</code> in core Prefect. <code>GcsBucket</code> does not use <code>gcsfs</code> under the hood, instead using the <code>google-cloud-storage</code> package, and offers more configuration and functionality.</p> <p>To load the GcsBucket: <pre><code>from prefect import flow\nfrom prefect_gcp.cloud_storage import GcsBucket\n\n@flow\ndef my_flow():\n    my_block = GcsBucket.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Cloud Storage Module under Examples Catalog.</p>"},{"location":"blocks_catalog/#secret-manager-module","title":"Secret Manager Module","text":"<p>GcpSecret</p> <p>Manages a secret in Google Cloud Platform's Secret Manager.</p> <p>To load the GcpSecret: <pre><code>from prefect import flow\nfrom prefect_gcp.secret_manager import GcpSecret\n\n@flow\ndef my_flow():\n    my_block = GcpSecret.load(\"MY_BLOCK_NAME\")\n\nmy_flow()\n</code></pre> For additional examples, check out the Secret Manager Module under Examples Catalog.</p>"},{"location":"cloud_run/","title":"Cloud Run","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run","title":"<code>prefect_gcp.cloud_run</code>","text":"<p>Integrations with Google Cloud Run Job.</p> <p>Note this module is experimental. The intefaces within may change without notice.</p> <p>Examples:</p> <pre><code>Run a job using Google Cloud Run Jobs:\n```python\nCloudRunJob(\n    image=\"gcr.io/my-project/my-image\",\n    region=\"us-east1\",\n    credentials=my_gcp_credentials\n).run()\n```\n\nRun a job that runs the command `echo hello world` using Google Cloud Run Jobs:\n```python\nCloudRunJob(\n    image=\"gcr.io/my-project/my-image\",\n    region=\"us-east1\",\n    credentials=my_gcp_credentials\n    command=[\"echo\", \"hello world\"]\n).run()\n```\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run-classes","title":"Classes","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob","title":"<code>CloudRunJob</code>","text":"<p>             Bases: <code>Infrastructure</code></p> <p></p> <p>Infrastructure block used to run GCP Cloud Run Jobs.</p> <p>Project name information is provided by the Credentials object, and should always be correct as long as the Credentials object is for the correct project.</p> <p>Note this block is experimental. The interface may change without notice.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class CloudRunJob(Infrastructure):\n    \"\"\"\n    &lt;span class=\"badge-api experimental\"/&gt;\n\n    Infrastructure block used to run GCP Cloud Run Jobs.\n\n    Project name information is provided by the Credentials object, and should always\n    be correct as long as the Credentials object is for the correct project.\n\n    Note this block is experimental. The interface may change without notice.\n    \"\"\"\n\n    _block_type_slug = \"cloud-run-job\"\n    _block_type_name = \"GCP Cloud Run Job\"\n    _description = \"Infrastructure block used to run GCP Cloud Run Jobs. Note this block is experimental. The interface may change without notice.\"  # noqa\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/cloud_run/#prefect_gcp.cloud_run.CloudRunJob\"  # noqa: E501\n\n    type: Literal[\"cloud-run-job\"] = Field(\n        \"cloud-run-job\", description=\"The slug for this task type.\"\n    )\n    image: str = Field(\n        ...,\n        title=\"Image Name\",\n        description=(\n            \"The image to use for a new Cloud Run Job. This value must \"\n            \"refer to an image within either Google Container Registry \"\n            \"or Google Artifact Registry, like `gcr.io/&lt;project_name&gt;/&lt;repo&gt;/`.\"\n        ),\n    )\n    region: str = Field(..., description=\"The region where the Cloud Run Job resides.\")\n    credentials: GcpCredentials  # cannot be Field; else it shows as Json\n\n    # Job settings\n    cpu: Optional[int] = Field(\n        default=None,\n        title=\"CPU\",\n        description=(\n            \"The amount of compute allocated to the Cloud Run Job. \"\n            \"The int must be valid based on the rules specified at \"\n            \"https://cloud.google.com/run/docs/configuring/cpu#setting-jobs .\"\n        ),\n    )\n    memory: Optional[int] = Field(\n        default=None,\n        title=\"Memory\",\n        description=\"The amount of memory allocated to the Cloud Run Job.\",\n    )\n    memory_unit: Optional[Literal[\"G\", \"Gi\", \"M\", \"Mi\"]] = Field(\n        default=None,\n        title=\"Memory Units\",\n        description=(\n            \"The unit of memory. See \"\n            \"https://cloud.google.com/run/docs/configuring/memory-limits#setting \"\n            \"for additional details.\"\n        ),\n    )\n    vpc_connector_name: Optional[str] = Field(\n        default=None,\n        title=\"VPC Connector Name\",\n        description=\"The name of the VPC connector to use for the Cloud Run Job.\",\n    )\n    args: Optional[List[str]] = Field(\n        default=None,\n        description=(\n            \"Arguments to be passed to your Cloud Run Job's entrypoint command.\"\n        ),\n    )\n    env: Dict[str, str] = Field(\n        default_factory=dict,\n        description=\"Environment variables to be passed to your Cloud Run Job.\",\n    )\n\n    # Cleanup behavior\n    keep_job: Optional[bool] = Field(\n        default=False,\n        title=\"Keep Job After Completion\",\n        description=\"Keep the completed Cloud Run Job on Google Cloud Platform.\",\n    )\n    timeout: Optional[int] = Field(\n        default=600,\n        gt=0,\n        le=3600,\n        title=\"Job Timeout\",\n        description=(\n            \"The length of time that Prefect will wait for a Cloud Run Job to complete \"\n            \"before raising an exception.\"\n        ),\n    )\n    # For private use\n    _job_name: str = None\n    _execution: Optional[Execution] = None\n\n    @property\n    def job_name(self):\n        \"\"\"Create a unique and valid job name.\"\"\"\n\n        if self._job_name is None:\n            # get `repo` from `gcr.io/&lt;project_name&gt;/repo/other`\n            components = self.image.split(\"/\")\n            image_name = components[2]\n            # only alphanumeric and '-' allowed for a job name\n            modified_image_name = image_name.replace(\":\", \"-\").replace(\".\", \"-\")\n            # make 50 char limit for final job name, which will be '&lt;name&gt;-&lt;uuid&gt;'\n            if len(modified_image_name) &gt; 17:\n                modified_image_name = modified_image_name[:17]\n            name = f\"{modified_image_name}-{uuid4().hex}\"\n            self._job_name = name\n\n        return self._job_name\n\n    @property\n    def memory_string(self):\n        \"\"\"Returns the string expected for memory resources argument.\"\"\"\n        if self.memory and self.memory_unit:\n            return str(self.memory) + self.memory_unit\n        return None\n\n    @validator(\"image\")\n    def _remove_image_spaces(cls, value):\n        \"\"\"Deal with spaces in image names.\"\"\"\n        if value is not None:\n            return value.strip()\n\n    @root_validator\n    def _check_valid_memory(cls, values):\n        \"\"\"Make sure memory conforms to expected values for API.\n        See: https://cloud.google.com/run/docs/configuring/memory-limits#setting\n        \"\"\"  # noqa\n        if (values.get(\"memory\") is not None and values.get(\"memory_unit\") is None) or (\n            values.get(\"memory_unit\") is not None and values.get(\"memory\") is None\n        ):\n            raise ValueError(\n                \"A memory value and unit must both be supplied to specify a memory\"\n                \" value other than the default memory value.\"\n            )\n        return values\n\n    def _create_job_error(self, exc):\n        \"\"\"Provides a nicer error for 404s when trying to create a Cloud Run Job.\"\"\"\n        # TODO consider lookup table instead of the if/else,\n        # also check for documented errors\n        if exc.status_code == 404:\n            raise RuntimeError(\n                f\"Failed to find resources at {exc.uri}. Confirm that region\"\n                f\" '{self.region}' is the correct region for your Cloud Run Job and\"\n                f\" that {self.credentials.project} is the correct GCP project. If\"\n                f\" your project ID is not correct, you are using a Credentials block\"\n                f\" with permissions for the wrong project.\"\n            ) from exc\n        raise exc\n\n    def _job_run_submission_error(self, exc):\n        \"\"\"Provides a nicer error for 404s when submitting job runs.\"\"\"\n        if exc.status_code == 404:\n            pat1 = r\"The requested URL [^ ]+ was not found on this server\"\n            # pat2 = (\n            #     r\"Resource '[^ ]+' of kind 'JOB' in region '[\\w\\-0-9]+' \"\n            #     r\"in project '[\\w\\-0-9]+' does not exist\"\n            # )\n            if re.findall(pat1, str(exc)):\n                raise RuntimeError(\n                    f\"Failed to find resources at {exc.uri}. \"\n                    f\"Confirm that region '{self.region}' is \"\n                    f\"the correct region for your Cloud Run Job \"\n                    f\"and that '{self.credentials.project}' is the \"\n                    f\"correct GCP project. If your project ID is not \"\n                    f\"correct, you are using a Credentials \"\n                    f\"block with permissions for the wrong project.\"\n                ) from exc\n            else:\n                raise exc\n\n        raise exc\n\n    def _cpu_as_k8s_quantity(self) -&gt; str:\n        \"\"\"Return the CPU integer in the format expected by GCP Cloud Run Jobs API.\n        See: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n        See also: https://cloud.google.com/run/docs/configuring/cpu#setting-jobs\n        \"\"\"  # noqa\n        return str(self.cpu * 1000) + \"m\"\n\n    @sync_compatible\n    async def run(self, task_status: Optional[TaskStatus] = None):\n        \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\"\n        with self._get_client() as client:\n            await run_sync_in_worker_thread(\n                self._create_job_and_wait_for_registration, client\n            )\n            job_execution = await run_sync_in_worker_thread(\n                self._begin_job_execution, client\n            )\n\n            if task_status:\n                task_status.started(self.job_name)\n\n            result = await run_sync_in_worker_thread(\n                self._watch_job_execution_and_get_result,\n                client,\n                job_execution,\n                5,\n            )\n            return result\n\n    @sync_compatible\n    async def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n        \"\"\"\n        Kill a task running Cloud Run.\n\n        Args:\n            identifier: The Cloud Run Job name. This should match a\n                value yielded by CloudRunJob.run.\n        \"\"\"\n        if grace_seconds != 30:\n            self.logger.warning(\n                f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n                \"support dynamic grace period configuration. See here for more info: \"\n                \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\"  # noqa\n            )\n\n        with self._get_client() as client:\n            await run_sync_in_worker_thread(\n                self._kill_job,\n                client=client,\n                namespace=self.credentials.project,\n                job_name=identifier,\n            )\n\n    def _kill_job(self, client: Resource, namespace: str, job_name: str) -&gt; None:\n        \"\"\"\n        Thin wrapper around Job.delete, wrapping a try/except since\n        Job is an independent class that doesn't have knowledge of\n        CloudRunJob and its associated logic.\n        \"\"\"\n        try:\n            Job.delete(client=client, namespace=namespace, job_name=job_name)\n        except Exception as exc:\n            if \"does not exist\" in str(exc):\n                raise InfrastructureNotFound(\n                    f\"Cannot stop Cloud Run Job; the job name {job_name!r} \"\n                    \"could not be found.\"\n                ) from exc\n            raise\n\n    def _create_job_and_wait_for_registration(self, client: Resource) -&gt; None:\n        \"\"\"Create a new job wait for it to finish registering.\"\"\"\n        try:\n            self.logger.info(f\"Creating Cloud Run Job {self.job_name}\")\n            Job.create(\n                client=client,\n                namespace=self.credentials.project,\n                body=self._jobs_body(),\n            )\n        except googleapiclient.errors.HttpError as exc:\n            self._create_job_error(exc)\n\n        try:\n            self._wait_for_job_creation(client=client, timeout=self.timeout)\n        except Exception:\n            self.logger.exception(\n                \"Encountered an exception while waiting for job run creation\"\n            )\n            if not self.keep_job:\n                self.logger.info(\n                    f\"Deleting Cloud Run Job {self.job_name} from Google Cloud Run.\"\n                )\n                try:\n                    Job.delete(\n                        client=client,\n                        namespace=self.credentials.project,\n                        job_name=self.job_name,\n                    )\n                except Exception:\n                    self.logger.exception(\n                        \"Received an unexpected exception while attempting to delete\"\n                        f\" Cloud Run Job {self.job_name!r}\"\n                    )\n            raise\n\n    def _begin_job_execution(self, client: Resource) -&gt; Execution:\n        \"\"\"Submit a job run for execution and return the execution object.\"\"\"\n        try:\n            self.logger.info(\n                f\"Submitting Cloud Run Job {self.job_name!r} for execution.\"\n            )\n            submission = Job.run(\n                client=client,\n                namespace=self.credentials.project,\n                job_name=self.job_name,\n            )\n\n            job_execution = Execution.get(\n                client=client,\n                namespace=submission[\"metadata\"][\"namespace\"],\n                execution_name=submission[\"metadata\"][\"name\"],\n            )\n\n            command = (\n                \" \".join(self.command) if self.command else \"default container command\"\n            )\n\n            self.logger.info(\n                f\"Cloud Run Job {self.job_name!r}: Running command {command!r}\"\n            )\n        except Exception as exc:\n            self._job_run_submission_error(exc)\n\n        return job_execution\n\n    def _watch_job_execution_and_get_result(\n        self, client: Resource, execution: Execution, poll_interval: int\n    ) -&gt; CloudRunJobResult:\n        \"\"\"Wait for execution to complete and then return result.\"\"\"\n        try:\n            job_execution = self._watch_job_execution(\n                client=client,\n                job_execution=execution,\n                timeout=self.timeout,\n                poll_interval=poll_interval,\n            )\n        except Exception:\n            self.logger.exception(\n                \"Received an unexpected exception while monitoring Cloud Run Job \"\n                f\"{self.job_name!r}\"\n            )\n            raise\n\n        if job_execution.succeeded():\n            status_code = 0\n            self.logger.info(f\"Job Run {self.job_name} completed successfully\")\n        else:\n            status_code = 1\n            error_msg = job_execution.condition_after_completion()[\"message\"]\n            self.logger.error(\n                f\"Job Run {self.job_name} did not complete successfully. {error_msg}\"\n            )\n\n        self.logger.info(\n            f\"Job Run logs can be found on GCP at: {job_execution.log_uri}\"\n        )\n\n        if not self.keep_job:\n            self.logger.info(\n                f\"Deleting completed Cloud Run Job {self.job_name!r} from Google Cloud\"\n                \" Run...\"\n            )\n            try:\n                Job.delete(\n                    client=client,\n                    namespace=self.credentials.project,\n                    job_name=self.job_name,\n                )\n            except Exception:\n                self.logger.exception(\n                    \"Received an unexpected exception while attempting to delete Cloud\"\n                    f\" Run Job {self.job_name}\"\n                )\n\n        return CloudRunJobResult(identifier=self.job_name, status_code=status_code)\n\n    def _jobs_body(self) -&gt; dict:\n        \"\"\"Create properly formatted body used for a Job CREATE request.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs\n        \"\"\"\n        jobs_metadata = {\"name\": self.job_name}\n\n        annotations = {\n            # See: https://cloud.google.com/run/docs/troubleshooting#launch-stage-validation  # noqa\n            \"run.googleapis.com/launch-stage\": \"BETA\",\n        }\n        # add vpc connector if specified\n        if self.vpc_connector_name:\n            annotations[\n                \"run.googleapis.com/vpc-access-connector\"\n            ] = self.vpc_connector_name\n\n        # env and command here\n        containers = [self._add_container_settings({\"image\": self.image})]\n\n        # apply this timeout to each task\n        timeout_seconds = str(self.timeout)\n\n        body = {\n            \"apiVersion\": \"run.googleapis.com/v1\",\n            \"kind\": \"Job\",\n            \"metadata\": jobs_metadata,\n            \"spec\": {  # JobSpec\n                \"template\": {  # ExecutionTemplateSpec\n                    \"metadata\": {\"annotations\": annotations},\n                    \"spec\": {  # ExecutionSpec\n                        \"template\": {  # TaskTemplateSpec\n                            \"spec\": {\n                                \"containers\": containers,\n                                \"timeoutSeconds\": timeout_seconds,\n                            }  # TaskSpec\n                        }\n                    },\n                }\n            },\n        }\n        return body\n\n    def preview(self) -&gt; str:\n        \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n        body = self._jobs_body()\n        container_settings = body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n            \"containers\"\n        ][0][\"env\"]\n        body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"env\"] = [\n            container_setting\n            for container_setting in container_settings\n            if container_setting[\"name\"] != \"PREFECT_API_KEY\"\n        ]\n        return json.dumps(body, indent=2)\n\n    def _watch_job_execution(\n        self, client, job_execution: Execution, timeout: int, poll_interval: int = 5\n    ):\n        \"\"\"\n        Update job_execution status until it is no longer running or timeout is reached.\n        \"\"\"\n        t0 = time.time()\n        while job_execution.is_running():\n            job_execution = Execution.get(\n                client=client,\n                namespace=job_execution.namespace,\n                execution_name=job_execution.name,\n            )\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while waiting for Cloud Run Job \"\n                    \"execution to complete. Your job may still be running on GCP.\"\n                )\n\n            time.sleep(poll_interval)\n\n        return job_execution\n\n    def _wait_for_job_creation(\n        self, client: Resource, timeout: int, poll_interval: int = 5\n    ):\n        \"\"\"Give created job time to register.\"\"\"\n        job = Job.get(\n            client=client, namespace=self.credentials.project, job_name=self.job_name\n        )\n\n        t0 = time.time()\n        while not job.is_ready():\n            ready_condition = (\n                job.ready_condition\n                if job.ready_condition\n                else \"waiting for condition update\"\n            )\n            self.logger.info(\n                f\"Job is not yet ready... Current condition: {ready_condition}\"\n            )\n            job = Job.get(\n                client=client,\n                namespace=self.credentials.project,\n                job_name=self.job_name,\n            )\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while waiting for Cloud Run Job \"\n                    \"execution to complete. Your job may still be running on GCP.\"\n                )\n\n            time.sleep(poll_interval)\n\n    def _get_client(self) -&gt; Resource:\n        \"\"\"Get the base client needed for interacting with GCP APIs.\"\"\"\n        # region needed for 'v1' API\n        api_endpoint = f\"https://{self.region}-run.googleapis.com\"\n        gcp_creds = self.credentials.get_credentials_from_service_account()\n        options = ClientOptions(api_endpoint=api_endpoint)\n\n        return discovery.build(\n            \"run\", \"v1\", client_options=options, credentials=gcp_creds\n        ).namespaces()\n\n    # CONTAINER SETTINGS\n    def _add_container_settings(self, base_settings: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"\n        Add settings related to containers for Cloud Run Jobs to a dictionary.\n        Includes environment variables, entrypoint command, entrypoint arguments,\n        and cpu and memory limits.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container\n        and https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements\n        \"\"\"  # noqa\n        container_settings = base_settings.copy()\n        container_settings.update(self._add_env())\n        container_settings.update(self._add_resources())\n        container_settings.update(self._add_command())\n        container_settings.update(self._add_args())\n        return container_settings\n\n    def _add_args(self) -&gt; dict:\n        \"\"\"Set the arguments that will be passed to the entrypoint for a Cloud Run Job.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container\n        \"\"\"  # noqa\n        return {\"args\": self.args} if self.args else {}\n\n    def _add_command(self) -&gt; dict:\n        \"\"\"Set the command that a container will run for a Cloud Run Job.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container\n        \"\"\"  # noqa\n        return {\"command\": self.command}\n\n    def _add_resources(self) -&gt; dict:\n        \"\"\"Set specified resources limits for a Cloud Run Job.\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container#ResourceRequirements\n        See also: https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/\n        \"\"\"  # noqa\n        resources = {\"limits\": {}, \"requests\": {}}\n\n        if self.cpu is not None:\n            cpu = self._cpu_as_k8s_quantity()\n            resources[\"limits\"][\"cpu\"] = cpu\n            resources[\"requests\"][\"cpu\"] = cpu\n        if self.memory_string is not None:\n            resources[\"limits\"][\"memory\"] = self.memory_string\n            resources[\"requests\"][\"memory\"] = self.memory_string\n\n        return {\"resources\": resources} if resources[\"requests\"] else {}\n\n    def _add_env(self) -&gt; dict:\n        \"\"\"Add environment variables for a Cloud Run Job.\n\n        Method `self._base_environment()` gets necessary Prefect environment variables\n        from the config.\n\n        See: https://cloud.google.com/run/docs/reference/rest/v1/Container#envvar for\n        how environment variables are specified for Cloud Run Jobs.\n        \"\"\"  # noqa\n        env = {**self._base_environment(), **self.env}\n        cloud_run_env = [{\"name\": k, \"value\": v} for k, v in env.items()]\n        return {\"env\": cloud_run_env}\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob-attributes","title":"Attributes","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.job_name","title":"<code>job_name</code>  <code>property</code>","text":"<p>Create a unique and valid job name.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.memory_string","title":"<code>memory_string</code>  <code>property</code>","text":"<p>Returns the string expected for memory resources argument.</p>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob-functions","title":"Functions","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.kill","title":"<code>kill</code>  <code>async</code>","text":"<p>Kill a task running Cloud Run.</p> <p>Parameters:</p> Name Type Description Default <code>identifier</code> <code>str</code> <p>The Cloud Run Job name. This should match a value yielded by CloudRunJob.run.</p> required Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@sync_compatible\nasync def kill(self, identifier: str, grace_seconds: int = 30) -&gt; None:\n    \"\"\"\n    Kill a task running Cloud Run.\n\n    Args:\n        identifier: The Cloud Run Job name. This should match a\n            value yielded by CloudRunJob.run.\n    \"\"\"\n    if grace_seconds != 30:\n        self.logger.warning(\n            f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n            \"support dynamic grace period configuration. See here for more info: \"\n            \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\"  # noqa\n        )\n\n    with self._get_client() as client:\n        await run_sync_in_worker_thread(\n            self._kill_job,\n            client=client,\n            namespace=self.credentials.project,\n            job_name=identifier,\n        )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.preview","title":"<code>preview</code>","text":"<p>Generate a preview of the job definition that will be sent to GCP.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def preview(self) -&gt; str:\n    \"\"\"Generate a preview of the job definition that will be sent to GCP.\"\"\"\n    body = self._jobs_body()\n    container_settings = body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n        \"containers\"\n    ][0][\"env\"]\n    body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\"env\"] = [\n        container_setting\n        for container_setting in container_settings\n        if container_setting[\"name\"] != \"PREFECT_API_KEY\"\n    ]\n    return json.dumps(body, indent=2)\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJob.run","title":"<code>run</code>  <code>async</code>","text":"<p>Run the configured job on a Google Cloud Run Job.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@sync_compatible\nasync def run(self, task_status: Optional[TaskStatus] = None):\n    \"\"\"Run the configured job on a Google Cloud Run Job.\"\"\"\n    with self._get_client() as client:\n        await run_sync_in_worker_thread(\n            self._create_job_and_wait_for_registration, client\n        )\n        job_execution = await run_sync_in_worker_thread(\n            self._begin_job_execution, client\n        )\n\n        if task_status:\n            task_status.started(self.job_name)\n\n        result = await run_sync_in_worker_thread(\n            self._watch_job_execution_and_get_result,\n            client,\n            job_execution,\n            5,\n        )\n        return result\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.CloudRunJobResult","title":"<code>CloudRunJobResult</code>","text":"<p>             Bases: <code>InfrastructureResult</code></p> <p>Result from a Cloud Run Job.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class CloudRunJobResult(InfrastructureResult):\n    \"\"\"Result from a Cloud Run Job.\"\"\"\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution","title":"<code>Execution</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Utility class to call GCP <code>executions</code> API and interact with the returned objects.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class Execution(BaseModel):\n    \"\"\"\n    Utility class to call GCP `executions` API and\n    interact with the returned objects.\n    \"\"\"\n\n    name: str\n    namespace: str\n    metadata: dict\n    spec: dict\n    status: dict\n    log_uri: str\n\n    def is_running(self) -&gt; bool:\n        \"\"\"Returns True if Execution is not completed.\"\"\"\n        return self.status.get(\"completionTime\") is None\n\n    def condition_after_completion(self):\n        \"\"\"Returns Execution condition if Execution has completed.\"\"\"\n        for condition in self.status[\"conditions\"]:\n            if condition[\"type\"] == \"Completed\":\n                return condition\n\n    def succeeded(self):\n        \"\"\"Whether or not the Execution completed is a successful state.\"\"\"\n        completed_condition = self.condition_after_completion()\n        if completed_condition and completed_condition[\"status\"] == \"True\":\n            return True\n\n        return False\n\n    @classmethod\n    def get(cls, client: Resource, namespace: str, execution_name: str):\n        \"\"\"\n        Make a get request to the GCP executions API\n        and return an Execution instance.\n        \"\"\"\n        request = client.executions().get(\n            name=f\"namespaces/{namespace}/executions/{execution_name}\"\n        )\n        response = request.execute()\n\n        return cls(\n            name=response[\"metadata\"][\"name\"],\n            namespace=response[\"metadata\"][\"namespace\"],\n            metadata=response[\"metadata\"],\n            spec=response[\"spec\"],\n            status=response[\"status\"],\n            log_uri=response[\"status\"][\"logUri\"],\n        )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution-functions","title":"Functions","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.condition_after_completion","title":"<code>condition_after_completion</code>","text":"<p>Returns Execution condition if Execution has completed.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def condition_after_completion(self):\n    \"\"\"Returns Execution condition if Execution has completed.\"\"\"\n    for condition in self.status[\"conditions\"]:\n        if condition[\"type\"] == \"Completed\":\n            return condition\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.get","title":"<code>get</code>  <code>classmethod</code>","text":"<p>Make a get request to the GCP executions API and return an Execution instance.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@classmethod\ndef get(cls, client: Resource, namespace: str, execution_name: str):\n    \"\"\"\n    Make a get request to the GCP executions API\n    and return an Execution instance.\n    \"\"\"\n    request = client.executions().get(\n        name=f\"namespaces/{namespace}/executions/{execution_name}\"\n    )\n    response = request.execute()\n\n    return cls(\n        name=response[\"metadata\"][\"name\"],\n        namespace=response[\"metadata\"][\"namespace\"],\n        metadata=response[\"metadata\"],\n        spec=response[\"spec\"],\n        status=response[\"status\"],\n        log_uri=response[\"status\"][\"logUri\"],\n    )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.is_running","title":"<code>is_running</code>","text":"<p>Returns True if Execution is not completed.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def is_running(self) -&gt; bool:\n    \"\"\"Returns True if Execution is not completed.\"\"\"\n    return self.status.get(\"completionTime\") is None\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Execution.succeeded","title":"<code>succeeded</code>","text":"<p>Whether or not the Execution completed is a successful state.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def succeeded(self):\n    \"\"\"Whether or not the Execution completed is a successful state.\"\"\"\n    completed_condition = self.condition_after_completion()\n    if completed_condition and completed_condition[\"status\"] == \"True\":\n        return True\n\n    return False\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job","title":"<code>Job</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>Utility class to call GCP <code>jobs</code> API and interact with the returned objects.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>class Job(BaseModel):\n    \"\"\"\n    Utility class to call GCP `jobs` API and\n    interact with the returned objects.\n    \"\"\"\n\n    metadata: dict\n    spec: dict\n    status: dict\n    name: str\n    ready_condition: dict\n    execution_status: dict\n\n    def _is_missing_container(self):\n        \"\"\"\n        Check if Job status is not ready because\n        the specified container cannot be found.\n        \"\"\"\n        if (\n            self.ready_condition.get(\"status\") == \"False\"\n            and self.ready_condition.get(\"reason\") == \"ContainerMissing\"\n        ):\n            return True\n        return False\n\n    def is_ready(self) -&gt; bool:\n        \"\"\"Whether a job is finished registering and ready to be executed\"\"\"\n        if self._is_missing_container():\n            raise Exception(f\"{self.ready_condition['message']}\")\n        return self.ready_condition.get(\"status\") == \"True\"\n\n    def has_execution_in_progress(self) -&gt; bool:\n        \"\"\"See if job has a run in progress.\"\"\"\n        return (\n            self.execution_status == {}\n            or self.execution_status.get(\"completionTimestamp\") is None\n        )\n\n    @staticmethod\n    def _get_ready_condition(job: dict) -&gt; dict:\n        \"\"\"Utility to access JSON field containing ready condition.\"\"\"\n        if job[\"status\"].get(\"conditions\"):\n            for condition in job[\"status\"][\"conditions\"]:\n                if condition[\"type\"] == \"Ready\":\n                    return condition\n\n        return {}\n\n    @staticmethod\n    def _get_execution_status(job: dict):\n        \"\"\"Utility to access JSON field containing execution status.\"\"\"\n        if job[\"status\"].get(\"latestCreatedExecution\"):\n            return job[\"status\"][\"latestCreatedExecution\"]\n\n        return {}\n\n    @classmethod\n    def get(cls, client: Resource, namespace: str, job_name: str):\n        \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\"\n        request = client.jobs().get(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n        response = request.execute()\n\n        return cls(\n            metadata=response[\"metadata\"],\n            spec=response[\"spec\"],\n            status=response[\"status\"],\n            name=response[\"metadata\"][\"name\"],\n            ready_condition=cls._get_ready_condition(response),\n            execution_status=cls._get_execution_status(response),\n        )\n\n    @staticmethod\n    def create(client: Resource, namespace: str, body: dict):\n        \"\"\"Make a create request to the GCP jobs API.\"\"\"\n        request = client.jobs().create(parent=f\"namespaces/{namespace}\", body=body)\n        response = request.execute()\n        return response\n\n    @staticmethod\n    def delete(client: Resource, namespace: str, job_name: str):\n        \"\"\"Make a delete request to the GCP jobs API.\"\"\"\n        request = client.jobs().delete(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n        response = request.execute()\n        return response\n\n    @staticmethod\n    def run(client: Resource, namespace: str, job_name: str):\n        \"\"\"Make a run request to the GCP jobs API.\"\"\"\n        request = client.jobs().run(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n        response = request.execute()\n        return response\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job-functions","title":"Functions","text":""},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.create","title":"<code>create</code>  <code>staticmethod</code>","text":"<p>Make a create request to the GCP jobs API.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@staticmethod\ndef create(client: Resource, namespace: str, body: dict):\n    \"\"\"Make a create request to the GCP jobs API.\"\"\"\n    request = client.jobs().create(parent=f\"namespaces/{namespace}\", body=body)\n    response = request.execute()\n    return response\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.delete","title":"<code>delete</code>  <code>staticmethod</code>","text":"<p>Make a delete request to the GCP jobs API.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@staticmethod\ndef delete(client: Resource, namespace: str, job_name: str):\n    \"\"\"Make a delete request to the GCP jobs API.\"\"\"\n    request = client.jobs().delete(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n    response = request.execute()\n    return response\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.get","title":"<code>get</code>  <code>classmethod</code>","text":"<p>Make a get request to the GCP jobs API and return a Job instance.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@classmethod\ndef get(cls, client: Resource, namespace: str, job_name: str):\n    \"\"\"Make a get request to the GCP jobs API and return a Job instance.\"\"\"\n    request = client.jobs().get(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n    response = request.execute()\n\n    return cls(\n        metadata=response[\"metadata\"],\n        spec=response[\"spec\"],\n        status=response[\"status\"],\n        name=response[\"metadata\"][\"name\"],\n        ready_condition=cls._get_ready_condition(response),\n        execution_status=cls._get_execution_status(response),\n    )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.has_execution_in_progress","title":"<code>has_execution_in_progress</code>","text":"<p>See if job has a run in progress.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def has_execution_in_progress(self) -&gt; bool:\n    \"\"\"See if job has a run in progress.\"\"\"\n    return (\n        self.execution_status == {}\n        or self.execution_status.get(\"completionTimestamp\") is None\n    )\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.is_ready","title":"<code>is_ready</code>","text":"<p>Whether a job is finished registering and ready to be executed</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>def is_ready(self) -&gt; bool:\n    \"\"\"Whether a job is finished registering and ready to be executed\"\"\"\n    if self._is_missing_container():\n        raise Exception(f\"{self.ready_condition['message']}\")\n    return self.ready_condition.get(\"status\") == \"True\"\n</code></pre>"},{"location":"cloud_run/#prefect_gcp.cloud_run.Job.run","title":"<code>run</code>  <code>staticmethod</code>","text":"<p>Make a run request to the GCP jobs API.</p> Source code in <code>prefect_gcp/cloud_run.py</code> <pre><code>@staticmethod\ndef run(client: Resource, namespace: str, job_name: str):\n    \"\"\"Make a run request to the GCP jobs API.\"\"\"\n    request = client.jobs().run(name=f\"namespaces/{namespace}/jobs/{job_name}\")\n    response = request.execute()\n    return response\n</code></pre>"},{"location":"cloud_run_worker/","title":"Cloud Run","text":""},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run","title":"<code>prefect_gcp.workers.cloud_run</code>","text":"<p>Module containing the Cloud Run worker used for executing flow runs as Cloud Run jobs.</p> <p>Get started by creating a Cloud Run work pool:</p> <pre><code>prefect work-pool create 'my-cloud-run-pool' --type cloud-run\n</code></pre> <p>Then start a Cloud Run worker with the following command:</p> <pre><code>prefect worker start --pool 'my-cloud-run-pool'\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run--configuration","title":"Configuration","text":"<p>Read more about configuring work pools here.</p>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run--advanced-configuration","title":"Advanced Configuration","text":"<p>Using a custom Cloud Run job template</p> <p>Below is the default job body template used by the Cloud Run Worker: <pre><code>{\n    \"apiVersion\": \"run.googleapis.com/v1\",\n    \"kind\": \"Job\",\n    \"metadata\":\n        {\n            \"name\": \"{{ name }}\",\n            \"annotations\":\n            {\n                \"run.googleapis.com/launch-stage\": \"BETA\",\n                \"run.googleapis.com/vpc-access-connector\": \"{{ vpc_connector_name }}\"\n            }\n        },\n        \"spec\":\n        {\n            \"template\":\n            {\n                \"spec\":\n                {\n                    \"template\":\n                    {\n                        \"spec\":\n                        {\n                            \"containers\":\n                            [\n                                {\n                                    \"image\": \"{{ image }}\",\n                                    \"args\": \"{{ args }}\",\n                                    \"resources\":\n                                    {\n                                        \"limits\":\n                                        {\n                                            \"cpu\": \"{{ cpu }}\",\n                                            \"memory\": \"{{ memory }}\"\n                                        },\n                                        \"requests\":\n                                        {\n                                            \"cpu\": \"{{ cpu }}\",\n                                            \"memory\": \"{{ memory }}\"\n                                        }\n                                    }\n                                }\n                            ],\n                            \"timeoutSeconds\": \"{{ timeout }}\",\n                            \"serviceAccountName\": \"{{ service_account_name }}\"\n                        }\n                    }\n                }\n                }\n            }\n    },\n    \"timeout\": \"{{ timeout }}\",\n    \"keep_job\": \"{{ keep_job }}\"\n}\n</code></pre> Each values enclosed in <code>{{ }}</code> is a placeholder that will be replaced with a value at runtime on a per-deployment basis. The values that can be used a placeholders are defined by the <code>variables</code> schema defined in the base job template.</p> <p>The default job body template and available variables can be customized on a work pool by work pool basis. By editing the default job body template you can:</p> <ul> <li>Add additional placeholders to the default job template</li> <li>Remove placeholders from the default job template</li> <li>Pass values to Cloud Run that are not defined in the <code>variables</code> schema</li> </ul>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run--adding-additional-placeholders","title":"Adding additional placeholders","text":"<p>For example, to allow for extra customization of a new annotation not described in the default job template, you can add the following: <pre><code>{\n    \"apiVersion\": \"run.googleapis.com/v1\",\n    \"kind\": \"Job\",\n    \"metadata\":\n    {\n        \"name\": \"{{ name }}\",\n        \"annotations\":\n        {\n            \"run.googleapis.com/my-custom-annotation\": \"{{ my_custom_annotation }}\",\n            \"run.googleapis.com/launch-stage\": \"BETA\",\n            \"run.googleapis.com/vpc-access-connector\": \"{{ vpc_connector_name }}\"\n        },\n      ...\n    },\n  ...\n}\n</code></pre> <code>my_custom_annotation</code> can now be used as a placeholder in the job template and set on a per-deployment basis.</p> <pre><code># deployment.yaml\n...\ninfra_overrides: {\"my_custom_annotation\": \"my-custom-value\"}\n</code></pre> <p>Additionally, fields can be set to prevent configuration at the deployment level. For example to configure the <code>vpc_connector_name</code> field, the placeholder can be removed and replaced with an actual value. Now all deployments that point to this work pool will use the same <code>vpc_connector_name</code> value.</p> <pre><code>{\n    \"apiVersion\": \"run.googleapis.com/v1\",\n    \"kind\": \"Job\",\n    \"metadata\":\n    {\n        \"name\": \"{{ name }}\",\n        \"annotations\":\n        {\n            \"run.googleapis.com/launch-stage\": \"BETA\",\n            \"run.googleapis.com/vpc-access-connector\": \"my-vpc-connector\"\n        }\n      ...\n    },\n  ...\n}\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run-classes","title":"Classes","text":""},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorker","title":"<code>CloudRunWorker</code>","text":"<p>             Bases: <code>BaseWorker</code></p> <p>Prefect worker that executes flow runs within Cloud Run Jobs.</p> Source code in <code>prefect_gcp/workers/cloud_run.py</code> <pre><code>class CloudRunWorker(BaseWorker):\n    \"\"\"Prefect worker that executes flow runs within Cloud Run Jobs.\"\"\"\n\n    type = \"cloud-run\"\n    job_configuration = CloudRunWorkerJobConfiguration\n    job_configuration_variables = CloudRunWorkerVariables\n    _description = (\n        \"Execute flow runs within containers on Google Cloud Run. Requires \"\n        \"a Google Cloud Platform account.\"\n    )\n    _display_name = \"Google Cloud Run\"\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/cloud_run_worker/\"\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n\n    def _create_job_error(self, exc, configuration):\n        \"\"\"Provides a nicer error for 404s when trying to create a Cloud Run Job.\"\"\"\n        # TODO consider lookup table instead of the if/else,\n        # also check for documented errors\n        if exc.status_code == 404:\n            raise RuntimeError(\n                f\"Failed to find resources at {exc.uri}. Confirm that region\"\n                f\" '{self.region}' is the correct region for your Cloud Run Job and\"\n                f\" that {configuration.project} is the correct GCP project. If\"\n                f\" your project ID is not correct, you are using a Credentials block\"\n                f\" with permissions for the wrong project.\"\n            ) from exc\n        raise exc\n\n    def _job_run_submission_error(self, exc, configuration):\n        \"\"\"Provides a nicer error for 404s when submitting job runs.\"\"\"\n        if exc.status_code == 404:\n            pat1 = r\"The requested URL [^ ]+ was not found on this server\"\n            # pat2 = (\n            #     r\"Resource '[^ ]+' of kind 'JOB' in region '[\\w\\-0-9]+' \"\n            #     r\"in project '[\\w\\-0-9]+' does not exist\"\n            # )\n            if re.findall(pat1, str(exc)):\n                raise RuntimeError(\n                    f\"Failed to find resources at {exc.uri}. \"\n                    f\"Confirm that region '{self.region}' is \"\n                    f\"the correct region for your Cloud Run Job \"\n                    f\"and that '{configuration.project}' is the \"\n                    f\"correct GCP project. If your project ID is not \"\n                    f\"correct, you are using a Credentials \"\n                    f\"block with permissions for the wrong project.\"\n                ) from exc\n            else:\n                raise exc\n\n        raise exc\n\n    async def run(\n        self,\n        flow_run: \"FlowRun\",\n        configuration: CloudRunWorkerJobConfiguration,\n        task_status: Optional[anyio.abc.TaskStatus] = None,\n    ) -&gt; CloudRunWorkerResult:\n        \"\"\"\n        Executes a flow run within a Cloud Run Job and waits for the flow run\n        to complete.\n\n        Args:\n            flow_run: The flow run to execute\n            configuration: The configuration to use when executing the flow run.\n            task_status: The task status object for the current flow run. If provided,\n                the task will be marked as started.\n\n        Returns:\n            CloudRunWorkerResult: A result object containing information about the\n                final state of the flow run\n        \"\"\"\n\n        logger = self.get_flow_run_logger(flow_run)\n\n        with self._get_client(configuration) as client:\n            await run_sync_in_worker_thread(\n                self._create_job_and_wait_for_registration,\n                configuration,\n                client,\n                logger,\n            )\n            job_execution = await run_sync_in_worker_thread(\n                self._begin_job_execution, configuration, client, logger\n            )\n\n            if task_status:\n                task_status.started(configuration.job_name)\n\n            result = await run_sync_in_worker_thread(\n                self._watch_job_execution_and_get_result,\n                configuration,\n                client,\n                job_execution,\n                logger,\n            )\n            return result\n\n    def _get_client(self, configuration: CloudRunWorkerJobConfiguration) -&gt; Resource:\n        \"\"\"Get the base client needed for interacting with GCP APIs.\"\"\"\n        # region needed for 'v1' API\n        api_endpoint = f\"https://{configuration.region}-run.googleapis.com\"\n        gcp_creds = configuration.credentials.get_credentials_from_service_account()\n        options = ClientOptions(api_endpoint=api_endpoint)\n\n        return discovery.build(\n            \"run\", \"v1\", client_options=options, credentials=gcp_creds\n        ).namespaces()\n\n    def _create_job_and_wait_for_registration(\n        self,\n        configuration: CloudRunWorkerJobConfiguration,\n        client: Resource,\n        logger: PrefectLogAdapter,\n    ) -&gt; None:\n        \"\"\"Create a new job wait for it to finish registering.\"\"\"\n        try:\n            logger.info(f\"Creating Cloud Run Job {configuration.job_name}\")\n\n            Job.create(\n                client=client,\n                namespace=configuration.credentials.project,\n                body=configuration.job_body,\n            )\n        except googleapiclient.errors.HttpError as exc:\n            self._create_job_error(exc, configuration)\n\n        try:\n            self._wait_for_job_creation(\n                client=client, configuration=configuration, logger=logger\n            )\n        except Exception:\n            logger.exception(\n                \"Encountered an exception while waiting for job run creation\"\n            )\n            if not configuration.keep_job:\n                logger.info(\n                    f\"Deleting Cloud Run Job {configuration.job_name} from \"\n                    \"Google Cloud Run.\"\n                )\n                try:\n                    Job.delete(\n                        client=client,\n                        namespace=configuration.credentials.project,\n                        job_name=configuration.job_name,\n                    )\n                except Exception:\n                    logger.exception(\n                        \"Received an unexpected exception while attempting to delete\"\n                        f\" Cloud Run Job {configuration.job_name!r}\"\n                    )\n            raise\n\n    def _begin_job_execution(\n        self,\n        configuration: CloudRunWorkerJobConfiguration,\n        client: Resource,\n        logger: PrefectLogAdapter,\n    ) -&gt; Execution:\n        \"\"\"Submit a job run for execution and return the execution object.\"\"\"\n        try:\n            logger.info(\n                f\"Submitting Cloud Run Job {configuration.job_name!r} for execution.\"\n            )\n            submission = Job.run(\n                client=client,\n                namespace=configuration.project,\n                job_name=configuration.job_name,\n            )\n\n            job_execution = Execution.get(\n                client=client,\n                namespace=submission[\"metadata\"][\"namespace\"],\n                execution_name=submission[\"metadata\"][\"name\"],\n            )\n        except Exception as exc:\n            self._job_run_submission_error(exc, configuration)\n\n        return job_execution\n\n    def _watch_job_execution_and_get_result(\n        self,\n        configuration: CloudRunWorkerJobConfiguration,\n        client: Resource,\n        execution: Execution,\n        logger: PrefectLogAdapter,\n        poll_interval: int = 5,\n    ) -&gt; CloudRunWorkerResult:\n        \"\"\"Wait for execution to complete and then return result.\"\"\"\n        try:\n            job_execution = self._watch_job_execution(\n                client=client,\n                job_execution=execution,\n                timeout=configuration.timeout,\n                poll_interval=poll_interval,\n            )\n        except Exception:\n            logger.exception(\n                \"Received an unexpected exception while monitoring Cloud Run Job \"\n                f\"{configuration.job_name!r}\"\n            )\n            raise\n\n        if job_execution.succeeded():\n            status_code = 0\n            logger.info(f\"Job Run {configuration.job_name} completed successfully\")\n        else:\n            status_code = 1\n            error_msg = job_execution.condition_after_completion()[\"message\"]\n            logger.error(\n                \"Job Run {configuration.job_name} did not complete successfully. \"\n                f\"{error_msg}\"\n            )\n\n        logger.info(f\"Job Run logs can be found on GCP at: {job_execution.log_uri}\")\n\n        if not configuration.keep_job:\n            logger.info(\n                f\"Deleting completed Cloud Run Job {configuration.job_name!r} \"\n                \"from Google Cloud Run...\"\n            )\n            try:\n                Job.delete(\n                    client=client,\n                    namespace=configuration.project,\n                    job_name=configuration.job_name,\n                )\n            except Exception:\n                logger.exception(\n                    \"Received an unexpected exception while attempting to delete Cloud\"\n                    f\" Run Job {configuration.job_name}\"\n                )\n\n        return CloudRunWorkerResult(\n            identifier=configuration.job_name, status_code=status_code\n        )\n\n    def _watch_job_execution(\n        self, client, job_execution: Execution, timeout: int, poll_interval: int = 5\n    ):\n        \"\"\"\n        Update job_execution status until it is no longer running or timeout is reached.\n        \"\"\"\n        t0 = time.time()\n        while job_execution.is_running():\n            job_execution = Execution.get(\n                client=client,\n                namespace=job_execution.namespace,\n                execution_name=job_execution.name,\n            )\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while waiting for Cloud Run Job \"\n                    \"execution to complete. Your job may still be running on GCP.\"\n                )\n\n            time.sleep(poll_interval)\n\n        return job_execution\n\n    def _wait_for_job_creation(\n        self,\n        client: Resource,\n        configuration: CloudRunWorkerJobConfiguration,\n        logger: PrefectLogAdapter,\n        poll_interval: int = 5,\n    ):\n        \"\"\"Give created job time to register.\"\"\"\n        job = Job.get(\n            client=client,\n            namespace=configuration.project,\n            job_name=configuration.job_name,\n        )\n\n        t0 = time.time()\n        while not job.is_ready():\n            ready_condition = (\n                job.ready_condition\n                if job.ready_condition\n                else \"waiting for condition update\"\n            )\n            logger.info(f\"Job is not yet ready... Current condition: {ready_condition}\")\n            job = Job.get(\n                client=client,\n                namespace=configuration.project,\n                job_name=configuration.job_name,\n            )\n\n            elapsed_time = time.time() - t0\n            if (\n                configuration.timeout is not None\n                and elapsed_time &gt; configuration.timeout\n            ):\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while waiting for Cloud Run Job \"\n                    \"execution to complete. Your job may still be running on GCP.\"\n                )\n\n            time.sleep(poll_interval)\n\n    async def kill_infrastructure(\n        self,\n        infrastructure_pid: str,\n        configuration: CloudRunWorkerJobConfiguration,\n        grace_seconds: int = 30,\n    ):\n        \"\"\"\n        Stops a job for a cancelled flow run based on the provided infrastructure PID\n        and run configuration.\n        \"\"\"\n        if grace_seconds != 30:\n            self._logger.warning(\n                f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n                \"support dynamic grace period configuration. See here for more info: \"\n                \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\"  # noqa\n            )\n\n        with self._get_client(configuration) as client:\n            await run_sync_in_worker_thread(\n                self._stop_job,\n                client=client,\n                namespace=configuration.project,\n                job_name=infrastructure_pid,\n            )\n\n    def _stop_job(self, client: Resource, namespace: str, job_name: str):\n        try:\n            Job.delete(client=client, namespace=namespace, job_name=job_name)\n        except Exception as exc:\n            if \"does not exist\" in str(exc):\n                raise InfrastructureNotFound(\n                    f\"Cannot stop Cloud Run Job; the job name {job_name!r} \"\n                    \"could not be found.\"\n                ) from exc\n            raise\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorker-functions","title":"Functions","text":""},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorker.kill_infrastructure","title":"<code>kill_infrastructure</code>  <code>async</code>","text":"<p>Stops a job for a cancelled flow run based on the provided infrastructure PID and run configuration.</p> Source code in <code>prefect_gcp/workers/cloud_run.py</code> <pre><code>async def kill_infrastructure(\n    self,\n    infrastructure_pid: str,\n    configuration: CloudRunWorkerJobConfiguration,\n    grace_seconds: int = 30,\n):\n    \"\"\"\n    Stops a job for a cancelled flow run based on the provided infrastructure PID\n    and run configuration.\n    \"\"\"\n    if grace_seconds != 30:\n        self._logger.warning(\n            f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n            \"support dynamic grace period configuration. See here for more info: \"\n            \"https://cloud.google.com/run/docs/reference/rest/v1/namespaces.jobs/delete\"  # noqa\n        )\n\n    with self._get_client(configuration) as client:\n        await run_sync_in_worker_thread(\n            self._stop_job,\n            client=client,\n            namespace=configuration.project,\n            job_name=infrastructure_pid,\n        )\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorker.run","title":"<code>run</code>  <code>async</code>","text":"<p>Executes a flow run within a Cloud Run Job and waits for the flow run to complete.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run</code> <code>FlowRun</code> <p>The flow run to execute</p> required <code>configuration</code> <code>CloudRunWorkerJobConfiguration</code> <p>The configuration to use when executing the flow run.</p> required <code>task_status</code> <code>Optional[TaskStatus]</code> <p>The task status object for the current flow run. If provided, the task will be marked as started.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>CloudRunWorkerResult</code> <code>CloudRunWorkerResult</code> <p>A result object containing information about the final state of the flow run</p> Source code in <code>prefect_gcp/workers/cloud_run.py</code> <pre><code>async def run(\n    self,\n    flow_run: \"FlowRun\",\n    configuration: CloudRunWorkerJobConfiguration,\n    task_status: Optional[anyio.abc.TaskStatus] = None,\n) -&gt; CloudRunWorkerResult:\n    \"\"\"\n    Executes a flow run within a Cloud Run Job and waits for the flow run\n    to complete.\n\n    Args:\n        flow_run: The flow run to execute\n        configuration: The configuration to use when executing the flow run.\n        task_status: The task status object for the current flow run. If provided,\n            the task will be marked as started.\n\n    Returns:\n        CloudRunWorkerResult: A result object containing information about the\n            final state of the flow run\n    \"\"\"\n\n    logger = self.get_flow_run_logger(flow_run)\n\n    with self._get_client(configuration) as client:\n        await run_sync_in_worker_thread(\n            self._create_job_and_wait_for_registration,\n            configuration,\n            client,\n            logger,\n        )\n        job_execution = await run_sync_in_worker_thread(\n            self._begin_job_execution, configuration, client, logger\n        )\n\n        if task_status:\n            task_status.started(configuration.job_name)\n\n        result = await run_sync_in_worker_thread(\n            self._watch_job_execution_and_get_result,\n            configuration,\n            client,\n            job_execution,\n            logger,\n        )\n        return result\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerJobConfiguration","title":"<code>CloudRunWorkerJobConfiguration</code>","text":"<p>             Bases: <code>BaseJobConfiguration</code></p> <p>Configuration class used by the Cloud Run Worker to create a Cloud Run Job.</p> <p>An instance of this class is passed to the Cloud Run worker's <code>run</code> method for each flow run. It contains all information necessary to execute the flow run as a Cloud Run Job.</p> <p>Attributes:</p> Name Type Description <code>region</code> <code>str</code> <p>The region where the Cloud Run Job resides.</p> <code>credentials</code> <code>Optional[GcpCredentials]</code> <p>The GCP Credentials used to connect to Cloud Run.</p> <code>job_body</code> <code>Dict[str, Any]</code> <p>The job body used to create the Cloud Run Job.</p> <code>timeout</code> <code>Optional[int]</code> <p>The length of time that Prefect will wait for a Cloud Run Job.</p> <code>keep_job</code> <code>Optional[bool]</code> <p>Whether to delete the Cloud Run Job after it completes.</p> Source code in <code>prefect_gcp/workers/cloud_run.py</code> <pre><code>class CloudRunWorkerJobConfiguration(BaseJobConfiguration):\n    \"\"\"\n    Configuration class used by the Cloud Run Worker to create a Cloud Run Job.\n\n    An instance of this class is passed to the Cloud Run worker's `run` method\n    for each flow run. It contains all information necessary to execute\n    the flow run as a Cloud Run Job.\n\n    Attributes:\n        region: The region where the Cloud Run Job resides.\n        credentials: The GCP Credentials used to connect to Cloud Run.\n        job_body: The job body used to create the Cloud Run Job.\n        timeout: The length of time that Prefect will wait for a Cloud Run Job.\n        keep_job: Whether to delete the Cloud Run Job after it completes.\n    \"\"\"\n\n    region: str = Field(\n        default=\"us-central1\", description=\"The region where the Cloud Run Job resides.\"\n    )\n    credentials: Optional[GcpCredentials] = Field(\n        title=\"GCP Credentials\",\n        default_factory=GcpCredentials,\n        description=\"The GCP Credentials used to connect to Cloud Run. \"\n        \"If not provided credentials will be inferred from \"\n        \"the local environment.\",\n    )\n    job_body: Dict[str, Any] = Field(template=_get_default_job_body_template())\n    timeout: Optional[int] = Field(\n        default=600,\n        gt=0,\n        le=3600,\n        title=\"Job Timeout\",\n        description=(\n            \"The length of time that Prefect will wait for a Cloud Run Job to complete \"\n            \"before raising an exception.\"\n        ),\n    )\n    keep_job: Optional[bool] = Field(\n        default=False,\n        title=\"Keep Job After Completion\",\n        description=\"Keep the completed Cloud Run Job on Google Cloud Platform.\",\n    )\n\n    @property\n    def project(self) -&gt; str:\n        \"\"\"property for accessing the project from the credentials.\"\"\"\n        return self.credentials.project\n\n    @property\n    def job_name(self) -&gt; str:\n        \"\"\"property for accessing the name from the job metadata.\"\"\"\n        return self.job_body[\"metadata\"][\"name\"]\n\n    def prepare_for_flow_run(\n        self,\n        flow_run: \"FlowRun\",\n        deployment: Optional[\"DeploymentResponse\"] = None,\n        flow: Optional[\"Flow\"] = None,\n    ):\n        \"\"\"\n        Prepares the job configuration for a flow run.\n\n        Ensures that necessary values are present in the job body and that the\n        job body is valid.\n\n        Args:\n            flow_run: The flow run to prepare the job configuration for\n            deployment: The deployment associated with the flow run used for\n                preparation.\n            flow: The flow associated with the flow run used for preparation.\n        \"\"\"\n        super().prepare_for_flow_run(flow_run, deployment, flow)\n\n        self._populate_envs()\n        self._populate_or_format_command()\n        self._format_args_if_present()\n        self._populate_image_if_not_present()\n        self._populate_name_if_not_present()\n\n    def _populate_envs(self):\n        \"\"\"Populate environment variables. BaseWorker.prepare_for_flow_run handles\n        putting the environment variables in the `env` attribute. This method\n        moves them into the jobs body\"\"\"\n        envs = [{\"name\": k, \"value\": v} for k, v in self.env.items()]\n        self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\"containers\"][0][\n            \"env\"\n        ] = envs\n\n    def _populate_name_if_not_present(self):\n        \"\"\"Adds the flow run name to the job if one is not already provided.\"\"\"\n        try:\n            if \"name\" not in self.job_body[\"metadata\"]:\n                self.job_body[\"metadata\"][\"name\"] = self.name\n        except KeyError:\n            raise ValueError(\"Unable to verify name due to invalid job body template.\")\n\n    def _populate_image_if_not_present(self):\n        \"\"\"Adds the latest prefect image to the job if one is not already provided.\"\"\"\n        try:\n            if (\n                \"image\"\n                not in self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n                    \"containers\"\n                ][0]\n            ):\n                self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n                    \"containers\"\n                ][0][\"image\"] = f\"docker.io/{get_prefect_image_name()}\"\n        except KeyError:\n            raise ValueError(\"Unable to verify image due to invalid job body template.\")\n\n    def _populate_or_format_command(self):\n        \"\"\"\n        Ensures that the command is present in the job manifest. Populates the command\n        with the `prefect -m prefect.engine` if a command is not present.\n        \"\"\"\n        try:\n            command = self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n                \"containers\"\n            ][0].get(\"command\")\n            if command is None:\n                self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n                    \"containers\"\n                ][0][\"command\"] = [\n                    \"python\",\n                    \"-m\",\n                    \"prefect.engine\",\n                ]\n            elif isinstance(command, str):\n                self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n                    \"containers\"\n                ][0][\"command\"] = shlex.split(command)\n        except KeyError:\n            raise ValueError(\n                \"Unable to verify command due to invalid job body template.\"\n            )\n\n    def _format_args_if_present(self):\n        try:\n            args = self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n                \"containers\"\n            ][0].get(\"args\")\n            if args is not None and isinstance(args, str):\n                self.job_body[\"spec\"][\"template\"][\"spec\"][\"template\"][\"spec\"][\n                    \"containers\"\n                ][0][\"args\"] = shlex.split(args)\n        except KeyError:\n            raise ValueError(\"Unable to verify args due to invalid job body template.\")\n\n    @validator(\"job_body\")\n    def _ensure_job_includes_all_required_components(cls, value: Dict[str, Any]):\n        \"\"\"\n        Ensures that the job body includes all required components.\n        \"\"\"\n        patch = JsonPatch.from_diff(value, _get_base_job_body())\n        missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])\n        if missing_paths:\n            raise ValueError(\n                \"Job is missing required attributes at the following paths: \"\n                f\"{', '.join(missing_paths)}\"\n            )\n        return value\n\n    @validator(\"job_body\")\n    def _ensure_job_has_compatible_values(cls, value: Dict[str, Any]):\n        \"\"\"Ensure that the job body has compatible values.\"\"\"\n        patch = JsonPatch.from_diff(value, _get_base_job_body())\n        incompatible = sorted(\n            [\n                f\"{op['path']} must have value {op['value']!r}\"\n                for op in patch\n                if op[\"op\"] == \"replace\"\n            ]\n        )\n        if incompatible:\n            raise ValueError(\n                \"Job has incompatible values for the following attributes: \"\n                f\"{', '.join(incompatible)}\"\n            )\n        return value\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerJobConfiguration-attributes","title":"Attributes","text":""},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerJobConfiguration.job_name","title":"<code>job_name: str</code>  <code>property</code>","text":"<p>property for accessing the name from the job metadata.</p>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerJobConfiguration.project","title":"<code>project: str</code>  <code>property</code>","text":"<p>property for accessing the project from the credentials.</p>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerJobConfiguration-functions","title":"Functions","text":""},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerJobConfiguration.prepare_for_flow_run","title":"<code>prepare_for_flow_run</code>","text":"<p>Prepares the job configuration for a flow run.</p> <p>Ensures that necessary values are present in the job body and that the job body is valid.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run</code> <code>FlowRun</code> <p>The flow run to prepare the job configuration for</p> required <code>deployment</code> <code>Optional[DeploymentResponse]</code> <p>The deployment associated with the flow run used for preparation.</p> <code>None</code> <code>flow</code> <code>Optional[Flow]</code> <p>The flow associated with the flow run used for preparation.</p> <code>None</code> Source code in <code>prefect_gcp/workers/cloud_run.py</code> <pre><code>def prepare_for_flow_run(\n    self,\n    flow_run: \"FlowRun\",\n    deployment: Optional[\"DeploymentResponse\"] = None,\n    flow: Optional[\"Flow\"] = None,\n):\n    \"\"\"\n    Prepares the job configuration for a flow run.\n\n    Ensures that necessary values are present in the job body and that the\n    job body is valid.\n\n    Args:\n        flow_run: The flow run to prepare the job configuration for\n        deployment: The deployment associated with the flow run used for\n            preparation.\n        flow: The flow associated with the flow run used for preparation.\n    \"\"\"\n    super().prepare_for_flow_run(flow_run, deployment, flow)\n\n    self._populate_envs()\n    self._populate_or_format_command()\n    self._format_args_if_present()\n    self._populate_image_if_not_present()\n    self._populate_name_if_not_present()\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerResult","title":"<code>CloudRunWorkerResult</code>","text":"<p>             Bases: <code>BaseWorkerResult</code></p> <p>Contains information about the final state of a completed process</p> Source code in <code>prefect_gcp/workers/cloud_run.py</code> <pre><code>class CloudRunWorkerResult(BaseWorkerResult):\n    \"\"\"Contains information about the final state of a completed process\"\"\"\n</code></pre>"},{"location":"cloud_run_worker/#prefect_gcp.workers.cloud_run.CloudRunWorkerVariables","title":"<code>CloudRunWorkerVariables</code>","text":"<p>             Bases: <code>BaseVariables</code></p> <p>Default variables for the Cloud Run worker.</p> <p>The schema for this class is used to populate the <code>variables</code> section of the default base job template.</p> Source code in <code>prefect_gcp/workers/cloud_run.py</code> <pre><code>class CloudRunWorkerVariables(BaseVariables):\n    \"\"\"\n    Default variables for the Cloud Run worker.\n\n    The schema for this class is used to populate the `variables` section of the default\n    base job template.\n    \"\"\"\n\n    region: str = Field(\n        default=\"us-central1\",\n        description=\"The region where the Cloud Run Job resides.\",\n        example=\"us-central1\",\n    )\n    credentials: Optional[GcpCredentials] = Field(\n        title=\"GCP Credentials\",\n        default_factory=GcpCredentials,\n        description=\"The GCP Credentials used to initiate the \"\n        \"Cloud Run Job. If not provided credentials will be \"\n        \"inferred from the local environment.\",\n    )\n    image: Optional[str] = Field(\n        default=None,\n        title=\"Image Name\",\n        description=(\n            \"The image to use for a new Cloud Run Job. \"\n            \"If not set, the latest Prefect image will be used. \"\n            \"See https://cloud.google.com/run/docs/deploying#images.\"\n        ),\n        example=\"docker.io/prefecthq/prefect:2-latest\",\n    )\n    cpu: Optional[str] = Field(\n        default=None,\n        title=\"CPU\",\n        description=(\n            \"The amount of compute allocated to the Cloud Run Job. \"\n            \"(1000m = 1 CPU). See \"\n            \"https://cloud.google.com/run/docs/configuring/cpu#setting-jobs.\"\n        ),\n        example=\"1000m\",\n        regex=r\"^(\\d*000)m$\",\n    )\n    memory: Optional[str] = Field(\n        default=None,\n        title=\"Memory\",\n        description=(\n            \"The amount of memory allocated to the Cloud Run Job. \"\n            \"Must be specified in units of 'G', 'Gi', 'M', or 'Mi'. \"\n            \"See https://cloud.google.com/run/docs/configuring/memory-limits#setting.\"\n        ),\n        example=\"512Mi\",\n        regex=r\"^\\d+(?:G|Gi|M|Mi)$\",\n    )\n    vpc_connector_name: Optional[str] = Field(\n        default=None,\n        title=\"VPC Connector Name\",\n        description=\"The name of the VPC connector to use for the Cloud Run Job.\",\n    )\n    service_account_name: Optional[str] = Field(\n        default=None,\n        title=\"Service Account Name\",\n        description=\"The name of the service account to use for the task execution \"\n        \"of Cloud Run Job. By default Cloud Run jobs run as the default \"\n        \"Compute Engine Service Account. \",\n        example=\"service-account@example.iam.gserviceaccount.com\",\n    )\n    keep_job: Optional[bool] = Field(\n        default=False,\n        title=\"Keep Job After Completion\",\n        description=\"Keep the completed Cloud Run Job after it has run.\",\n    )\n    timeout: Optional[int] = Field(\n        default=600,\n        gt=0,\n        le=3600,\n        title=\"Job Timeout\",\n        description=(\n            \"The length of time that Prefect will wait for Cloud Run Job state changes.\"\n        ),\n    )\n</code></pre>"},{"location":"cloud_storage/","title":"Cloud Storage","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage","title":"<code>prefect_gcp.cloud_storage</code>","text":"<p>Tasks for interacting with GCP Cloud Storage.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage-classes","title":"Classes","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat","title":"<code>DataFrameSerializationFormat</code>","text":"<p>             Bases: <code>Enum</code></p> <p>An enumeration class to represent different file formats, compression options for upload_from_dataframe</p> <p>Attributes:</p> Name Type Description <code>CSV</code> <p>Representation for 'csv' file format with no compression and its related content type and suffix.</p> <code>CSV_GZIP</code> <p>Representation for 'csv' file format with 'gzip' compression and its related content type and suffix.</p> <code>PARQUET</code> <p>Representation for 'parquet' file format with no compression and its related content type and suffix.</p> <code>PARQUET_SNAPPY</code> <p>Representation for 'parquet' file format with 'snappy' compression and its related content type and suffix.</p> <code>PARQUET_GZIP</code> <p>Representation for 'parquet' file format with 'gzip' compression and its related content type and suffix.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>class DataFrameSerializationFormat(Enum):\n    \"\"\"\n    An enumeration class to represent different file formats,\n    compression options for upload_from_dataframe\n\n    Attributes:\n        CSV: Representation for 'csv' file format with no compression\n            and its related content type and suffix.\n\n        CSV_GZIP: Representation for 'csv' file format with 'gzip' compression\n            and its related content type and suffix.\n\n        PARQUET: Representation for 'parquet' file format with no compression\n            and its related content type and suffix.\n\n        PARQUET_SNAPPY: Representation for 'parquet' file format\n            with 'snappy' compression and its related content type and suffix.\n\n        PARQUET_GZIP: Representation for 'parquet' file format\n            with 'gzip' compression and its related content type and suffix.\n    \"\"\"\n\n    CSV = (\"csv\", None, \"text/csv\", \".csv\")\n    CSV_GZIP = (\"csv\", \"gzip\", \"application/x-gzip\", \".csv.gz\")\n    PARQUET = (\"parquet\", None, \"application/octet-stream\", \".parquet\")\n    PARQUET_SNAPPY = (\n        \"parquet\",\n        \"snappy\",\n        \"application/octet-stream\",\n        \".snappy.parquet\",\n    )\n    PARQUET_GZIP = (\"parquet\", \"gzip\", \"application/octet-stream\", \".gz.parquet\")\n\n    @property\n    def format(self) -&gt; str:\n        \"\"\"The file format of the current instance.\"\"\"\n        return self.value[0]\n\n    @property\n    def compression(self) -&gt; Union[str, None]:\n        \"\"\"The compression type of the current instance.\"\"\"\n        return self.value[1]\n\n    @property\n    def content_type(self) -&gt; str:\n        \"\"\"The content type of the current instance.\"\"\"\n        return self.value[2]\n\n    @property\n    def suffix(self) -&gt; str:\n        \"\"\"The suffix of the file format of the current instance.\"\"\"\n        return self.value[3]\n\n    def fix_extension_with(self, gcs_blob_path: str) -&gt; str:\n        \"\"\"Fix the extension of a GCS blob.\n\n        Args:\n            gcs_blob_path: The path to the GCS blob to be modified.\n\n        Returns:\n            The modified path to the GCS blob with the new extension.\n        \"\"\"\n        gcs_blob_path = PurePosixPath(gcs_blob_path)\n        folder = gcs_blob_path.parent\n        filename = PurePosixPath(gcs_blob_path.stem).with_suffix(self.suffix)\n        return str(folder.joinpath(filename))\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat-attributes","title":"Attributes","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat.compression","title":"<code>compression: Union[str, None]</code>  <code>property</code>","text":"<p>The compression type of the current instance.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat.content_type","title":"<code>content_type: str</code>  <code>property</code>","text":"<p>The content type of the current instance.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat.format","title":"<code>format: str</code>  <code>property</code>","text":"<p>The file format of the current instance.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat.suffix","title":"<code>suffix: str</code>  <code>property</code>","text":"<p>The suffix of the file format of the current instance.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat-functions","title":"Functions","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.DataFrameSerializationFormat.fix_extension_with","title":"<code>fix_extension_with</code>","text":"<p>Fix the extension of a GCS blob.</p> <p>Parameters:</p> Name Type Description Default <code>gcs_blob_path</code> <code>str</code> <p>The path to the GCS blob to be modified.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The modified path to the GCS blob with the new extension.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>def fix_extension_with(self, gcs_blob_path: str) -&gt; str:\n    \"\"\"Fix the extension of a GCS blob.\n\n    Args:\n        gcs_blob_path: The path to the GCS blob to be modified.\n\n    Returns:\n        The modified path to the GCS blob with the new extension.\n    \"\"\"\n    gcs_blob_path = PurePosixPath(gcs_blob_path)\n    folder = gcs_blob_path.parent\n    filename = PurePosixPath(gcs_blob_path.stem).with_suffix(self.suffix)\n    return str(folder.joinpath(filename))\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket","title":"<code>GcsBucket</code>","text":"<p>             Bases: <code>WritableDeploymentStorage</code>, <code>WritableFileSystem</code>, <code>ObjectStorageBlock</code></p> <p>Block used to store data using GCP Cloud Storage Buckets.</p> <p>Note! <code>GcsBucket</code> in <code>prefect-gcp</code> is a unique block, separate from <code>GCS</code> in core Prefect. <code>GcsBucket</code> does not use <code>gcsfs</code> under the hood, instead using the <code>google-cloud-storage</code> package, and offers more configuration and functionality.</p> <p>Attributes:</p> Name Type Description <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> <code>gcp_credentials</code> <code>GcpCredentials</code> <p>The credentials to authenticate with GCP.</p> <code>bucket_folder</code> <code>str</code> <p>A default path to a folder within the GCS bucket to use for reading and writing objects.</p> Example <p>Load stored GCP Cloud Storage Bucket: <pre><code>from prefect_gcp.cloud_storage import GcsBucket\ngcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>class GcsBucket(WritableDeploymentStorage, WritableFileSystem, ObjectStorageBlock):\n    \"\"\"\n    Block used to store data using GCP Cloud Storage Buckets.\n\n    Note! `GcsBucket` in `prefect-gcp` is a unique block, separate from `GCS`\n    in core Prefect. `GcsBucket` does not use `gcsfs` under the hood,\n    instead using the `google-cloud-storage` package, and offers more configuration\n    and functionality.\n\n    Attributes:\n        bucket: Name of the bucket.\n        gcp_credentials: The credentials to authenticate with GCP.\n        bucket_folder: A default path to a folder within the GCS bucket to use\n            for reading and writing objects.\n\n    Example:\n        Load stored GCP Cloud Storage Bucket:\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n        gcp_cloud_storage_bucket_block = GcsBucket.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"\n\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n    _block_type_name = \"GCS Bucket\"\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/cloud_storage/#prefect_gcp.cloud_storage.GcsBucket\"  # noqa: E501\n\n    bucket: str = Field(..., description=\"Name of the bucket.\")\n    gcp_credentials: GcpCredentials = Field(\n        default_factory=GcpCredentials,\n        description=\"The credentials to authenticate with GCP.\",\n    )\n    bucket_folder: str = Field(\n        default=\"\",\n        description=(\n            \"A default path to a folder within the GCS bucket to use \"\n            \"for reading and writing objects.\"\n        ),\n    )\n\n    @property\n    def basepath(self) -&gt; str:\n        \"\"\"\n        Read-only property that mirrors the bucket folder.\n\n        Used for deployment.\n        \"\"\"\n        return self.bucket_folder\n\n    @validator(\"bucket_folder\", pre=True, always=True)\n    def _bucket_folder_suffix(cls, value):\n        \"\"\"\n        Ensures that the bucket folder is suffixed with a forward slash.\n        \"\"\"\n        if value != \"\" and not value.endswith(\"/\"):\n            value = f\"{value}/\"\n        return value\n\n    def _resolve_path(self, path: str) -&gt; str:\n        \"\"\"\n        A helper function used in write_path to join `self.bucket_folder` and `path`.\n\n        Args:\n            path: Name of the key, e.g. \"file1\". Each object in your\n                bucket has a unique key (or key name).\n\n        Returns:\n            The joined path.\n        \"\"\"\n        # If bucket_folder provided, it means we won't write to the root dir of\n        # the bucket. So we need to add it on the front of the path.\n        path = (\n            str(PurePosixPath(self.bucket_folder, path)) if self.bucket_folder else path\n        )\n        if path in [\"\", \".\", \"/\"]:\n            # client.bucket.list_blobs(prefix=None) is the proper way\n            # of specifying the root folder of the bucket\n            path = None\n        return path\n\n    @sync_compatible\n    async def get_directory(\n        self, from_path: Optional[str] = None, local_path: Optional[str] = None\n    ) -&gt; List[Union[str, Path]]:\n        \"\"\"\n        Copies a folder from the configured GCS bucket to a local directory.\n        Defaults to copying the entire contents of the block's bucket_folder\n        to the current working directory.\n\n        Args:\n            from_path: Path in GCS bucket to download from. Defaults to the block's\n                configured bucket_folder.\n            local_path: Local path to download GCS bucket contents to.\n                Defaults to the current working directory.\n\n        Returns:\n            A list of downloaded file paths.\n        \"\"\"\n        from_path = (\n            self.bucket_folder if from_path is None else self._resolve_path(from_path)\n        )\n\n        if local_path is None:\n            local_path = os.path.abspath(\".\")\n        else:\n            local_path = os.path.abspath(os.path.expanduser(local_path))\n\n        project = self.gcp_credentials.project\n        client = self.gcp_credentials.get_cloud_storage_client(project=project)\n\n        blobs = await run_sync_in_worker_thread(\n            client.list_blobs, self.bucket, prefix=from_path\n        )\n\n        file_paths = []\n        for blob in blobs:\n            blob_path = blob.name\n            if blob_path[-1] == \"/\":\n                # object is a folder and will be created if it contains any objects\n                continue\n            local_file_path = os.path.join(local_path, blob_path)\n            os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n            with disable_run_logger():\n                file_path = await cloud_storage_download_blob_to_file.fn(\n                    bucket=self.bucket,\n                    blob=blob_path,\n                    path=local_file_path,\n                    gcp_credentials=self.gcp_credentials,\n                )\n                file_paths.append(file_path)\n        return file_paths\n\n    @sync_compatible\n    async def put_directory(\n        self,\n        local_path: Optional[str] = None,\n        to_path: Optional[str] = None,\n        ignore_file: Optional[str] = None,\n    ) -&gt; int:\n        \"\"\"\n        Uploads a directory from a given local path to the configured GCS bucket in a\n        given folder.\n\n        Defaults to uploading the entire contents the current working directory to the\n        block's bucket_folder.\n\n        Args:\n            local_path: Path to local directory to upload from.\n            to_path: Path in GCS bucket to upload to. Defaults to block's configured\n                bucket_folder.\n            ignore_file: Path to file containing gitignore style expressions for\n                filepaths to ignore.\n\n        Returns:\n            The number of files uploaded.\n        \"\"\"\n        if local_path is None:\n            local_path = os.path.abspath(\".\")\n        else:\n            local_path = os.path.expanduser(local_path)\n\n        to_path = self.bucket_folder if to_path is None else self._resolve_path(to_path)\n\n        included_files = None\n        if ignore_file:\n            with open(ignore_file, \"r\") as f:\n                ignore_patterns = f.readlines()\n            included_files = filter_files(local_path, ignore_patterns)\n\n        uploaded_file_count = 0\n        for local_file_path in Path(local_path).rglob(\"*\"):\n            if (\n                included_files is not None\n                and local_file_path.name not in included_files\n            ):\n                continue\n            elif not local_file_path.is_dir():\n                remote_file_path = str(\n                    PurePosixPath(to_path, local_file_path.relative_to(local_path))\n                )\n                local_file_content = local_file_path.read_bytes()\n                await self.write_path(remote_file_path, content=local_file_content)\n                uploaded_file_count += 1\n\n        return uploaded_file_count\n\n    @sync_compatible\n    async def read_path(self, path: str) -&gt; bytes:\n        \"\"\"\n        Read specified path from GCS and return contents. Provide the entire\n        path to the key in GCS.\n\n        Args:\n            path: Entire path to (and including) the key.\n\n        Returns:\n            A bytes or string representation of the blob object.\n        \"\"\"\n        path = self._resolve_path(path)\n        with disable_run_logger():\n            contents = await cloud_storage_download_blob_as_bytes.fn(\n                bucket=self.bucket, blob=path, gcp_credentials=self.gcp_credentials\n            )\n        return contents\n\n    @sync_compatible\n    async def write_path(self, path: str, content: bytes) -&gt; str:\n        \"\"\"\n        Writes to an GCS bucket.\n\n        Args:\n            path: The key name. Each object in your bucket has a unique\n                key (or key name).\n            content: What you are uploading to GCS Bucket.\n\n        Returns:\n            The path that the contents were written to.\n        \"\"\"\n        path = self._resolve_path(path)\n        with disable_run_logger():\n            await cloud_storage_upload_blob_from_string.fn(\n                data=content,\n                bucket=self.bucket,\n                blob=path,\n                gcp_credentials=self.gcp_credentials,\n            )\n        return path\n\n    # NEW BLOCK INTERFACE METHODS BELOW\n    def _join_bucket_folder(self, bucket_path: str = \"\") -&gt; str:\n        \"\"\"\n        Joins the base bucket folder to the bucket path.\n\n        NOTE: If a method reuses another method in this class, be careful to not\n        call this  twice because it'll join the bucket folder twice.\n        See https://github.com/PrefectHQ/prefect-aws/issues/141 for a past issue.\n        \"\"\"\n        bucket_path = str(bucket_path)\n        if self.bucket_folder != \"\" and bucket_path.startswith(self.bucket_folder):\n            self.logger.info(\n                f\"Bucket path {bucket_path!r} is already prefixed with \"\n                f\"bucket folder {self.bucket_folder!r}; is this intentional?\"\n            )\n\n        bucket_path = str(PurePosixPath(self.bucket_folder) / bucket_path)\n        if bucket_path in [\"\", \".\", \"/\"]:\n            # client.bucket.list_blobs(prefix=None) is the proper way\n            # of specifying the root folder of the bucket\n            bucket_path = None\n        return bucket_path\n\n    @sync_compatible\n    async def create_bucket(\n        self, location: Optional[str] = None, **create_kwargs\n    ) -&gt; \"Bucket\":\n        \"\"\"\n        Creates a bucket.\n\n        Args:\n            location: The location of the bucket.\n            **create_kwargs: Additional keyword arguments to pass to the\n                `create_bucket` method.\n\n        Returns:\n            The bucket object.\n\n        Examples:\n            Create a bucket.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket(bucket=\"my-bucket\")\n            gcs_bucket.create_bucket()\n            ```\n        \"\"\"\n        self.logger.info(f\"Creating bucket {self.bucket!r}.\")\n        client = self.gcp_credentials.get_cloud_storage_client()\n        bucket = await run_sync_in_worker_thread(\n            client.create_bucket, self.bucket, location=location, **create_kwargs\n        )\n        return bucket\n\n    @sync_compatible\n    async def get_bucket(self) -&gt; \"Bucket\":\n        \"\"\"\n        Returns the bucket object.\n\n        Returns:\n            The bucket object.\n\n        Examples:\n            Get the bucket object.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.get_bucket()\n            ```\n        \"\"\"\n        self.logger.info(f\"Getting bucket {self.bucket!r}.\")\n        client = self.gcp_credentials.get_cloud_storage_client()\n        bucket = await run_sync_in_worker_thread(client.get_bucket, self.bucket)\n        return bucket\n\n    @sync_compatible\n    async def list_blobs(self, folder: str = \"\") -&gt; List[\"Blob\"]:\n        \"\"\"\n        Lists all blobs in the bucket that are in a folder.\n        Folders are not included in the output.\n\n        Args:\n            folder: The folder to list blobs from.\n\n        Returns:\n            A list of Blob objects.\n\n        Examples:\n            Get all blobs from a folder named \"prefect\".\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.list_blobs(\"prefect\")\n            ```\n        \"\"\"\n        client = self.gcp_credentials.get_cloud_storage_client()\n\n        bucket_path = self._join_bucket_folder(folder)\n        if bucket_path is None:\n            self.logger.info(f\"Listing blobs in bucket {self.bucket!r}.\")\n        else:\n            self.logger.info(\n                f\"Listing blobs in folder {bucket_path!r} in bucket {self.bucket!r}.\"\n            )\n        blobs = await run_sync_in_worker_thread(\n            client.list_blobs, self.bucket, prefix=bucket_path\n        )\n\n        # Ignore folders\n        return [blob for blob in blobs if not blob.name.endswith(\"/\")]\n\n    @sync_compatible\n    async def list_folders(self, folder: str = \"\") -&gt; List[str]:\n        \"\"\"\n        Lists all folders and subfolders in the bucket.\n\n        Args:\n            folder: List all folders and subfolders inside given folder.\n\n        Returns:\n            A list of folders.\n\n        Examples:\n            Get all folders from a bucket named \"my-bucket\".\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.list_folders()\n            ```\n\n            Get all folders from a folder called years\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.list_folders(\"years\")\n            ```\n        \"\"\"\n\n        # Beware of calling _join_bucket_folder twice, see note in method.\n        # However, we just want to use it to check if we are listing the root folder\n        bucket_path = self._join_bucket_folder(folder)\n        if bucket_path is None:\n            self.logger.info(f\"Listing folders in bucket {self.bucket!r}.\")\n        else:\n            self.logger.info(\n                f\"Listing folders in {bucket_path!r} in bucket {self.bucket!r}.\"\n            )\n\n        blobs = await self.list_blobs(folder)\n        # gets all folders with full path\n        folders = {str(PurePosixPath(blob.name).parent) for blob in blobs}\n\n        return [folder for folder in folders if folder != \".\"]\n\n    @sync_compatible\n    async def download_object_to_path(\n        self,\n        from_path: str,\n        to_path: Optional[Union[str, Path]] = None,\n        **download_kwargs: Dict[str, Any],\n    ) -&gt; Path:\n        \"\"\"\n        Downloads an object from the object storage service to a path.\n\n        Args:\n            from_path: The path to the blob to download; this gets prefixed\n                with the bucket_folder.\n            to_path: The path to download the blob to. If not provided, the\n                blob's name will be used.\n            **download_kwargs: Additional keyword arguments to pass to\n                `Blob.download_to_filename`.\n\n        Returns:\n            The absolute path that the object was downloaded to.\n\n        Examples:\n            Download my_folder/notes.txt object to notes.txt.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n            ```\n        \"\"\"\n        if to_path is None:\n            to_path = Path(from_path).name\n\n        # making path absolute, but converting back to str here\n        # since !r looks nicer that way and filename arg expects str\n        to_path = str(Path(to_path).absolute())\n\n        bucket = await self.get_bucket()\n        bucket_path = self._join_bucket_folder(from_path)\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n            f\"to {to_path!r}.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.download_to_filename, filename=to_path, **download_kwargs\n        )\n        return Path(to_path)\n\n    @sync_compatible\n    async def download_object_to_file_object(\n        self,\n        from_path: str,\n        to_file_object: BinaryIO,\n        **download_kwargs: Dict[str, Any],\n    ) -&gt; BinaryIO:\n        \"\"\"\n        Downloads an object from the object storage service to a file-like object,\n        which can be a BytesIO object or a BufferedWriter.\n\n        Args:\n            from_path: The path to the blob to download from; this gets prefixed\n                with the bucket_folder.\n            to_file_object: The file-like object to download the blob to.\n            **download_kwargs: Additional keyword arguments to pass to\n                `Blob.download_to_file`.\n\n        Returns:\n            The file-like object that the object was downloaded to.\n\n        Examples:\n            Download my_folder/notes.txt object to a BytesIO object.\n            ```python\n            from io import BytesIO\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with BytesIO() as buf:\n                gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n            ```\n\n            Download my_folder/notes.txt object to a BufferedWriter.\n            ```python\n                from prefect_gcp.cloud_storage import GcsBucket\n\n                gcs_bucket = GcsBucket.load(\"my-bucket\")\n                with open(\"notes.txt\", \"wb\") as f:\n                    gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n            ```\n        \"\"\"\n        bucket = await self.get_bucket()\n\n        bucket_path = self._join_bucket_folder(from_path)\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n            f\"to file object.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.download_to_file, file_obj=to_file_object, **download_kwargs\n        )\n        return to_file_object\n\n    @sync_compatible\n    async def download_folder_to_path(\n        self,\n        from_folder: str,\n        to_folder: Optional[Union[str, Path]] = None,\n        **download_kwargs: Dict[str, Any],\n    ) -&gt; Path:\n        \"\"\"\n        Downloads objects *within* a folder (excluding the folder itself)\n        from the object storage service to a folder.\n\n        Args:\n            from_folder: The path to the folder to download from; this gets prefixed\n                with the bucket_folder.\n            to_folder: The path to download the folder to. If not provided, will default\n                to the current directory.\n            **download_kwargs: Additional keyword arguments to pass to\n                `Blob.download_to_filename`.\n\n        Returns:\n            The absolute path that the folder was downloaded to.\n\n        Examples:\n            Download my_folder to a local folder named my_folder.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n            ```\n        \"\"\"\n        if to_folder is None:\n            to_folder = \"\"\n        to_folder = Path(to_folder).absolute()\n\n        blobs = await self.list_blobs(folder=from_folder)\n        if len(blobs) == 0:\n            self.logger.warning(\n                f\"No blobs were downloaded from \"\n                f\"bucket {self.bucket!r} path {from_folder!r}.\"\n            )\n            return to_folder\n\n        # do not call self._join_bucket_folder for list_blobs\n        # because it's built-in to that method already!\n        # however, we still need to do it because we're using relative_to\n        bucket_folder = self._join_bucket_folder(from_folder)\n\n        async_coros = []\n        for blob in blobs:\n            bucket_path = PurePosixPath(blob.name).relative_to(bucket_folder)\n            if str(bucket_path).endswith(\"/\"):\n                continue\n            to_path = to_folder / bucket_path\n            to_path.parent.mkdir(parents=True, exist_ok=True)\n            self.logger.info(\n                f\"Downloading blob from bucket {self.bucket!r} path \"\n                f\"{str(bucket_path)!r} to {to_path}.\"\n            )\n            async_coros.append(\n                run_sync_in_worker_thread(\n                    blob.download_to_filename, filename=str(to_path), **download_kwargs\n                )\n            )\n        await asyncio.gather(*async_coros)\n\n        return to_folder\n\n    @sync_compatible\n    async def upload_from_path(\n        self,\n        from_path: Union[str, Path],\n        to_path: Optional[str] = None,\n        **upload_kwargs: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"\n        Uploads an object from a path to the object storage service.\n\n        Args:\n            from_path: The path to the file to upload from.\n            to_path: The path to upload the file to. If not provided, will use\n                the file name of from_path; this gets prefixed\n                with the bucket_folder.\n            **upload_kwargs: Additional keyword arguments to pass to\n                `Blob.upload_from_filename`.\n\n        Returns:\n            The path that the object was uploaded to.\n\n        Examples:\n            Upload notes.txt to my_folder/notes.txt.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n            ```\n        \"\"\"\n        if to_path is None:\n            to_path = Path(from_path).name\n\n        bucket_path = self._join_bucket_folder(to_path)\n        bucket = await self.get_bucket()\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Uploading from {from_path!r} to the bucket \"\n            f\"{self.bucket!r} path {bucket_path!r}.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.upload_from_filename, filename=from_path, **upload_kwargs\n        )\n        return bucket_path\n\n    @sync_compatible\n    async def upload_from_file_object(\n        self, from_file_object: BinaryIO, to_path: str, **upload_kwargs\n    ) -&gt; str:\n        \"\"\"\n        Uploads an object to the object storage service from a file-like object,\n        which can be a BytesIO object or a BufferedReader.\n\n        Args:\n            from_file_object: The file-like object to upload from.\n            to_path: The path to upload the object to; this gets prefixed\n                with the bucket_folder.\n            **upload_kwargs: Additional keyword arguments to pass to\n                `Blob.upload_from_file`.\n\n        Returns:\n            The path that the object was uploaded to.\n\n        Examples:\n            Upload my_folder/notes.txt object to a BytesIO object.\n            ```python\n            from io import BytesIO\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with open(\"notes.txt\", \"rb\") as f:\n                gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n            ```\n\n            Upload BufferedReader object to my_folder/notes.txt.\n            ```python\n            from io import BufferedReader\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with open(\"notes.txt\", \"rb\") as f:\n                gcs_bucket.upload_from_file_object(\n                    BufferedReader(f), \"my_folder/notes.txt\"\n                )\n            ```\n        \"\"\"\n        bucket = await self.get_bucket()\n\n        bucket_path = self._join_bucket_folder(to_path)\n        blob = bucket.blob(bucket_path)\n        self.logger.info(\n            f\"Uploading from file object to the bucket \"\n            f\"{self.bucket!r} path {bucket_path!r}.\"\n        )\n\n        await run_sync_in_worker_thread(\n            blob.upload_from_file, from_file_object, **upload_kwargs\n        )\n        return bucket_path\n\n    @sync_compatible\n    async def upload_from_folder(\n        self,\n        from_folder: Union[str, Path],\n        to_folder: Optional[str] = None,\n        **upload_kwargs: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"\n        Uploads files *within* a folder (excluding the folder itself)\n        to the object storage service folder.\n\n        Args:\n            from_folder: The path to the folder to upload from.\n            to_folder: The path to upload the folder to. If not provided, will default\n                to bucket_folder or the base directory of the bucket.\n            **upload_kwargs: Additional keyword arguments to pass to\n                `Blob.upload_from_filename`.\n\n        Returns:\n            The path that the folder was uploaded to.\n\n        Examples:\n            Upload local folder my_folder to the bucket's folder my_folder.\n            ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            gcs_bucket.upload_from_folder(\"my_folder\")\n            ```\n        \"\"\"\n        from_folder = Path(from_folder)\n        # join bucket folder expects string for the first input\n        # when it returns None, we need to convert it back to empty string\n        # so relative_to works\n        bucket_folder = self._join_bucket_folder(to_folder or \"\") or \"\"\n\n        num_uploaded = 0\n        bucket = await self.get_bucket()\n\n        async_coros = []\n        for from_path in from_folder.rglob(\"**/*\"):\n            if from_path.is_dir():\n                continue\n            bucket_path = str(Path(bucket_folder) / from_path.relative_to(from_folder))\n            self.logger.info(\n                f\"Uploading from {str(from_path)!r} to the bucket \"\n                f\"{self.bucket!r} path {bucket_path!r}.\"\n            )\n            blob = bucket.blob(bucket_path)\n            async_coros.append(\n                run_sync_in_worker_thread(\n                    blob.upload_from_filename, filename=from_path, **upload_kwargs\n                )\n            )\n            num_uploaded += 1\n        await asyncio.gather(*async_coros)\n        if num_uploaded == 0:\n            self.logger.warning(f\"No files were uploaded from {from_folder}.\")\n        return bucket_folder\n\n    @sync_compatible\n    async def upload_from_dataframe(\n        self,\n        df: \"DataFrame\",\n        to_path: str,\n        serialization_format: Union[\n            str, DataFrameSerializationFormat\n        ] = DataFrameSerializationFormat.CSV_GZIP,\n        **upload_kwargs: Dict[str, Any],\n    ) -&gt; str:\n        \"\"\"Upload a Pandas DataFrame to Google Cloud Storage in various formats.\n\n        This function uploads the data in a Pandas DataFrame to Google Cloud Storage\n        in a specified format, such as .csv, .csv.gz, .parquet,\n        .parquet.snappy, and .parquet.gz.\n\n        Args:\n            df: The Pandas DataFrame to be uploaded.\n            to_path: The destination path for the uploaded DataFrame.\n            serialization_format: The format to serialize the DataFrame into.\n                When passed as a `str`, the valid options are:\n                'csv', 'csv_gzip',  'parquet', 'parquet_snappy', 'parquet_gzip'.\n                Defaults to `DataFrameSerializationFormat.CSV_GZIP`.\n            **upload_kwargs: Additional keyword arguments to pass to the underlying\n            `Blob.upload_from_dataframe` method.\n\n        Returns:\n            The path that the object was uploaded to.\n        \"\"\"\n        if isinstance(serialization_format, str):\n            serialization_format = DataFrameSerializationFormat[\n                serialization_format.upper()\n            ]\n\n        with BytesIO() as bytes_buffer:\n            if serialization_format.format == \"parquet\":\n                df.to_parquet(\n                    path=bytes_buffer,\n                    compression=serialization_format.compression,\n                    index=False,\n                )\n            elif serialization_format.format == \"csv\":\n                df.to_csv(\n                    path_or_buf=bytes_buffer,\n                    compression=serialization_format.compression,\n                    index=False,\n                )\n\n            bytes_buffer.seek(0)\n            to_path = serialization_format.fix_extension_with(gcs_blob_path=to_path)\n\n            return await self.upload_from_file_object(\n                from_file_object=bytes_buffer,\n                to_path=to_path,\n                **{\"content_type\": serialization_format.content_type, **upload_kwargs},\n            )\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket-attributes","title":"Attributes","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.basepath","title":"<code>basepath: str</code>  <code>property</code>","text":"<p>Read-only property that mirrors the bucket folder.</p> <p>Used for deployment.</p>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket-functions","title":"Functions","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.create_bucket","title":"<code>create_bucket</code>  <code>async</code>","text":"<p>Creates a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>location</code> <code>Optional[str]</code> <p>The location of the bucket.</p> <code>None</code> <code>**create_kwargs</code> <p>Additional keyword arguments to pass to the <code>create_bucket</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Bucket</code> <p>The bucket object.</p> <p>Examples:</p> <p>Create a bucket. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket(bucket=\"my-bucket\")\ngcs_bucket.create_bucket()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def create_bucket(\n    self, location: Optional[str] = None, **create_kwargs\n) -&gt; \"Bucket\":\n    \"\"\"\n    Creates a bucket.\n\n    Args:\n        location: The location of the bucket.\n        **create_kwargs: Additional keyword arguments to pass to the\n            `create_bucket` method.\n\n    Returns:\n        The bucket object.\n\n    Examples:\n        Create a bucket.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket(bucket=\"my-bucket\")\n        gcs_bucket.create_bucket()\n        ```\n    \"\"\"\n    self.logger.info(f\"Creating bucket {self.bucket!r}.\")\n    client = self.gcp_credentials.get_cloud_storage_client()\n    bucket = await run_sync_in_worker_thread(\n        client.create_bucket, self.bucket, location=location, **create_kwargs\n    )\n    return bucket\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_folder_to_path","title":"<code>download_folder_to_path</code>  <code>async</code>","text":"<p>Downloads objects within a folder (excluding the folder itself) from the object storage service to a folder.</p> <p>Parameters:</p> Name Type Description Default <code>from_folder</code> <code>str</code> <p>The path to the folder to download from; this gets prefixed with the bucket_folder.</p> required <code>to_folder</code> <code>Optional[Union[str, Path]]</code> <p>The path to download the folder to. If not provided, will default to the current directory.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>The absolute path that the folder was downloaded to.</p> <p>Examples:</p> <p>Download my_folder to a local folder named my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def download_folder_to_path(\n    self,\n    from_folder: str,\n    to_folder: Optional[Union[str, Path]] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; Path:\n    \"\"\"\n    Downloads objects *within* a folder (excluding the folder itself)\n    from the object storage service to a folder.\n\n    Args:\n        from_folder: The path to the folder to download from; this gets prefixed\n            with the bucket_folder.\n        to_folder: The path to download the folder to. If not provided, will default\n            to the current directory.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_filename`.\n\n    Returns:\n        The absolute path that the folder was downloaded to.\n\n    Examples:\n        Download my_folder to a local folder named my_folder.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n        ```\n    \"\"\"\n    if to_folder is None:\n        to_folder = \"\"\n    to_folder = Path(to_folder).absolute()\n\n    blobs = await self.list_blobs(folder=from_folder)\n    if len(blobs) == 0:\n        self.logger.warning(\n            f\"No blobs were downloaded from \"\n            f\"bucket {self.bucket!r} path {from_folder!r}.\"\n        )\n        return to_folder\n\n    # do not call self._join_bucket_folder for list_blobs\n    # because it's built-in to that method already!\n    # however, we still need to do it because we're using relative_to\n    bucket_folder = self._join_bucket_folder(from_folder)\n\n    async_coros = []\n    for blob in blobs:\n        bucket_path = PurePosixPath(blob.name).relative_to(bucket_folder)\n        if str(bucket_path).endswith(\"/\"):\n            continue\n        to_path = to_folder / bucket_path\n        to_path.parent.mkdir(parents=True, exist_ok=True)\n        self.logger.info(\n            f\"Downloading blob from bucket {self.bucket!r} path \"\n            f\"{str(bucket_path)!r} to {to_path}.\"\n        )\n        async_coros.append(\n            run_sync_in_worker_thread(\n                blob.download_to_filename, filename=str(to_path), **download_kwargs\n            )\n        )\n    await asyncio.gather(*async_coros)\n\n    return to_folder\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_object_to_file_object","title":"<code>download_object_to_file_object</code>  <code>async</code>","text":"<p>Downloads an object from the object storage service to a file-like object, which can be a BytesIO object or a BufferedWriter.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the blob to download from; this gets prefixed with the bucket_folder.</p> required <code>to_file_object</code> <code>BinaryIO</code> <p>The file-like object to download the blob to.</p> required <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_file</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>BinaryIO</code> <p>The file-like object that the object was downloaded to.</p> <p>Examples:</p> <p>Download my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith BytesIO() as buf:\n    gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n</code></pre></p> <p>Download my_folder/notes.txt object to a BufferedWriter. <pre><code>    from prefect_gcp.cloud_storage import GcsBucket\n\n    gcs_bucket = GcsBucket.load(\"my-bucket\")\n    with open(\"notes.txt\", \"wb\") as f:\n        gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def download_object_to_file_object(\n    self,\n    from_path: str,\n    to_file_object: BinaryIO,\n    **download_kwargs: Dict[str, Any],\n) -&gt; BinaryIO:\n    \"\"\"\n    Downloads an object from the object storage service to a file-like object,\n    which can be a BytesIO object or a BufferedWriter.\n\n    Args:\n        from_path: The path to the blob to download from; this gets prefixed\n            with the bucket_folder.\n        to_file_object: The file-like object to download the blob to.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_file`.\n\n    Returns:\n        The file-like object that the object was downloaded to.\n\n    Examples:\n        Download my_folder/notes.txt object to a BytesIO object.\n        ```python\n        from io import BytesIO\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        with BytesIO() as buf:\n            gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n        ```\n\n        Download my_folder/notes.txt object to a BufferedWriter.\n        ```python\n            from prefect_gcp.cloud_storage import GcsBucket\n\n            gcs_bucket = GcsBucket.load(\"my-bucket\")\n            with open(\"notes.txt\", \"wb\") as f:\n                gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n        ```\n    \"\"\"\n    bucket = await self.get_bucket()\n\n    bucket_path = self._join_bucket_folder(from_path)\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n        f\"to file object.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.download_to_file, file_obj=to_file_object, **download_kwargs\n    )\n    return to_file_object\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.download_object_to_path","title":"<code>download_object_to_path</code>  <code>async</code>","text":"<p>Downloads an object from the object storage service to a path.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>str</code> <p>The path to the blob to download; this gets prefixed with the bucket_folder.</p> required <code>to_path</code> <code>Optional[Union[str, Path]]</code> <p>The path to download the blob to. If not provided, the blob's name will be used.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Path</code> <p>The absolute path that the object was downloaded to.</p> <p>Examples:</p> <p>Download my_folder/notes.txt object to notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def download_object_to_path(\n    self,\n    from_path: str,\n    to_path: Optional[Union[str, Path]] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; Path:\n    \"\"\"\n    Downloads an object from the object storage service to a path.\n\n    Args:\n        from_path: The path to the blob to download; this gets prefixed\n            with the bucket_folder.\n        to_path: The path to download the blob to. If not provided, the\n            blob's name will be used.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_filename`.\n\n    Returns:\n        The absolute path that the object was downloaded to.\n\n    Examples:\n        Download my_folder/notes.txt object to notes.txt.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n        ```\n    \"\"\"\n    if to_path is None:\n        to_path = Path(from_path).name\n\n    # making path absolute, but converting back to str here\n    # since !r looks nicer that way and filename arg expects str\n    to_path = str(Path(to_path).absolute())\n\n    bucket = await self.get_bucket()\n    bucket_path = self._join_bucket_folder(from_path)\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Downloading blob from bucket {self.bucket!r} path {bucket_path!r}\"\n        f\"to {to_path!r}.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.download_to_filename, filename=to_path, **download_kwargs\n    )\n    return Path(to_path)\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.get_bucket","title":"<code>get_bucket</code>  <code>async</code>","text":"<p>Returns the bucket object.</p> <p>Returns:</p> Type Description <code>Bucket</code> <p>The bucket object.</p> <p>Examples:</p> <p>Get the bucket object. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.get_bucket()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def get_bucket(self) -&gt; \"Bucket\":\n    \"\"\"\n    Returns the bucket object.\n\n    Returns:\n        The bucket object.\n\n    Examples:\n        Get the bucket object.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.get_bucket()\n        ```\n    \"\"\"\n    self.logger.info(f\"Getting bucket {self.bucket!r}.\")\n    client = self.gcp_credentials.get_cloud_storage_client()\n    bucket = await run_sync_in_worker_thread(client.get_bucket, self.bucket)\n    return bucket\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.get_directory","title":"<code>get_directory</code>  <code>async</code>","text":"<p>Copies a folder from the configured GCS bucket to a local directory. Defaults to copying the entire contents of the block's bucket_folder to the current working directory.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>Optional[str]</code> <p>Path in GCS bucket to download from. Defaults to the block's configured bucket_folder.</p> <code>None</code> <code>local_path</code> <code>Optional[str]</code> <p>Local path to download GCS bucket contents to. Defaults to the current working directory.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Union[str, Path]]</code> <p>A list of downloaded file paths.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def get_directory(\n    self, from_path: Optional[str] = None, local_path: Optional[str] = None\n) -&gt; List[Union[str, Path]]:\n    \"\"\"\n    Copies a folder from the configured GCS bucket to a local directory.\n    Defaults to copying the entire contents of the block's bucket_folder\n    to the current working directory.\n\n    Args:\n        from_path: Path in GCS bucket to download from. Defaults to the block's\n            configured bucket_folder.\n        local_path: Local path to download GCS bucket contents to.\n            Defaults to the current working directory.\n\n    Returns:\n        A list of downloaded file paths.\n    \"\"\"\n    from_path = (\n        self.bucket_folder if from_path is None else self._resolve_path(from_path)\n    )\n\n    if local_path is None:\n        local_path = os.path.abspath(\".\")\n    else:\n        local_path = os.path.abspath(os.path.expanduser(local_path))\n\n    project = self.gcp_credentials.project\n    client = self.gcp_credentials.get_cloud_storage_client(project=project)\n\n    blobs = await run_sync_in_worker_thread(\n        client.list_blobs, self.bucket, prefix=from_path\n    )\n\n    file_paths = []\n    for blob in blobs:\n        blob_path = blob.name\n        if blob_path[-1] == \"/\":\n            # object is a folder and will be created if it contains any objects\n            continue\n        local_file_path = os.path.join(local_path, blob_path)\n        os.makedirs(os.path.dirname(local_file_path), exist_ok=True)\n\n        with disable_run_logger():\n            file_path = await cloud_storage_download_blob_to_file.fn(\n                bucket=self.bucket,\n                blob=blob_path,\n                path=local_file_path,\n                gcp_credentials=self.gcp_credentials,\n            )\n            file_paths.append(file_path)\n    return file_paths\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.list_blobs","title":"<code>list_blobs</code>  <code>async</code>","text":"<p>Lists all blobs in the bucket that are in a folder. Folders are not included in the output.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>The folder to list blobs from.</p> <code>''</code> <p>Returns:</p> Type Description <code>List[Blob]</code> <p>A list of Blob objects.</p> <p>Examples:</p> <p>Get all blobs from a folder named \"prefect\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_blobs(\"prefect\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def list_blobs(self, folder: str = \"\") -&gt; List[\"Blob\"]:\n    \"\"\"\n    Lists all blobs in the bucket that are in a folder.\n    Folders are not included in the output.\n\n    Args:\n        folder: The folder to list blobs from.\n\n    Returns:\n        A list of Blob objects.\n\n    Examples:\n        Get all blobs from a folder named \"prefect\".\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.list_blobs(\"prefect\")\n        ```\n    \"\"\"\n    client = self.gcp_credentials.get_cloud_storage_client()\n\n    bucket_path = self._join_bucket_folder(folder)\n    if bucket_path is None:\n        self.logger.info(f\"Listing blobs in bucket {self.bucket!r}.\")\n    else:\n        self.logger.info(\n            f\"Listing blobs in folder {bucket_path!r} in bucket {self.bucket!r}.\"\n        )\n    blobs = await run_sync_in_worker_thread(\n        client.list_blobs, self.bucket, prefix=bucket_path\n    )\n\n    # Ignore folders\n    return [blob for blob in blobs if not blob.name.endswith(\"/\")]\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.list_folders","title":"<code>list_folders</code>  <code>async</code>","text":"<p>Lists all folders and subfolders in the bucket.</p> <p>Parameters:</p> Name Type Description Default <code>folder</code> <code>str</code> <p>List all folders and subfolders inside given folder.</p> <code>''</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of folders.</p> <p>Examples:</p> <p>Get all folders from a bucket named \"my-bucket\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders()\n</code></pre></p> <p>Get all folders from a folder called years <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders(\"years\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def list_folders(self, folder: str = \"\") -&gt; List[str]:\n    \"\"\"\n    Lists all folders and subfolders in the bucket.\n\n    Args:\n        folder: List all folders and subfolders inside given folder.\n\n    Returns:\n        A list of folders.\n\n    Examples:\n        Get all folders from a bucket named \"my-bucket\".\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.list_folders()\n        ```\n\n        Get all folders from a folder called years\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.list_folders(\"years\")\n        ```\n    \"\"\"\n\n    # Beware of calling _join_bucket_folder twice, see note in method.\n    # However, we just want to use it to check if we are listing the root folder\n    bucket_path = self._join_bucket_folder(folder)\n    if bucket_path is None:\n        self.logger.info(f\"Listing folders in bucket {self.bucket!r}.\")\n    else:\n        self.logger.info(\n            f\"Listing folders in {bucket_path!r} in bucket {self.bucket!r}.\"\n        )\n\n    blobs = await self.list_blobs(folder)\n    # gets all folders with full path\n    folders = {str(PurePosixPath(blob.name).parent) for blob in blobs}\n\n    return [folder for folder in folders if folder != \".\"]\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.put_directory","title":"<code>put_directory</code>  <code>async</code>","text":"<p>Uploads a directory from a given local path to the configured GCS bucket in a given folder.</p> <p>Defaults to uploading the entire contents the current working directory to the block's bucket_folder.</p> <p>Parameters:</p> Name Type Description Default <code>local_path</code> <code>Optional[str]</code> <p>Path to local directory to upload from.</p> <code>None</code> <code>to_path</code> <code>Optional[str]</code> <p>Path in GCS bucket to upload to. Defaults to block's configured bucket_folder.</p> <code>None</code> <code>ignore_file</code> <code>Optional[str]</code> <p>Path to file containing gitignore style expressions for filepaths to ignore.</p> <code>None</code> <p>Returns:</p> Type Description <code>int</code> <p>The number of files uploaded.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def put_directory(\n    self,\n    local_path: Optional[str] = None,\n    to_path: Optional[str] = None,\n    ignore_file: Optional[str] = None,\n) -&gt; int:\n    \"\"\"\n    Uploads a directory from a given local path to the configured GCS bucket in a\n    given folder.\n\n    Defaults to uploading the entire contents the current working directory to the\n    block's bucket_folder.\n\n    Args:\n        local_path: Path to local directory to upload from.\n        to_path: Path in GCS bucket to upload to. Defaults to block's configured\n            bucket_folder.\n        ignore_file: Path to file containing gitignore style expressions for\n            filepaths to ignore.\n\n    Returns:\n        The number of files uploaded.\n    \"\"\"\n    if local_path is None:\n        local_path = os.path.abspath(\".\")\n    else:\n        local_path = os.path.expanduser(local_path)\n\n    to_path = self.bucket_folder if to_path is None else self._resolve_path(to_path)\n\n    included_files = None\n    if ignore_file:\n        with open(ignore_file, \"r\") as f:\n            ignore_patterns = f.readlines()\n        included_files = filter_files(local_path, ignore_patterns)\n\n    uploaded_file_count = 0\n    for local_file_path in Path(local_path).rglob(\"*\"):\n        if (\n            included_files is not None\n            and local_file_path.name not in included_files\n        ):\n            continue\n        elif not local_file_path.is_dir():\n            remote_file_path = str(\n                PurePosixPath(to_path, local_file_path.relative_to(local_path))\n            )\n            local_file_content = local_file_path.read_bytes()\n            await self.write_path(remote_file_path, content=local_file_content)\n            uploaded_file_count += 1\n\n    return uploaded_file_count\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.read_path","title":"<code>read_path</code>  <code>async</code>","text":"<p>Read specified path from GCS and return contents. Provide the entire path to the key in GCS.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Entire path to (and including) the key.</p> required <p>Returns:</p> Type Description <code>bytes</code> <p>A bytes or string representation of the blob object.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def read_path(self, path: str) -&gt; bytes:\n    \"\"\"\n    Read specified path from GCS and return contents. Provide the entire\n    path to the key in GCS.\n\n    Args:\n        path: Entire path to (and including) the key.\n\n    Returns:\n        A bytes or string representation of the blob object.\n    \"\"\"\n    path = self._resolve_path(path)\n    with disable_run_logger():\n        contents = await cloud_storage_download_blob_as_bytes.fn(\n            bucket=self.bucket, blob=path, gcp_credentials=self.gcp_credentials\n        )\n    return contents\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_dataframe","title":"<code>upload_from_dataframe</code>  <code>async</code>","text":"<p>Upload a Pandas DataFrame to Google Cloud Storage in various formats.</p> <p>This function uploads the data in a Pandas DataFrame to Google Cloud Storage in a specified format, such as .csv, .csv.gz, .parquet, .parquet.snappy, and .parquet.gz.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>The Pandas DataFrame to be uploaded.</p> required <code>to_path</code> <code>str</code> <p>The destination path for the uploaded DataFrame.</p> required <code>serialization_format</code> <code>Union[str, DataFrameSerializationFormat]</code> <p>The format to serialize the DataFrame into. When passed as a <code>str</code>, the valid options are: 'csv', 'csv_gzip',  'parquet', 'parquet_snappy', 'parquet_gzip'. Defaults to <code>DataFrameSerializationFormat.CSV_GZIP</code>.</p> <code>CSV_GZIP</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to the underlying</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path that the object was uploaded to.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def upload_from_dataframe(\n    self,\n    df: \"DataFrame\",\n    to_path: str,\n    serialization_format: Union[\n        str, DataFrameSerializationFormat\n    ] = DataFrameSerializationFormat.CSV_GZIP,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"Upload a Pandas DataFrame to Google Cloud Storage in various formats.\n\n    This function uploads the data in a Pandas DataFrame to Google Cloud Storage\n    in a specified format, such as .csv, .csv.gz, .parquet,\n    .parquet.snappy, and .parquet.gz.\n\n    Args:\n        df: The Pandas DataFrame to be uploaded.\n        to_path: The destination path for the uploaded DataFrame.\n        serialization_format: The format to serialize the DataFrame into.\n            When passed as a `str`, the valid options are:\n            'csv', 'csv_gzip',  'parquet', 'parquet_snappy', 'parquet_gzip'.\n            Defaults to `DataFrameSerializationFormat.CSV_GZIP`.\n        **upload_kwargs: Additional keyword arguments to pass to the underlying\n        `Blob.upload_from_dataframe` method.\n\n    Returns:\n        The path that the object was uploaded to.\n    \"\"\"\n    if isinstance(serialization_format, str):\n        serialization_format = DataFrameSerializationFormat[\n            serialization_format.upper()\n        ]\n\n    with BytesIO() as bytes_buffer:\n        if serialization_format.format == \"parquet\":\n            df.to_parquet(\n                path=bytes_buffer,\n                compression=serialization_format.compression,\n                index=False,\n            )\n        elif serialization_format.format == \"csv\":\n            df.to_csv(\n                path_or_buf=bytes_buffer,\n                compression=serialization_format.compression,\n                index=False,\n            )\n\n        bytes_buffer.seek(0)\n        to_path = serialization_format.fix_extension_with(gcs_blob_path=to_path)\n\n        return await self.upload_from_file_object(\n            from_file_object=bytes_buffer,\n            to_path=to_path,\n            **{\"content_type\": serialization_format.content_type, **upload_kwargs},\n        )\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_file_object","title":"<code>upload_from_file_object</code>  <code>async</code>","text":"<p>Uploads an object to the object storage service from a file-like object, which can be a BytesIO object or a BufferedReader.</p> <p>Parameters:</p> Name Type Description Default <code>from_file_object</code> <code>BinaryIO</code> <p>The file-like object to upload from.</p> required <code>to_path</code> <code>str</code> <p>The path to upload the object to; this gets prefixed with the bucket_folder.</p> required <code>**upload_kwargs</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_file</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path that the object was uploaded to.</p> <p>Examples:</p> <p>Upload my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n</code></pre></p> <p>Upload BufferedReader object to my_folder/notes.txt. <pre><code>from io import BufferedReader\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(\n        BufferedReader(f), \"my_folder/notes.txt\"\n    )\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def upload_from_file_object(\n    self, from_file_object: BinaryIO, to_path: str, **upload_kwargs\n) -&gt; str:\n    \"\"\"\n    Uploads an object to the object storage service from a file-like object,\n    which can be a BytesIO object or a BufferedReader.\n\n    Args:\n        from_file_object: The file-like object to upload from.\n        to_path: The path to upload the object to; this gets prefixed\n            with the bucket_folder.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_file`.\n\n    Returns:\n        The path that the object was uploaded to.\n\n    Examples:\n        Upload my_folder/notes.txt object to a BytesIO object.\n        ```python\n        from io import BytesIO\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        with open(\"notes.txt\", \"rb\") as f:\n            gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n        ```\n\n        Upload BufferedReader object to my_folder/notes.txt.\n        ```python\n        from io import BufferedReader\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        with open(\"notes.txt\", \"rb\") as f:\n            gcs_bucket.upload_from_file_object(\n                BufferedReader(f), \"my_folder/notes.txt\"\n            )\n        ```\n    \"\"\"\n    bucket = await self.get_bucket()\n\n    bucket_path = self._join_bucket_folder(to_path)\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Uploading from file object to the bucket \"\n        f\"{self.bucket!r} path {bucket_path!r}.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.upload_from_file, from_file_object, **upload_kwargs\n    )\n    return bucket_path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_folder","title":"<code>upload_from_folder</code>  <code>async</code>","text":"<p>Uploads files within a folder (excluding the folder itself) to the object storage service folder.</p> <p>Parameters:</p> Name Type Description Default <code>from_folder</code> <code>Union[str, Path]</code> <p>The path to the folder to upload from.</p> required <code>to_folder</code> <code>Optional[str]</code> <p>The path to upload the folder to. If not provided, will default to bucket_folder or the base directory of the bucket.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path that the folder was uploaded to.</p> <p>Examples:</p> <p>Upload local folder my_folder to the bucket's folder my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_folder(\"my_folder\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def upload_from_folder(\n    self,\n    from_folder: Union[str, Path],\n    to_folder: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Uploads files *within* a folder (excluding the folder itself)\n    to the object storage service folder.\n\n    Args:\n        from_folder: The path to the folder to upload from.\n        to_folder: The path to upload the folder to. If not provided, will default\n            to bucket_folder or the base directory of the bucket.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_filename`.\n\n    Returns:\n        The path that the folder was uploaded to.\n\n    Examples:\n        Upload local folder my_folder to the bucket's folder my_folder.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.upload_from_folder(\"my_folder\")\n        ```\n    \"\"\"\n    from_folder = Path(from_folder)\n    # join bucket folder expects string for the first input\n    # when it returns None, we need to convert it back to empty string\n    # so relative_to works\n    bucket_folder = self._join_bucket_folder(to_folder or \"\") or \"\"\n\n    num_uploaded = 0\n    bucket = await self.get_bucket()\n\n    async_coros = []\n    for from_path in from_folder.rglob(\"**/*\"):\n        if from_path.is_dir():\n            continue\n        bucket_path = str(Path(bucket_folder) / from_path.relative_to(from_folder))\n        self.logger.info(\n            f\"Uploading from {str(from_path)!r} to the bucket \"\n            f\"{self.bucket!r} path {bucket_path!r}.\"\n        )\n        blob = bucket.blob(bucket_path)\n        async_coros.append(\n            run_sync_in_worker_thread(\n                blob.upload_from_filename, filename=from_path, **upload_kwargs\n            )\n        )\n        num_uploaded += 1\n    await asyncio.gather(*async_coros)\n    if num_uploaded == 0:\n        self.logger.warning(f\"No files were uploaded from {from_folder}.\")\n    return bucket_folder\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.upload_from_path","title":"<code>upload_from_path</code>  <code>async</code>","text":"<p>Uploads an object from a path to the object storage service.</p> <p>Parameters:</p> Name Type Description Default <code>from_path</code> <code>Union[str, Path]</code> <p>The path to the file to upload from.</p> required <code>to_path</code> <code>Optional[str]</code> <p>The path to upload the file to. If not provided, will use the file name of from_path; this gets prefixed with the bucket_folder.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path that the object was uploaded to.</p> <p>Examples:</p> <p>Upload notes.txt to my_folder/notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def upload_from_path(\n    self,\n    from_path: Union[str, Path],\n    to_path: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Uploads an object from a path to the object storage service.\n\n    Args:\n        from_path: The path to the file to upload from.\n        to_path: The path to upload the file to. If not provided, will use\n            the file name of from_path; this gets prefixed\n            with the bucket_folder.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_filename`.\n\n    Returns:\n        The path that the object was uploaded to.\n\n    Examples:\n        Upload notes.txt to my_folder/notes.txt.\n        ```python\n        from prefect_gcp.cloud_storage import GcsBucket\n\n        gcs_bucket = GcsBucket.load(\"my-bucket\")\n        gcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n        ```\n    \"\"\"\n    if to_path is None:\n        to_path = Path(from_path).name\n\n    bucket_path = self._join_bucket_folder(to_path)\n    bucket = await self.get_bucket()\n    blob = bucket.blob(bucket_path)\n    self.logger.info(\n        f\"Uploading from {from_path!r} to the bucket \"\n        f\"{self.bucket!r} path {bucket_path!r}.\"\n    )\n\n    await run_sync_in_worker_thread(\n        blob.upload_from_filename, filename=from_path, **upload_kwargs\n    )\n    return bucket_path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.GcsBucket.write_path","title":"<code>write_path</code>  <code>async</code>","text":"<p>Writes to an GCS bucket.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The key name. Each object in your bucket has a unique key (or key name).</p> required <code>content</code> <code>bytes</code> <p>What you are uploading to GCS Bucket.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path that the contents were written to.</p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@sync_compatible\nasync def write_path(self, path: str, content: bytes) -&gt; str:\n    \"\"\"\n    Writes to an GCS bucket.\n\n    Args:\n        path: The key name. Each object in your bucket has a unique\n            key (or key name).\n        content: What you are uploading to GCS Bucket.\n\n    Returns:\n        The path that the contents were written to.\n    \"\"\"\n    path = self._resolve_path(path)\n    with disable_run_logger():\n        await cloud_storage_upload_blob_from_string.fn(\n            data=content,\n            bucket=self.bucket,\n            blob=path,\n            gcp_credentials=self.gcp_credentials,\n        )\n    return path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage-functions","title":"Functions","text":""},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_copy_blob","title":"<code>cloud_storage_copy_blob</code>  <code>async</code>","text":"<p>Copies data from one Google Cloud Storage bucket to another, without downloading it locally.</p> <p>Parameters:</p> Name Type Description Default <code>source_bucket</code> <code>str</code> <p>Source bucket name.</p> required <code>dest_bucket</code> <code>str</code> <p>Destination bucket name.</p> required <code>source_blob</code> <code>str</code> <p>Source blob name.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>dest_blob</code> <code>Optional[str]</code> <p>Destination blob name; if not provided, defaults to source_blob.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**copy_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Bucket.copy_blob</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>Destination blob name.</p> Example <p>Copies blob from one bucket to another. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_copy_blob\n\n@flow()\ndef example_cloud_storage_copy_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_copy_blob(\n        \"source_bucket\",\n        \"dest_bucket\",\n        \"source_blob\",\n        gcp_credentials\n    )\n    return blob\n\nexample_cloud_storage_copy_blob_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_copy_blob(\n    source_bucket: str,\n    dest_bucket: str,\n    source_blob: str,\n    gcp_credentials: GcpCredentials,\n    dest_blob: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **copy_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Copies data from one Google Cloud Storage bucket to another,\n    without downloading it locally.\n\n    Args:\n        source_bucket: Source bucket name.\n        dest_bucket: Destination bucket name.\n        source_blob: Source blob name.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        dest_blob: Destination blob name; if not provided, defaults to source_blob.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **copy_kwargs: Additional keyword arguments to pass to\n            `Bucket.copy_blob`.\n\n    Returns:\n        Destination blob name.\n\n    Example:\n        Copies blob from one bucket to another.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_copy_blob\n\n        @flow()\n        def example_cloud_storage_copy_blob_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            blob = cloud_storage_copy_blob(\n                \"source_bucket\",\n                \"dest_bucket\",\n                \"source_blob\",\n                gcp_credentials\n            )\n            return blob\n\n        example_cloud_storage_copy_blob_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\n        \"Copying blob named %s from the %s bucket to the %s bucket\",\n        source_blob,\n        source_bucket,\n        dest_bucket,\n    )\n\n    source_bucket_obj = await _get_bucket(\n        source_bucket, gcp_credentials, project=project\n    )\n\n    dest_bucket_obj = await _get_bucket(dest_bucket, gcp_credentials, project=project)\n    if dest_blob is None:\n        dest_blob = source_blob\n\n    source_blob_obj = source_bucket_obj.blob(source_blob)\n    await run_sync_in_worker_thread(\n        source_bucket_obj.copy_blob,\n        blob=source_blob_obj,\n        destination_bucket=dest_bucket_obj,\n        new_name=dest_blob,\n        timeout=timeout,\n        **copy_kwargs,\n    )\n\n    return dest_blob\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_create_bucket","title":"<code>cloud_storage_create_bucket</code>  <code>async</code>","text":"<p>Creates a bucket.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>location</code> <code>Optional[str]</code> <p>Location of the bucket.</p> <code>None</code> <code>**create_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>client.create_bucket</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The bucket name.</p> Example <p>Creates a bucket named \"prefect\". <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_create_bucket\n\n@flow()\ndef example_cloud_storage_create_bucket_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials)\n\nexample_cloud_storage_create_bucket_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_create_bucket(\n    bucket: str,\n    gcp_credentials: GcpCredentials,\n    project: Optional[str] = None,\n    location: Optional[str] = None,\n    **create_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Creates a bucket.\n\n    Args:\n        bucket: Name of the bucket.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        location: Location of the bucket.\n        **create_kwargs: Additional keyword arguments to pass to `client.create_bucket`.\n\n    Returns:\n        The bucket name.\n\n    Example:\n        Creates a bucket named \"prefect\".\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_create_bucket\n\n        @flow()\n        def example_cloud_storage_create_bucket_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials)\n\n        example_cloud_storage_create_bucket_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Creating %s bucket\", bucket)\n\n    client = gcp_credentials.get_cloud_storage_client(project=project)\n    await run_sync_in_worker_thread(\n        client.create_bucket, bucket, location=location, **create_kwargs\n    )\n    return bucket\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_as_bytes","title":"<code>cloud_storage_download_blob_as_bytes</code>  <code>async</code>","text":"<p>Downloads a blob as bytes.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>chunk_size</code> <code>int</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_as_bytes</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>bytes</code> <p>A bytes or string representation of the blob object.</p> Example <p>Downloads blob from bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes\n\n@flow()\ndef example_cloud_storage_download_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    contents = cloud_storage_download_blob_as_bytes(\n        \"bucket\", \"blob\", gcp_credentials)\n    return contents\n\nexample_cloud_storage_download_blob_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_download_blob_as_bytes(\n    bucket: str,\n    blob: str,\n    gcp_credentials: GcpCredentials,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; bytes:\n    \"\"\"\n    Downloads a blob as bytes.\n\n    Args:\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        chunk_size (int, optional): The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_as_bytes`.\n\n    Returns:\n        A bytes or string representation of the blob object.\n\n    Example:\n        Downloads blob from bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes\n\n        @flow()\n        def example_cloud_storage_download_blob_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            contents = cloud_storage_download_blob_as_bytes(\n                \"bucket\", \"blob\", gcp_credentials)\n            return contents\n\n        example_cloud_storage_download_blob_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Downloading blob named %s from the %s bucket\", blob, bucket)\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    contents = await run_sync_in_worker_thread(\n        blob_obj.download_as_bytes, timeout=timeout, **download_kwargs\n    )\n    return contents\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_download_blob_to_file","title":"<code>cloud_storage_download_blob_to_file</code>  <code>async</code>","text":"<p>Downloads a blob to a file path.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>path</code> <code>Union[str, Path]</code> <p>Downloads the contents to the provided file path; if the path is a directory, automatically joins the blob name.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>chunk_size</code> <code>int</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**download_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.download_to_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[str, Path]</code> <p>The path to the blob object.</p> Example <p>Downloads blob from bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file\n\n@flow()\ndef example_cloud_storage_download_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    path = cloud_storage_download_blob_to_file(\n        \"bucket\", \"blob\", \"file_path\", gcp_credentials)\n    return path\n\nexample_cloud_storage_download_blob_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_download_blob_to_file(\n    bucket: str,\n    blob: str,\n    path: Union[str, Path],\n    gcp_credentials: GcpCredentials,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **download_kwargs: Dict[str, Any],\n) -&gt; Union[str, Path]:\n    \"\"\"\n    Downloads a blob to a file path.\n\n    Args:\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        path: Downloads the contents to the provided file path;\n            if the path is a directory, automatically joins the blob name.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        chunk_size (int, optional): The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **download_kwargs: Additional keyword arguments to pass to\n            `Blob.download_to_filename`.\n\n    Returns:\n        The path to the blob object.\n\n    Example:\n        Downloads blob from bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file\n\n        @flow()\n        def example_cloud_storage_download_blob_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            path = cloud_storage_download_blob_to_file(\n                \"bucket\", \"blob\", \"file_path\", gcp_credentials)\n            return path\n\n        example_cloud_storage_download_blob_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\n        \"Downloading blob named %s from the %s bucket to %s\", blob, bucket, path\n    )\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    if os.path.isdir(path):\n        if isinstance(path, Path):\n            path = path.joinpath(blob)  # keep as Path if Path is passed\n        else:\n            path = os.path.join(path, blob)  # keep as str if a str is passed\n\n    await run_sync_in_worker_thread(\n        blob_obj.download_to_filename, path, timeout=timeout, **download_kwargs\n    )\n    return path\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_file","title":"<code>cloud_storage_upload_blob_from_file</code>  <code>async</code>","text":"<p>Uploads a blob from file path or file-like object. Usage for passing in file-like object is if the data was downloaded from the web; can bypass writing to disk and directly upload to Cloud Storage.</p> <p>Parameters:</p> Name Type Description Default <code>file</code> <code>Union[str, Path, BytesIO]</code> <p>Path to data or file like object to upload.</p> required <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>content_type</code> <code>Optional[str]</code> <p>Type of content being uploaded.</p> <code>None</code> <code>chunk_size</code> <code>Optional[int]</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_file</code> or <code>Blob.upload_from_filename</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The blob name.</p> Example <p>Uploads blob to bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file\n\n@flow()\ndef example_cloud_storage_upload_blob_from_file_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_upload_blob_from_file(\n        \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials)\n    return blob\n\nexample_cloud_storage_upload_blob_from_file_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_upload_blob_from_file(\n    file: Union[str, Path, BytesIO],\n    bucket: str,\n    blob: str,\n    gcp_credentials: GcpCredentials,\n    content_type: Optional[str] = None,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Uploads a blob from file path or file-like object. Usage for passing in\n    file-like object is if the data was downloaded from the web;\n    can bypass writing to disk and directly upload to Cloud Storage.\n\n    Args:\n        file: Path to data or file like object to upload.\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        content_type: Type of content being uploaded.\n        chunk_size: The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_file` or `Blob.upload_from_filename`.\n\n    Returns:\n        The blob name.\n\n    Example:\n        Uploads blob to bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file\n\n        @flow()\n        def example_cloud_storage_upload_blob_from_file_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            blob = cloud_storage_upload_blob_from_file(\n                \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials)\n            return blob\n\n        example_cloud_storage_upload_blob_from_file_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Uploading blob named %s to the %s bucket\", blob, bucket)\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    if isinstance(file, BytesIO):\n        await run_sync_in_worker_thread(\n            blob_obj.upload_from_file,\n            file,\n            content_type=content_type,\n            timeout=timeout,\n            **upload_kwargs,\n        )\n    else:\n        await run_sync_in_worker_thread(\n            blob_obj.upload_from_filename,\n            file,\n            content_type=content_type,\n            timeout=timeout,\n            **upload_kwargs,\n        )\n    return blob\n</code></pre>"},{"location":"cloud_storage/#prefect_gcp.cloud_storage.cloud_storage_upload_blob_from_string","title":"<code>cloud_storage_upload_blob_from_string</code>  <code>async</code>","text":"<p>Uploads a blob from a string or bytes representation of data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Union[str, bytes]</code> <p>String or bytes representation of data to upload.</p> required <code>bucket</code> <code>str</code> <p>Name of the bucket.</p> required <code>blob</code> <code>str</code> <p>Name of the Cloud Storage blob.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>content_type</code> <code>Optional[str]</code> <p>Type of content being uploaded.</p> <code>None</code> <code>chunk_size</code> <code>Optional[int]</code> <p>The size of a chunk of data whenever iterating (in bytes). This must be a multiple of 256 KB per the API specification.</p> <code>None</code> <code>encryption_key</code> <code>Optional[str]</code> <p>An encryption key.</p> <code>None</code> <code>timeout</code> <code>Union[float, Tuple[float, float]]</code> <p>The number of seconds the transport should wait for the server response. Can also be passed as a tuple (connect_timeout, read_timeout).</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <code>**upload_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to <code>Blob.upload_from_string</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The blob name.</p> Example <p>Uploads blob to bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string\n\n@flow()\ndef example_cloud_storage_upload_blob_from_string_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_upload_blob_from_string(\n        \"data\", \"bucket\", \"blob\", gcp_credentials)\n    return blob\n\nexample_cloud_storage_upload_blob_from_string_flow()\n</code></pre></p> Source code in <code>prefect_gcp/cloud_storage.py</code> <pre><code>@task\nasync def cloud_storage_upload_blob_from_string(\n    data: Union[str, bytes],\n    bucket: str,\n    blob: str,\n    gcp_credentials: GcpCredentials,\n    content_type: Optional[str] = None,\n    chunk_size: Optional[int] = None,\n    encryption_key: Optional[str] = None,\n    timeout: Union[float, Tuple[float, float]] = 60,\n    project: Optional[str] = None,\n    **upload_kwargs: Dict[str, Any],\n) -&gt; str:\n    \"\"\"\n    Uploads a blob from a string or bytes representation of data.\n\n    Args:\n        data: String or bytes representation of data to upload.\n        bucket: Name of the bucket.\n        blob: Name of the Cloud Storage blob.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        content_type: Type of content being uploaded.\n        chunk_size: The size of a chunk of data whenever\n            iterating (in bytes). This must be a multiple of 256 KB\n            per the API specification.\n        encryption_key: An encryption key.\n        timeout: The number of seconds the transport should wait\n            for the server response. Can also be passed as a tuple\n            (connect_timeout, read_timeout).\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n        **upload_kwargs: Additional keyword arguments to pass to\n            `Blob.upload_from_string`.\n\n    Returns:\n        The blob name.\n\n    Example:\n        Uploads blob to bucket.\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string\n\n        @flow()\n        def example_cloud_storage_upload_blob_from_string_flow():\n            gcp_credentials = GcpCredentials(\n                service_account_file=\"/path/to/service/account/keyfile.json\")\n            blob = cloud_storage_upload_blob_from_string(\n                \"data\", \"bucket\", \"blob\", gcp_credentials)\n            return blob\n\n        example_cloud_storage_upload_blob_from_string_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Uploading blob named %s to the %s bucket\", blob, bucket)\n\n    bucket_obj = await _get_bucket(bucket, gcp_credentials, project=project)\n    blob_obj = bucket_obj.blob(\n        blob, chunk_size=chunk_size, encryption_key=encryption_key\n    )\n\n    await run_sync_in_worker_thread(\n        blob_obj.upload_from_string,\n        data,\n        content_type=content_type,\n        timeout=timeout,\n        **upload_kwargs,\n    )\n    return blob\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>If you'd like to help contribute to fix an issue or add a feature to <code>prefect-gcp</code>, please propose changes through a pull request from a fork of the repository.</p> <p>Here are the steps:</p> <ol> <li>Fork the repository</li> <li>Clone the forked repository</li> <li>Install the repository and its dependencies: <pre><code>pip install -e \".[dev]\"\n</code></pre></li> <li>Make desired changes</li> <li>Add tests</li> <li>Insert an entry to CHANGELOG.md</li> <li>Install <code>pre-commit</code> to perform quality checks prior to commit: <pre><code>pre-commit install\n</code></pre></li> <li><code>git commit</code>, <code>git push</code>, and create a pull request</li> </ol>"},{"location":"credentials/","title":"Credentials","text":""},{"location":"credentials/#prefect_gcp.credentials","title":"<code>prefect_gcp.credentials</code>","text":"<p>Module handling GCP credentials.</p>"},{"location":"credentials/#prefect_gcp.credentials-classes","title":"Classes","text":""},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials","title":"<code>GcpCredentials</code>","text":"<p>             Bases: <code>CredentialsBlock</code></p> <p>Block used to manage authentication with GCP. Google authentication is handled via the <code>google.oauth2</code> module or through the CLI. Specify either one of service <code>account_file</code> or <code>service_account_info</code>; if both are not specified, the client will try to detect the credentials following Google's Application Default Credentials. See Google's Authentication documentation for details on inference and recommended authentication patterns.</p> <p>Attributes:</p> Name Type Description <code>service_account_file</code> <code>Optional[Path]</code> <p>Path to the service account JSON keyfile.</p> <code>service_account_info</code> <code>Optional[SecretDict]</code> <p>The contents of the keyfile as a dict.</p> Example <p>Load GCP credentials stored in a <code>GCP Credentials</code> Block: <pre><code>from prefect_gcp import GcpCredentials\ngcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\")\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>class GcpCredentials(CredentialsBlock):\n    \"\"\"\n    Block used to manage authentication with GCP. Google authentication is\n    handled via the `google.oauth2` module or through the CLI.\n    Specify either one of service `account_file` or `service_account_info`; if both\n    are not specified, the client will try to detect the credentials following Google's\n    [Application Default Credentials](https://cloud.google.com/docs/authentication/application-default-credentials).\n    See Google's [Authentication documentation](https://cloud.google.com/docs/authentication#service-accounts)\n    for details on inference and recommended authentication patterns.\n\n    Attributes:\n        service_account_file: Path to the service account JSON keyfile.\n        service_account_info: The contents of the keyfile as a dict.\n\n    Example:\n        Load GCP credentials stored in a `GCP Credentials` Block:\n        ```python\n        from prefect_gcp import GcpCredentials\n        gcp_credentials_block = GcpCredentials.load(\"BLOCK_NAME\")\n        ```\n    \"\"\"  # noqa\n\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n    _block_type_name = \"GCP Credentials\"\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/credentials/#prefect_gcp.credentials.GcpCredentials\"  # noqa: E501\n\n    service_account_file: Optional[Path] = Field(\n        default=None, description=\"Path to the service account JSON keyfile.\"\n    )\n    service_account_info: Optional[SecretDict] = Field(\n        default=None, description=\"The contents of the keyfile as a dict.\"\n    )\n    project: Optional[str] = Field(\n        default=None, description=\"The GCP project to use for the client.\"\n    )\n\n    _service_account_email: Optional[str] = None\n\n    @root_validator\n    def _provide_one_service_account_source(cls, values):\n        \"\"\"\n        Ensure that only a service account file or service account info ias provided.\n        \"\"\"\n        both_service_account = (\n            values.get(\"service_account_info\") is not None\n            and values.get(\"service_account_file\") is not None\n        )\n        if both_service_account:\n            raise ValueError(\n                \"Only one of service_account_info or service_account_file \"\n                \"can be specified at once\"\n            )\n        return values\n\n    @validator(\"service_account_file\")\n    def _check_service_account_file(cls, file):\n        \"\"\"Get full path of provided file and make sure that it exists.\"\"\"\n        if not file:\n            return file\n\n        service_account_file = Path(file).expanduser()\n        if not service_account_file.exists():\n            raise ValueError(\"The provided path to the service account is invalid\")\n        return service_account_file\n\n    @validator(\"service_account_info\", pre=True)\n    def _convert_json_string_json_service_account_info(cls, value):\n        \"\"\"\n        Converts service account info provided as a json formatted string\n        to a dictionary\n        \"\"\"\n        if isinstance(value, str):\n            try:\n                service_account_info = json.loads(value)\n                return service_account_info\n            except Exception:\n                raise ValueError(\"Unable to decode service_account_info\")\n        else:\n            return value\n\n    def block_initialization(self):\n        credentials = self.get_credentials_from_service_account()\n        if self.project is None:\n            if self.service_account_info or self.service_account_file:\n                credentials_project = credentials.project_id\n            else:  # google.auth.default using gcloud auth application-default login\n                credentials_project = credentials.quota_project_id\n            self.project = credentials_project\n\n        if hasattr(credentials, \"service_account_email\"):\n            self._service_account_email = credentials.service_account_email\n\n    def get_credentials_from_service_account(self) -&gt; Credentials:\n        \"\"\"\n        Helper method to serialize credentials by using either\n        service_account_file or service_account_info.\n        \"\"\"\n        if self.service_account_info:\n            credentials = Credentials.from_service_account_info(\n                self.service_account_info.get_secret_value(),\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n        elif self.service_account_file:\n            credentials = Credentials.from_service_account_file(\n                self.service_account_file,\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n        else:\n            credentials, _ = google.auth.default()\n        return credentials\n\n    @sync_compatible\n    async def get_access_token(self):\n        \"\"\"\n        See: https://stackoverflow.com/a/69107745\n        Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/\n        \"\"\"  # noqa\n        request = google.auth.transport.requests.Request()\n        credentials = self.get_credentials_from_service_account()\n        await run_sync_in_worker_thread(credentials.refresh, request)\n        return credentials.token\n\n    def get_client(\n        self,\n        client_type: Union[str, ClientType],\n        **get_client_kwargs: Dict[str, Any],\n    ) -&gt; Any:\n        \"\"\"\n        Helper method to dynamically get a client type.\n\n        Args:\n            client_type: The name of the client to get.\n            **get_client_kwargs: Additional keyword arguments to pass to the\n                `get_*_client` method.\n\n        Returns:\n            An authenticated client.\n\n        Raises:\n            ValueError: if the client is not supported.\n        \"\"\"\n        if isinstance(client_type, str):\n            client_type = ClientType(client_type)\n        client_type = client_type.value\n        get_client_method = getattr(self, f\"get_{client_type}_client\")\n        return get_client_method(**get_client_kwargs)\n\n    @_raise_help_msg(\"cloud_storage\")\n    def get_cloud_storage_client(\n        self, project: Optional[str] = None\n    ) -&gt; \"StorageClient\":\n        \"\"\"\n        Gets an authenticated Cloud Storage client.\n\n        Args:\n            project: Name of the project to use; overrides the base\n                class's project if provided.\n\n        Returns:\n            An authenticated Cloud Storage client.\n\n        Examples:\n            Gets a GCP Cloud Storage client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_cloud_storage_client()\n            example_get_client_flow()\n            ```\n\n            Gets a GCP Cloud Storage client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_cloud_storage_client()\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n\n        # override class project if method project is provided\n        project = project or self.project\n        storage_client = StorageClient(credentials=credentials, project=project)\n        return storage_client\n\n    @_raise_help_msg(\"bigquery\")\n    def get_bigquery_client(\n        self, project: str = None, location: str = None\n    ) -&gt; \"BigQueryClient\":\n        \"\"\"\n        Gets an authenticated BigQuery client.\n\n        Args:\n            project: Name of the project to use; overrides the base\n                class's project if provided.\n            location: Location to use.\n\n        Returns:\n            An authenticated BigQuery client.\n\n        Examples:\n            Gets a GCP BigQuery client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_bigquery_client()\n            example_get_client_flow()\n            ```\n\n            Gets a GCP BigQuery client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_bigquery_client()\n\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n\n        # override class project if method project is provided\n        project = project or self.project\n        big_query_client = BigQueryClient(\n            credentials=credentials, project=project, location=location\n        )\n        return big_query_client\n\n    @_raise_help_msg(\"secret_manager\")\n    def get_secret_manager_client(self) -&gt; \"SecretManagerServiceClient\":\n        \"\"\"\n        Gets an authenticated Secret Manager Service client.\n\n        Returns:\n            An authenticated Secret Manager Service client.\n\n        Examples:\n            Gets a GCP Secret Manager client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_secret_manager_client()\n            example_get_client_flow()\n            ```\n\n            Gets a GCP Cloud Storage client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_secret_manager_client()\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n\n        # doesn't accept project; must pass in project in tasks\n        secret_manager_client = SecretManagerServiceClient(credentials=credentials)\n        return secret_manager_client\n\n    @_raise_help_msg(\"aiplatform\")\n    def get_job_service_client(\n        self, client_options: Dict[str, Any] = None\n    ) -&gt; \"JobServiceClient\":\n        \"\"\"\n        Gets an authenticated Job Service client for Vertex AI.\n\n        Returns:\n            An authenticated Job Service client.\n\n        Examples:\n            Gets a GCP Job Service client from a path.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_file = \"~/.secrets/prefect-service-account.json\"\n                client = GcpCredentials(\n                    service_account_file=service_account_file\n                ).get_job_service_client()\n\n            example_get_client_flow()\n            ```\n\n            Gets a GCP Cloud Storage client from a dictionary.\n            ```python\n            from prefect import flow\n            from prefect_gcp.credentials import GcpCredentials\n\n            @flow()\n            def example_get_client_flow():\n                service_account_info = {\n                    \"type\": \"service_account\",\n                    \"project_id\": \"project_id\",\n                    \"private_key_id\": \"private_key_id\",\n                    \"private_key\": \"private_key\",\n                    \"client_email\": \"client_email\",\n                    \"client_id\": \"client_id\",\n                    \"auth_uri\": \"auth_uri\",\n                    \"token_uri\": \"token_uri\",\n                    \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                    \"client_x509_cert_url\": \"client_x509_cert_url\"\n                }\n                client = GcpCredentials(\n                    service_account_info=service_account_info\n                ).get_job_service_client()\n\n            example_get_client_flow()\n            ```\n        \"\"\"\n        credentials = self.get_credentials_from_service_account()\n        job_service_client = JobServiceClient(\n            credentials=credentials, client_options=client_options\n        )\n        return job_service_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials-functions","title":"Functions","text":""},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_access_token","title":"<code>get_access_token</code>  <code>async</code>","text":"Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@sync_compatible\nasync def get_access_token(self):\n    \"\"\"\n    See: https://stackoverflow.com/a/69107745\n    Also: https://www.jhanley.com/google-cloud-creating-oauth-access-tokens-for-rest-api-calls/\n    \"\"\"  # noqa\n    request = google.auth.transport.requests.Request()\n    credentials = self.get_credentials_from_service_account()\n    await run_sync_in_worker_thread(credentials.refresh, request)\n    return credentials.token\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_bigquery_client","title":"<code>get_bigquery_client</code>","text":"<p>Gets an authenticated BigQuery client.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>str</code> <p>Name of the project to use; overrides the base class's project if provided.</p> <code>None</code> <code>location</code> <code>str</code> <p>Location to use.</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>An authenticated BigQuery client.</p> <p>Examples:</p> <p>Gets a GCP BigQuery client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_bigquery_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP BigQuery client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_bigquery_client()\n\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"bigquery\")\ndef get_bigquery_client(\n    self, project: str = None, location: str = None\n) -&gt; \"BigQueryClient\":\n    \"\"\"\n    Gets an authenticated BigQuery client.\n\n    Args:\n        project: Name of the project to use; overrides the base\n            class's project if provided.\n        location: Location to use.\n\n    Returns:\n        An authenticated BigQuery client.\n\n    Examples:\n        Gets a GCP BigQuery client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_bigquery_client()\n        example_get_client_flow()\n        ```\n\n        Gets a GCP BigQuery client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_bigquery_client()\n\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n\n    # override class project if method project is provided\n    project = project or self.project\n    big_query_client = BigQueryClient(\n        credentials=credentials, project=project, location=location\n    )\n    return big_query_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_client","title":"<code>get_client</code>","text":"<p>Helper method to dynamically get a client type.</p> <p>Parameters:</p> Name Type Description Default <code>client_type</code> <code>Union[str, ClientType]</code> <p>The name of the client to get.</p> required <code>**get_client_kwargs</code> <code>Dict[str, Any]</code> <p>Additional keyword arguments to pass to the <code>get_*_client</code> method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Any</code> <p>An authenticated client.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>if the client is not supported.</p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>def get_client(\n    self,\n    client_type: Union[str, ClientType],\n    **get_client_kwargs: Dict[str, Any],\n) -&gt; Any:\n    \"\"\"\n    Helper method to dynamically get a client type.\n\n    Args:\n        client_type: The name of the client to get.\n        **get_client_kwargs: Additional keyword arguments to pass to the\n            `get_*_client` method.\n\n    Returns:\n        An authenticated client.\n\n    Raises:\n        ValueError: if the client is not supported.\n    \"\"\"\n    if isinstance(client_type, str):\n        client_type = ClientType(client_type)\n    client_type = client_type.value\n    get_client_method = getattr(self, f\"get_{client_type}_client\")\n    return get_client_method(**get_client_kwargs)\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_cloud_storage_client","title":"<code>get_cloud_storage_client</code>","text":"<p>Gets an authenticated Cloud Storage client.</p> <p>Parameters:</p> Name Type Description Default <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the base class's project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>Client</code> <p>An authenticated Cloud Storage client.</p> <p>Examples:</p> <p>Gets a GCP Cloud Storage client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"cloud_storage\")\ndef get_cloud_storage_client(\n    self, project: Optional[str] = None\n) -&gt; \"StorageClient\":\n    \"\"\"\n    Gets an authenticated Cloud Storage client.\n\n    Args:\n        project: Name of the project to use; overrides the base\n            class's project if provided.\n\n    Returns:\n        An authenticated Cloud Storage client.\n\n    Examples:\n        Gets a GCP Cloud Storage client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_cloud_storage_client()\n        example_get_client_flow()\n        ```\n\n        Gets a GCP Cloud Storage client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_cloud_storage_client()\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n\n    # override class project if method project is provided\n    project = project or self.project\n    storage_client = StorageClient(credentials=credentials, project=project)\n    return storage_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_credentials_from_service_account","title":"<code>get_credentials_from_service_account</code>","text":"<p>Helper method to serialize credentials by using either service_account_file or service_account_info.</p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>def get_credentials_from_service_account(self) -&gt; Credentials:\n    \"\"\"\n    Helper method to serialize credentials by using either\n    service_account_file or service_account_info.\n    \"\"\"\n    if self.service_account_info:\n        credentials = Credentials.from_service_account_info(\n            self.service_account_info.get_secret_value(),\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n        )\n    elif self.service_account_file:\n        credentials = Credentials.from_service_account_file(\n            self.service_account_file,\n            scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n        )\n    else:\n        credentials, _ = google.auth.default()\n    return credentials\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_job_service_client","title":"<code>get_job_service_client</code>","text":"<p>Gets an authenticated Job Service client for Vertex AI.</p> <p>Returns:</p> Type Description <code>JobServiceClient</code> <p>An authenticated Job Service client.</p> <p>Examples:</p> <p>Gets a GCP Job Service client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"aiplatform\")\ndef get_job_service_client(\n    self, client_options: Dict[str, Any] = None\n) -&gt; \"JobServiceClient\":\n    \"\"\"\n    Gets an authenticated Job Service client for Vertex AI.\n\n    Returns:\n        An authenticated Job Service client.\n\n    Examples:\n        Gets a GCP Job Service client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_job_service_client()\n\n        example_get_client_flow()\n        ```\n\n        Gets a GCP Cloud Storage client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_job_service_client()\n\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n    job_service_client = JobServiceClient(\n        credentials=credentials, client_options=client_options\n    )\n    return job_service_client\n</code></pre>"},{"location":"credentials/#prefect_gcp.credentials.GcpCredentials.get_secret_manager_client","title":"<code>get_secret_manager_client</code>","text":"<p>Gets an authenticated Secret Manager Service client.</p> <p>Returns:</p> Type Description <code>SecretManagerServiceClient</code> <p>An authenticated Secret Manager Service client.</p> <p>Examples:</p> <p>Gets a GCP Secret Manager client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre></p> Source code in <code>prefect_gcp/credentials.py</code> <pre><code>@_raise_help_msg(\"secret_manager\")\ndef get_secret_manager_client(self) -&gt; \"SecretManagerServiceClient\":\n    \"\"\"\n    Gets an authenticated Secret Manager Service client.\n\n    Returns:\n        An authenticated Secret Manager Service client.\n\n    Examples:\n        Gets a GCP Secret Manager client from a path.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_file = \"~/.secrets/prefect-service-account.json\"\n            client = GcpCredentials(\n                service_account_file=service_account_file\n            ).get_secret_manager_client()\n        example_get_client_flow()\n        ```\n\n        Gets a GCP Cloud Storage client from a dictionary.\n        ```python\n        from prefect import flow\n        from prefect_gcp.credentials import GcpCredentials\n\n        @flow()\n        def example_get_client_flow():\n            service_account_info = {\n                \"type\": \"service_account\",\n                \"project_id\": \"project_id\",\n                \"private_key_id\": \"private_key_id\",\n                \"private_key\": \"private_key\",\n                \"client_email\": \"client_email\",\n                \"client_id\": \"client_id\",\n                \"auth_uri\": \"auth_uri\",\n                \"token_uri\": \"token_uri\",\n                \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n                \"client_x509_cert_url\": \"client_x509_cert_url\"\n            }\n            client = GcpCredentials(\n                service_account_info=service_account_info\n            ).get_secret_manager_client()\n        example_get_client_flow()\n        ```\n    \"\"\"\n    credentials = self.get_credentials_from_service_account()\n\n    # doesn't accept project; must pass in project in tasks\n    secret_manager_client = SecretManagerServiceClient(credentials=credentials)\n    return secret_manager_client\n</code></pre>"},{"location":"examples_catalog/","title":"Examples Catalog","text":"<p>Below is a list of examples for <code>prefect-gcp</code>.</p>"},{"location":"examples_catalog/#bigquery-module","title":"Bigquery Module","text":"<p>Execute operation with parameters: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        CREATE TABLE mydataset.trips AS (\n        SELECT\n            bikeid,\n            start_time,\n            duration_minutes\n        FROM\n            bigquery-public-data.austin_bikeshare.bikeshare_trips\n        LIMIT %(limit)s\n        );\n    '''\n    warehouse.execute(operation, parameters={\"limit\": 5})\n</code></pre> Execute operation with parameters, fetching two new rows at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 6;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_many(\n            operation,\n            parameters=parameters,\n            size=2\n        )\n        print(result)\n</code></pre> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_load_cloud_storage\n\n@flow\ndef example_bigquery_load_cloud_storage_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    result = bigquery_load_cloud_storage(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        uri=\"uri\",\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_load_cloud_storage_flow()\n</code></pre> Create mytable in mydataset and insert two rows into it: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"bigquery\") as warehouse:\n    create_operation = '''\n    CREATE TABLE IF NOT EXISTS mydataset.mytable (\n        col1 STRING,\n        col2 INTEGER,\n        col3 BOOLEAN\n    )\n    '''\n    warehouse.execute(create_operation)\n    insert_operation = '''\n    INSERT INTO mydataset.mytable (col1, col2, col3) VALUES (%s, %s, %s)\n    '''\n    seq_of_parameters = [\n        (\"a\", 1, True),\n        (\"b\", 2, False),\n    ]\n    warehouse.execute_many(\n        insert_operation,\n        seq_of_parameters=seq_of_parameters\n    )\n</code></pre> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_insert_stream\nfrom google.cloud.bigquery import SchemaField\n\n@flow\ndef example_bigquery_insert_stream_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    records = [\n        {\"number\": 1, \"text\": \"abc\", \"bool\": True},\n        {\"number\": 2, \"text\": \"def\", \"bool\": False},\n    ]\n    result = bigquery_insert_stream(\n        dataset=\"integrations\",\n        table=\"test_table\",\n        records=records,\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_insert_stream_flow()\n</code></pre> <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_load_file\nfrom google.cloud.bigquery import SchemaField\n\n@flow\ndef example_bigquery_load_file_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    result = bigquery_load_file(\n        dataset=\"dataset\",\n        table=\"test_table\",\n        path=\"path\",\n        gcp_credentials=gcp_credentials\n    )\n    return result\n\nexample_bigquery_load_file_flow()\n</code></pre> Execute operation with parameters, fetching one new row at a time: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    for _ in range(0, 3):\n        result = warehouse.fetch_one(operation, parameters=parameters)\n        print(result)\n</code></pre> Execute operation with parameters, fetching all rows: <pre><code>from prefect_gcp.bigquery import BigQueryWarehouse\n\nwith BigQueryWarehouse.load(\"BLOCK_NAME\") as warehouse:\n    operation = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = %(corpus)s\n        AND word_count &gt;= %(min_word_count)s\n        ORDER BY word_count DESC\n        LIMIT 3;\n    '''\n    parameters = {\n        \"corpus\": \"romeoandjuliet\",\n        \"min_word_count\": 250,\n    }\n    result = warehouse.fetch_all(operation, parameters=parameters)\n</code></pre> Queries the public names database, returning 10 results. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.bigquery import bigquery_query\n\n@flow\ndef example_bigquery_query_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\",\n        project=\"project\"\n    )\n    query = '''\n        SELECT word, word_count\n        FROM `bigquery-public-data.samples.shakespeare`\n        WHERE corpus = @corpus\n        AND word_count &gt;= @min_word_count\n        ORDER BY word_count DESC;\n    '''\n    query_params = [\n        (\"corpus\", \"STRING\", \"romeoandjuliet\"),\n        (\"min_word_count\", \"INT64\", 250)\n    ]\n    result = bigquery_query(\n        query, gcp_credentials, query_params=query_params\n    )\n    return result\n\nexample_bigquery_query_flow()\n</code></pre></p>"},{"location":"examples_catalog/#cloud-storage-module","title":"Cloud Storage Module","text":"<p>Upload my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(f, \"my_folder/notes.txt\")\n</code></pre></p> <p>Upload BufferedReader object to my_folder/notes.txt. <pre><code>from io import BufferedReader\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith open(\"notes.txt\", \"rb\") as f:\n    gcs_bucket.upload_from_file_object(\n        BufferedReader(f), \"my_folder/notes.txt\"\n    )\n</code></pre> Copies blob from one bucket to another. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_copy_blob\n\n@flow()\ndef example_cloud_storage_copy_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_copy_blob(\n        \"source_bucket\",\n        \"dest_bucket\",\n        \"source_blob\",\n        gcp_credentials\n    )\n    return blob\n\nexample_cloud_storage_copy_blob_flow()\n</code></pre> Uploads blob to bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_file\n\n@flow()\ndef example_cloud_storage_upload_blob_from_file_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_upload_blob_from_file(\n        \"/path/somewhere\", \"bucket\", \"blob\", gcp_credentials)\n    return blob\n\nexample_cloud_storage_upload_blob_from_file_flow()\n</code></pre> Upload local folder my_folder to the bucket's folder my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_folder(\"my_folder\")\n</code></pre> Create a bucket. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket(bucket=\"my-bucket\")\ngcs_bucket.create_bucket()\n</code></pre> Downloads blob from bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_download_blob_as_bytes\n\n@flow()\ndef example_cloud_storage_download_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    contents = cloud_storage_download_blob_as_bytes(\n        \"bucket\", \"blob\", gcp_credentials)\n    return contents\n\nexample_cloud_storage_download_blob_flow()\n</code></pre> Upload notes.txt to my_folder/notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.upload_from_path(\"notes.txt\", \"my_folder/notes.txt\")\n</code></pre> Download my_folder/notes.txt object to a BytesIO object. <pre><code>from io import BytesIO\nfrom prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\nwith BytesIO() as buf:\n    gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", buf)\n</code></pre></p> <p>Download my_folder/notes.txt object to a BufferedWriter. <pre><code>    from prefect_gcp.cloud_storage import GcsBucket\n\n    gcs_bucket = GcsBucket.load(\"my-bucket\")\n    with open(\"notes.txt\", \"wb\") as f:\n        gcs_bucket.download_object_to_file_object(\"my_folder/notes.txt\", f)\n</code></pre> Download my_folder to a local folder named my_folder. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_folder_to_path(\"my_folder\", \"my_folder\")\n</code></pre> Creates a bucket named \"prefect\". <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_create_bucket\n\n@flow()\ndef example_cloud_storage_create_bucket_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    bucket = cloud_storage_create_bucket(\"prefect\", gcp_credentials)\n\nexample_cloud_storage_create_bucket_flow()\n</code></pre> Uploads blob to bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_upload_blob_from_string\n\n@flow()\ndef example_cloud_storage_upload_blob_from_string_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    blob = cloud_storage_upload_blob_from_string(\n        \"data\", \"bucket\", \"blob\", gcp_credentials)\n    return blob\n\nexample_cloud_storage_upload_blob_from_string_flow()\n</code></pre> Download my_folder/notes.txt object to notes.txt. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.download_object_to_path(\"my_folder/notes.txt\", \"notes.txt\")\n</code></pre> Get all blobs from a folder named \"prefect\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_blobs(\"prefect\")\n</code></pre> Get all folders from a bucket named \"my-bucket\". <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders()\n</code></pre></p> <p>Get all folders from a folder called years <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.list_folders(\"years\")\n</code></pre> Get the bucket object. <pre><code>from prefect_gcp.cloud_storage import GcsBucket\n\ngcs_bucket = GcsBucket.load(\"my-bucket\")\ngcs_bucket.get_bucket()\n</code></pre> Downloads blob from bucket. <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.cloud_storage import cloud_storage_download_blob_to_file\n\n@flow()\ndef example_cloud_storage_download_blob_flow():\n    gcp_credentials = GcpCredentials(\n        service_account_file=\"/path/to/service/account/keyfile.json\")\n    path = cloud_storage_download_blob_to_file(\n        \"bucket\", \"blob\", \"file_path\", gcp_credentials)\n    return path\n\nexample_cloud_storage_download_blob_flow()\n</code></pre></p>"},{"location":"examples_catalog/#credentials-module","title":"Credentials Module","text":"<p>Gets a GCP BigQuery client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_bigquery_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP BigQuery client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_bigquery_client()\n\nexample_get_client_flow()\n</code></pre> Gets a GCP Secret Manager client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_secret_manager_client()\nexample_get_client_flow()\n</code></pre> Gets a GCP Cloud Storage client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_cloud_storage_client()\nexample_get_client_flow()\n</code></pre> Gets a GCP Job Service client from a path. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_file = \"~/.secrets/prefect-service-account.json\"\n    client = GcpCredentials(\n        service_account_file=service_account_file\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p> <p>Gets a GCP Cloud Storage client from a dictionary. <pre><code>from prefect import flow\nfrom prefect_gcp.credentials import GcpCredentials\n\n@flow()\ndef example_get_client_flow():\n    service_account_info = {\n        \"type\": \"service_account\",\n        \"project_id\": \"project_id\",\n        \"private_key_id\": \"private_key_id\",\n        \"private_key\": \"private_key\",\n        \"client_email\": \"client_email\",\n        \"client_id\": \"client_id\",\n        \"auth_uri\": \"auth_uri\",\n        \"token_uri\": \"token_uri\",\n        \"auth_provider_x509_cert_url\": \"auth_provider_x509_cert_url\",\n        \"client_x509_cert_url\": \"client_x509_cert_url\"\n    }\n    client = GcpCredentials(\n        service_account_info=service_account_info\n    ).get_job_service_client()\n\nexample_get_client_flow()\n</code></pre></p>"},{"location":"secret_manager/","title":"Secret Manager","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager","title":"<code>prefect_gcp.secret_manager</code>","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager-classes","title":"Classes","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret","title":"<code>GcpSecret</code>","text":"<p>             Bases: <code>SecretBlock</code></p> <p>Manages a secret in Google Cloud Platform's Secret Manager.</p> <p>Attributes:</p> Name Type Description <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> <code>secret_name</code> <code>str</code> <p>Name of the secret to manage.</p> <code>secret_version</code> <code>str</code> <p>Version number of the secret to use, or \"latest\".</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>class GcpSecret(SecretBlock):\n    \"\"\"\n    Manages a secret in Google Cloud Platform's Secret Manager.\n\n    Attributes:\n        gcp_credentials: Credentials to use for authentication with GCP.\n        secret_name: Name of the secret to manage.\n        secret_version: Version number of the secret to use, or \"latest\".\n    \"\"\"\n\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/secret_manager/#prefect_gcp.secret_manager.GcpSecret\"  # noqa: E501\n\n    gcp_credentials: GcpCredentials\n    secret_name: str = Field(default=..., description=\"Name of the secret to manage.\")\n    secret_version: str = Field(\n        default=\"latest\", description=\"Version number of the secret to use.\"\n    )\n\n    @sync_compatible\n    async def read_secret(self) -&gt; bytes:\n        \"\"\"\n        Reads the secret data from the secret storage service.\n\n        Returns:\n            The secret data as bytes.\n        \"\"\"\n        client = self.gcp_credentials.get_secret_manager_client()\n        project = self.gcp_credentials.project\n        name = f\"projects/{project}/secrets/{self.secret_name}/versions/{self.secret_version}\"  # noqa\n        request = AccessSecretVersionRequest(name=name)\n\n        self.logger.debug(f\"Preparing to read secret data from {name!r}.\")\n        response = await run_sync_in_worker_thread(\n            client.access_secret_version, request=request\n        )\n        secret = response.payload.data\n        self.logger.info(f\"The secret {name!r} data was successfully read.\")\n        return secret\n\n    @sync_compatible\n    async def write_secret(self, secret_data: bytes) -&gt; str:\n        \"\"\"\n        Writes the secret data to the secret storage service; if it doesn't exist\n        it will be created.\n\n        Args:\n            secret_data: The secret to write.\n\n        Returns:\n            The path that the secret was written to.\n        \"\"\"\n        client = self.gcp_credentials.get_secret_manager_client()\n        project = self.gcp_credentials.project\n        parent = f\"projects/{project}/secrets/{self.secret_name}\"\n        payload = SecretPayload(data=secret_data)\n        add_request = AddSecretVersionRequest(parent=parent, payload=payload)\n\n        self.logger.debug(f\"Preparing to write secret data to {parent!r}.\")\n        try:\n            response = await run_sync_in_worker_thread(\n                client.add_secret_version, request=add_request\n            )\n        except NotFound:\n            self.logger.info(\n                f\"The secret {parent!r} does not exist yet, creating it now.\"\n            )\n            create_parent = f\"projects/{project}\"\n            secret_id = self.secret_name\n            secret = Secret(replication=Replication(automatic=Replication.Automatic()))\n            create_request = CreateSecretRequest(\n                parent=create_parent, secret_id=secret_id, secret=secret\n            )\n            await run_sync_in_worker_thread(\n                client.create_secret, request=create_request\n            )\n\n            self.logger.debug(f\"Preparing to write secret data to {parent!r} again.\")\n            response = await run_sync_in_worker_thread(\n                client.add_secret_version, request=add_request\n            )\n\n        self.logger.info(f\"The secret data was written successfully to {parent!r}.\")\n        return response.name\n\n    @sync_compatible\n    async def delete_secret(self) -&gt; str:\n        \"\"\"\n        Deletes the secret from the secret storage service.\n\n        Returns:\n            The path that the secret was deleted from.\n        \"\"\"\n        client = self.gcp_credentials.get_secret_manager_client()\n        project = self.gcp_credentials.project\n\n        name = f\"projects/{project}/secrets/{self.secret_name}\"\n        request = DeleteSecretRequest(name=name)\n\n        self.logger.debug(f\"Preparing to delete the secret {name!r}.\")\n        await run_sync_in_worker_thread(client.delete_secret, request=request)\n        self.logger.info(f\"The secret {name!r} was successfully deleted.\")\n        return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret-functions","title":"Functions","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.delete_secret","title":"<code>delete_secret</code>  <code>async</code>","text":"<p>Deletes the secret from the secret storage service.</p> <p>Returns:</p> Type Description <code>str</code> <p>The path that the secret was deleted from.</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@sync_compatible\nasync def delete_secret(self) -&gt; str:\n    \"\"\"\n    Deletes the secret from the secret storage service.\n\n    Returns:\n        The path that the secret was deleted from.\n    \"\"\"\n    client = self.gcp_credentials.get_secret_manager_client()\n    project = self.gcp_credentials.project\n\n    name = f\"projects/{project}/secrets/{self.secret_name}\"\n    request = DeleteSecretRequest(name=name)\n\n    self.logger.debug(f\"Preparing to delete the secret {name!r}.\")\n    await run_sync_in_worker_thread(client.delete_secret, request=request)\n    self.logger.info(f\"The secret {name!r} was successfully deleted.\")\n    return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.read_secret","title":"<code>read_secret</code>  <code>async</code>","text":"<p>Reads the secret data from the secret storage service.</p> <p>Returns:</p> Type Description <code>bytes</code> <p>The secret data as bytes.</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@sync_compatible\nasync def read_secret(self) -&gt; bytes:\n    \"\"\"\n    Reads the secret data from the secret storage service.\n\n    Returns:\n        The secret data as bytes.\n    \"\"\"\n    client = self.gcp_credentials.get_secret_manager_client()\n    project = self.gcp_credentials.project\n    name = f\"projects/{project}/secrets/{self.secret_name}/versions/{self.secret_version}\"  # noqa\n    request = AccessSecretVersionRequest(name=name)\n\n    self.logger.debug(f\"Preparing to read secret data from {name!r}.\")\n    response = await run_sync_in_worker_thread(\n        client.access_secret_version, request=request\n    )\n    secret = response.payload.data\n    self.logger.info(f\"The secret {name!r} data was successfully read.\")\n    return secret\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.GcpSecret.write_secret","title":"<code>write_secret</code>  <code>async</code>","text":"<p>Writes the secret data to the secret storage service; if it doesn't exist it will be created.</p> <p>Parameters:</p> Name Type Description Default <code>secret_data</code> <code>bytes</code> <p>The secret to write.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The path that the secret was written to.</p> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@sync_compatible\nasync def write_secret(self, secret_data: bytes) -&gt; str:\n    \"\"\"\n    Writes the secret data to the secret storage service; if it doesn't exist\n    it will be created.\n\n    Args:\n        secret_data: The secret to write.\n\n    Returns:\n        The path that the secret was written to.\n    \"\"\"\n    client = self.gcp_credentials.get_secret_manager_client()\n    project = self.gcp_credentials.project\n    parent = f\"projects/{project}/secrets/{self.secret_name}\"\n    payload = SecretPayload(data=secret_data)\n    add_request = AddSecretVersionRequest(parent=parent, payload=payload)\n\n    self.logger.debug(f\"Preparing to write secret data to {parent!r}.\")\n    try:\n        response = await run_sync_in_worker_thread(\n            client.add_secret_version, request=add_request\n        )\n    except NotFound:\n        self.logger.info(\n            f\"The secret {parent!r} does not exist yet, creating it now.\"\n        )\n        create_parent = f\"projects/{project}\"\n        secret_id = self.secret_name\n        secret = Secret(replication=Replication(automatic=Replication.Automatic()))\n        create_request = CreateSecretRequest(\n            parent=create_parent, secret_id=secret_id, secret=secret\n        )\n        await run_sync_in_worker_thread(\n            client.create_secret, request=create_request\n        )\n\n        self.logger.debug(f\"Preparing to write secret data to {parent!r} again.\")\n        response = await run_sync_in_worker_thread(\n            client.add_secret_version, request=add_request\n        )\n\n    self.logger.info(f\"The secret data was written successfully to {parent!r}.\")\n    return response.name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager-functions","title":"Functions","text":""},{"location":"secret_manager/#prefect_gcp.secret_manager.create_secret","title":"<code>create_secret</code>  <code>async</code>","text":"<p>Creates a secret in Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the created secret.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import create_secret\n\n@flow()\ndef example_cloud_storage_create_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_path = create_secret(\"secret_name\", gcp_credentials)\n    return secret_path\n\nexample_cloud_storage_create_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def create_secret(\n    secret_name: str,\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Creates a secret in Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the created secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import create_secret\n\n        @flow()\n        def example_cloud_storage_create_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_path = create_secret(\"secret_name\", gcp_credentials)\n            return secret_path\n\n        example_cloud_storage_create_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Creating the %s secret\", secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    parent = f\"projects/{project}\"\n    secret_settings = {\"replication\": {\"automatic\": {}}}\n\n    partial_create = partial(\n        client.create_secret,\n        parent=parent,\n        secret_id=secret_name,\n        secret=secret_settings,\n        timeout=timeout,\n    )\n    response = await to_thread.run_sync(partial_create)\n    return response.name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret","title":"<code>delete_secret</code>  <code>async</code>","text":"<p>Deletes the specified secret from Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to delete.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the deleted secret.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import delete_secret\n\n@flow()\ndef example_cloud_storage_delete_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_path = delete_secret(\"secret_name\", gcp_credentials)\n    return secret_path\n\nexample_cloud_storage_delete_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def delete_secret(\n    secret_name: str,\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Deletes the specified secret from Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to delete.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the deleted secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import delete_secret\n\n        @flow()\n        def example_cloud_storage_delete_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_path = delete_secret(\"secret_name\", gcp_credentials)\n            return secret_path\n\n        example_cloud_storage_delete_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Deleting %s secret\", secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    name = f\"projects/{project}/secrets/{secret_name}/\"\n    partial_delete = partial(client.delete_secret, name=name, timeout=timeout)\n    await to_thread.run_sync(partial_delete)\n    return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.delete_secret_version","title":"<code>delete_secret_version</code>  <code>async</code>","text":"<p>Deletes a version of a given secret from Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>version_id</code> <code>int</code> <p>Version number of the secret to use; \"latest\" can NOT be used.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the deleted secret version.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import delete_secret_version\n\n@flow()\ndef example_cloud_storage_delete_secret_version_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials)\n    return secret_value\n\nexample_cloud_storage_delete_secret_version_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def delete_secret_version(\n    secret_name: str,\n    version_id: int,\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Deletes a version of a given secret from Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        version_id: Version number of the secret to use; \"latest\" can NOT be used.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the deleted secret version.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import delete_secret_version\n\n        @flow()\n        def example_cloud_storage_delete_secret_version_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_value = delete_secret_version(\"secret_name\", 1, gcp_credentials)\n            return secret_value\n\n        example_cloud_storage_delete_secret_version_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Reading %s version of %s secret\", version_id, secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    if version_id == \"latest\":\n        raise ValueError(\"The version_id cannot be 'latest'\")\n\n    name = f\"projects/{project}/secrets/{secret_name}/versions/{version_id}\"\n    partial_destroy = partial(client.destroy_secret_version, name=name, timeout=timeout)\n    await to_thread.run_sync(partial_destroy)\n    return name\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.read_secret","title":"<code>read_secret</code>  <code>async</code>","text":"<p>Reads the value of a given secret from Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>Contents of the specified secret.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import read_secret\n\n@flow()\ndef example_cloud_storage_read_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1)\n    return secret_value\n\nexample_cloud_storage_read_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def read_secret(\n    secret_name: str,\n    gcp_credentials: \"GcpCredentials\",\n    version_id: Union[str, int] = \"latest\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Reads the value of a given secret from Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        Contents of the specified secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import read_secret\n\n        @flow()\n        def example_cloud_storage_read_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_value = read_secret(\"secret_name\", gcp_credentials, version_id=1)\n            return secret_value\n\n        example_cloud_storage_read_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Reading %s version of %s secret\", version_id, secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    name = f\"projects/{project}/secrets/{secret_name}/versions/{version_id}\"\n    partial_access = partial(client.access_secret_version, name=name, timeout=timeout)\n    response = await to_thread.run_sync(partial_access)\n    secret = response.payload.data.decode(\"UTF-8\")\n    return secret\n</code></pre>"},{"location":"secret_manager/#prefect_gcp.secret_manager.update_secret","title":"<code>update_secret</code>  <code>async</code>","text":"<p>Updates a secret in Google Cloud Platform's Secret Manager.</p> <p>Parameters:</p> Name Type Description Default <code>secret_name</code> <code>str</code> <p>Name of the secret to retrieve.</p> required <code>secret_value</code> <code>Union[str, bytes]</code> <p>Desired value of the secret. Can be either <code>str</code> or <code>bytes</code>.</p> required <code>gcp_credentials</code> <code>GcpCredentials</code> <p>Credentials to use for authentication with GCP.</p> required <code>timeout</code> <code>float</code> <p>The number of seconds the transport should wait for the server response.</p> <code>60</code> <code>project</code> <code>Optional[str]</code> <p>Name of the project to use; overrides the gcp_credentials project if provided.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>The path of the updated secret.</p> Example <pre><code>from prefect import flow\nfrom prefect_gcp import GcpCredentials\nfrom prefect_gcp.secret_manager import update_secret\n\n@flow()\ndef example_cloud_storage_update_secret_flow():\n    gcp_credentials = GcpCredentials(project=\"project\")\n    secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials)\n    return secret_path\n\nexample_cloud_storage_update_secret_flow()\n</code></pre> Source code in <code>prefect_gcp/secret_manager.py</code> <pre><code>@task\nasync def update_secret(\n    secret_name: str,\n    secret_value: Union[str, bytes],\n    gcp_credentials: \"GcpCredentials\",\n    timeout: float = 60,\n    project: Optional[str] = None,\n) -&gt; str:\n    \"\"\"\n    Updates a secret in Google Cloud Platform's Secret Manager.\n\n    Args:\n        secret_name: Name of the secret to retrieve.\n        secret_value: Desired value of the secret. Can be either `str` or `bytes`.\n        gcp_credentials: Credentials to use for authentication with GCP.\n        timeout: The number of seconds the transport should wait\n            for the server response.\n        project: Name of the project to use; overrides the\n            gcp_credentials project if provided.\n\n    Returns:\n        The path of the updated secret.\n\n    Example:\n        ```python\n        from prefect import flow\n        from prefect_gcp import GcpCredentials\n        from prefect_gcp.secret_manager import update_secret\n\n        @flow()\n        def example_cloud_storage_update_secret_flow():\n            gcp_credentials = GcpCredentials(project=\"project\")\n            secret_path = update_secret(\"secret_name\", \"secret_value\", gcp_credentials)\n            return secret_path\n\n        example_cloud_storage_update_secret_flow()\n        ```\n    \"\"\"\n    logger = get_run_logger()\n    logger.info(\"Updating the %s secret\", secret_name)\n\n    client = gcp_credentials.get_secret_manager_client()\n    project = project or gcp_credentials.project\n\n    parent = f\"projects/{project}/secrets/{secret_name}\"\n    if isinstance(secret_value, str):\n        secret_value = secret_value.encode(\"UTF-8\")\n    partial_add = partial(\n        client.add_secret_version,\n        parent=parent,\n        payload={\"data\": secret_value},\n        timeout=timeout,\n    )\n    response = await to_thread.run_sync(partial_add)\n    return response.name\n</code></pre>"},{"location":"vertex_worker/","title":"Vertex AI","text":""},{"location":"vertex_worker/#prefect_gcp.workers.vertex","title":"<code>prefect_gcp.workers.vertex</code>","text":"<p>Module containing the custom worker used for executing flow runs as Vertex AI Custom Jobs.</p> <p>Get started by creating a Cloud Run work pool:</p> <pre><code>prefect work-pool create 'my-vertex-pool' --type vertex-ai\n</code></pre> <p>Then start a Cloud Run worker with the following command:</p> <pre><code>prefect worker start --pool 'my-vertex-pool'\n</code></pre>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex--configuration","title":"Configuration","text":"<p>Read more about configuring work pools here.</p>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex-classes","title":"Classes","text":""},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorker","title":"<code>VertexAIWorker</code>","text":"<p>             Bases: <code>BaseWorker</code></p> <p>Prefect worker that executes flow runs within Vertex AI Jobs.</p> Source code in <code>prefect_gcp/workers/vertex.py</code> <pre><code>class VertexAIWorker(BaseWorker):\n    \"\"\"Prefect worker that executes flow runs within Vertex AI Jobs.\"\"\"\n\n    type = \"vertex-ai\"\n    job_configuration = VertexAIWorkerJobConfiguration\n    job_configuration_variables = VertexAIWorkerVariables\n    _description = (\n        \"Execute flow runs within containers on Google Vertex AI. Requires \"\n        \"a Google Cloud Platform account.\"\n    )\n    _display_name = \"Google Vertex AI\"\n    _documentation_url = \"https://prefecthq.github.io/prefect-gcp/vertex_worker/\"\n    _logo_url = \"https://cdn.sanity.io/images/3ugk85nk/production/10424e311932e31c477ac2b9ef3d53cefbaad708-250x250.png\"  # noqa\n\n    async def run(\n        self,\n        flow_run: \"FlowRun\",\n        configuration: VertexAIWorkerJobConfiguration,\n        task_status: Optional[anyio.abc.TaskStatus] = None,\n    ) -&gt; VertexAIWorkerResult:\n        \"\"\"\n        Executes a flow run within a Vertex AI Job and waits for the flow run\n        to complete.\n\n        Args:\n            flow_run: The flow run to execute\n            configuration: The configuration to use when executing the flow run.\n            task_status: The task status object for the current flow run. If provided,\n                the task will be marked as started.\n\n        Returns:\n            VertexAIWorkerResult: A result object containing information about the\n                final state of the flow run\n        \"\"\"\n        logger = self.get_flow_run_logger(flow_run)\n\n        client_options = ClientOptions(\n            api_endpoint=f\"{configuration.region}-aiplatform.googleapis.com\"\n        )\n\n        job_name = configuration.job_name\n\n        job_spec = self._build_job_spec(configuration)\n        with configuration.credentials.get_job_service_client(\n            client_options=client_options\n        ) as job_service_client:\n            job_run = await self._create_and_begin_job(\n                job_name, job_spec, job_service_client, configuration, logger\n            )\n\n            if task_status:\n                task_status.started(job_run.name)\n\n            final_job_run = await self._watch_job_run(\n                job_name=job_name,\n                full_job_name=job_run.name,\n                job_service_client=job_service_client,\n                current_state=job_run.state,\n                until_states=(\n                    JobState.JOB_STATE_SUCCEEDED,\n                    JobState.JOB_STATE_FAILED,\n                    JobState.JOB_STATE_CANCELLED,\n                    JobState.JOB_STATE_EXPIRED,\n                ),\n                configuration=configuration,\n                logger=logger,\n                timeout=int(\n                    datetime.timedelta(\n                        hours=configuration.job_spec[\"maximum_run_time_hours\"]\n                    ).total_seconds()\n                ),\n            )\n\n        error_msg = final_job_run.error.message\n\n        # Vertex will include an error message upon valid\n        # flow cancellations, so we'll avoid raising an error in that case\n        if error_msg and \"CANCELED\" not in error_msg:\n            raise RuntimeError(error_msg)\n\n        status_code = 0 if final_job_run.state == JobState.JOB_STATE_SUCCEEDED else 1\n\n        return VertexAIWorkerResult(\n            identifier=final_job_run.display_name, status_code=status_code\n        )\n\n    def _build_job_spec(\n        self, configuration: VertexAIWorkerJobConfiguration\n    ) -&gt; \"CustomJobSpec\":\n        \"\"\"\n        Builds a job spec by gathering details.\n        \"\"\"\n        # here, we extract the `worker_pool_specs` out of the job_spec\n        worker_pool_specs = [\n            WorkerPoolSpec(\n                container_spec=ContainerSpec(**spec[\"container_spec\"]),\n                machine_spec=MachineSpec(**spec[\"machine_spec\"]),\n                replica_count=spec[\"replica_count\"],\n                disk_spec=DiskSpec(**spec[\"disk_spec\"]),\n            )\n            for spec in configuration.job_spec.pop(\"worker_pool_specs\", [])\n        ]\n\n        timeout = Duration().FromTimedelta(\n            td=datetime.timedelta(\n                hours=configuration.job_spec[\"maximum_run_time_hours\"]\n            )\n        )\n        scheduling = Scheduling(timeout=timeout)\n\n        # construct the final job spec that we will provide to Vertex AI\n        job_spec = CustomJobSpec(\n            worker_pool_specs=worker_pool_specs,\n            scheduling=scheduling,\n            ignore_unknown_fields=True,\n            **configuration.job_spec,\n        )\n        return job_spec\n\n    async def _create_and_begin_job(\n        self,\n        job_name: str,\n        job_spec: \"CustomJobSpec\",\n        job_service_client: \"JobServiceClient\",\n        configuration: VertexAIWorkerJobConfiguration,\n        logger: PrefectLogAdapter,\n    ) -&gt; \"CustomJob\":\n        \"\"\"\n        Builds a custom job and begins running it.\n        \"\"\"\n        # create custom job\n        custom_job = CustomJob(\n            display_name=job_name,\n            job_spec=job_spec,\n            labels=self._get_compatible_labels(configuration=configuration),\n        )\n\n        # run job\n        logger.info(f\"Job {job_name!r} starting to run \")\n\n        project = configuration.project\n        resource_name = f\"projects/{project}/locations/{configuration.region}\"\n\n        retry_policy = retry(\n            stop=stop_after_attempt(3), wait=wait_fixed(1) + wait_random(0, 3)\n        )\n\n        custom_job_run = await run_sync_in_worker_thread(\n            retry_policy(job_service_client.create_custom_job),\n            parent=resource_name,\n            custom_job=custom_job,\n        )\n\n        logger.info(\n            f\"Job {job_name!r} has successfully started; \"\n            f\"the full job name is {custom_job_run.name!r}\"\n        )\n\n        return custom_job_run\n\n    async def _watch_job_run(\n        self,\n        job_name: str,\n        full_job_name: str,  # different from job_name\n        job_service_client: \"JobServiceClient\",\n        current_state: \"JobState\",\n        until_states: Tuple[\"JobState\"],\n        configuration: VertexAIWorkerJobConfiguration,\n        logger: PrefectLogAdapter,\n        timeout: int = None,\n    ) -&gt; \"CustomJob\":\n        \"\"\"\n        Polls job run to see if status changed.\n        \"\"\"\n        state = JobState.JOB_STATE_UNSPECIFIED\n        last_state = current_state\n        t0 = time.time()\n\n        while state not in until_states:\n            job_run = await run_sync_in_worker_thread(\n                job_service_client.get_custom_job,\n                name=full_job_name,\n            )\n            state = job_run.state\n            if state != last_state:\n                state_label = (\n                    state.name.replace(\"_\", \" \")\n                    .lower()\n                    .replace(\"state\", \"state is now:\")\n                )\n                # results in \"New job state is now: succeeded\"\n                logger.info(f\"{job_name} has new {state_label}\")\n                last_state = state\n            else:\n                # Intermittently, the job will not be described. We want to respect the\n                # watch timeout though.\n                logger.debug(f\"Job {job_name} not found.\")\n\n            elapsed_time = time.time() - t0\n            if timeout is not None and elapsed_time &gt; timeout:\n                raise RuntimeError(\n                    f\"Timed out after {elapsed_time}s while watching job for states \"\n                    \"{until_states!r}\"\n                )\n            time.sleep(configuration.job_watch_poll_interval)\n\n        return job_run\n\n    def _get_compatible_labels(\n        self, configuration: VertexAIWorkerJobConfiguration\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Ensures labels are compatible with GCP label requirements.\n        https://cloud.google.com/resource-manager/docs/creating-managing-labels\n\n        Ex: the Prefect provided key of prefect.io/flow-name -&gt; prefect-io_flow-name\n        \"\"\"\n        compatible_labels = {}\n        for key, val in configuration.labels.items():\n            new_key = slugify(\n                key,\n                lowercase=True,\n                replacements=[(\"/\", \"_\"), (\".\", \"-\")],\n                max_length=63,\n                regex_pattern=_DISALLOWED_GCP_LABEL_CHARACTERS,\n            )\n            compatible_labels[new_key] = slugify(\n                val,\n                lowercase=True,\n                replacements=[(\"/\", \"_\"), (\".\", \"-\")],\n                max_length=63,\n                regex_pattern=_DISALLOWED_GCP_LABEL_CHARACTERS,\n            )\n        return compatible_labels\n\n    async def kill_infrastructure(\n        self,\n        infrastructure_pid: str,\n        configuration: VertexAIWorkerJobConfiguration,\n        grace_seconds: int = 30,\n    ):\n        \"\"\"\n        Stops a job running in Vertex AI upon flow cancellation,\n        based on the provided infrastructure PID + run configuration.\n        \"\"\"\n        if grace_seconds != 30:\n            self._logger.warning(\n                f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n                \"support dynamic grace period configuration. See here for more info: \"\n                \"https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs/cancel\"  # noqa\n            )\n\n        client_options = ClientOptions(\n            api_endpoint=f\"{configuration.region}-aiplatform.googleapis.com\"\n        )\n        with configuration.credentials.get_job_service_client(\n            client_options=client_options\n        ) as job_service_client:\n            await run_sync_in_worker_thread(\n                self._stop_job,\n                client=job_service_client,\n                vertex_job_name=infrastructure_pid,\n            )\n\n    def _stop_job(self, client: \"JobServiceClient\", vertex_job_name: str):\n        \"\"\"\n        Calls the `cancel_custom_job` method on the Vertex AI Job Service Client.\n        \"\"\"\n        cancel_custom_job_request = CancelCustomJobRequest(name=vertex_job_name)\n        try:\n            client.cancel_custom_job(\n                request=cancel_custom_job_request,\n            )\n        except Exception as exc:\n            if \"does not exist\" in str(exc):\n                raise InfrastructureNotFound(\n                    f\"Cannot stop Vertex AI job; the job name {vertex_job_name!r} \"\n                    \"could not be found.\"\n                ) from exc\n            raise\n</code></pre>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorker-functions","title":"Functions","text":""},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorker.kill_infrastructure","title":"<code>kill_infrastructure</code>  <code>async</code>","text":"<p>Stops a job running in Vertex AI upon flow cancellation, based on the provided infrastructure PID + run configuration.</p> Source code in <code>prefect_gcp/workers/vertex.py</code> <pre><code>async def kill_infrastructure(\n    self,\n    infrastructure_pid: str,\n    configuration: VertexAIWorkerJobConfiguration,\n    grace_seconds: int = 30,\n):\n    \"\"\"\n    Stops a job running in Vertex AI upon flow cancellation,\n    based on the provided infrastructure PID + run configuration.\n    \"\"\"\n    if grace_seconds != 30:\n        self._logger.warning(\n            f\"Kill grace period of {grace_seconds}s requested, but GCP does not \"\n            \"support dynamic grace period configuration. See here for more info: \"\n            \"https://cloud.google.com/vertex-ai/docs/reference/rest/v1/projects.locations.customJobs/cancel\"  # noqa\n        )\n\n    client_options = ClientOptions(\n        api_endpoint=f\"{configuration.region}-aiplatform.googleapis.com\"\n    )\n    with configuration.credentials.get_job_service_client(\n        client_options=client_options\n    ) as job_service_client:\n        await run_sync_in_worker_thread(\n            self._stop_job,\n            client=job_service_client,\n            vertex_job_name=infrastructure_pid,\n        )\n</code></pre>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorker.run","title":"<code>run</code>  <code>async</code>","text":"<p>Executes a flow run within a Vertex AI Job and waits for the flow run to complete.</p> <p>Parameters:</p> Name Type Description Default <code>flow_run</code> <code>FlowRun</code> <p>The flow run to execute</p> required <code>configuration</code> <code>VertexAIWorkerJobConfiguration</code> <p>The configuration to use when executing the flow run.</p> required <code>task_status</code> <code>Optional[TaskStatus]</code> <p>The task status object for the current flow run. If provided, the task will be marked as started.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>VertexAIWorkerResult</code> <code>VertexAIWorkerResult</code> <p>A result object containing information about the final state of the flow run</p> Source code in <code>prefect_gcp/workers/vertex.py</code> <pre><code>async def run(\n    self,\n    flow_run: \"FlowRun\",\n    configuration: VertexAIWorkerJobConfiguration,\n    task_status: Optional[anyio.abc.TaskStatus] = None,\n) -&gt; VertexAIWorkerResult:\n    \"\"\"\n    Executes a flow run within a Vertex AI Job and waits for the flow run\n    to complete.\n\n    Args:\n        flow_run: The flow run to execute\n        configuration: The configuration to use when executing the flow run.\n        task_status: The task status object for the current flow run. If provided,\n            the task will be marked as started.\n\n    Returns:\n        VertexAIWorkerResult: A result object containing information about the\n            final state of the flow run\n    \"\"\"\n    logger = self.get_flow_run_logger(flow_run)\n\n    client_options = ClientOptions(\n        api_endpoint=f\"{configuration.region}-aiplatform.googleapis.com\"\n    )\n\n    job_name = configuration.job_name\n\n    job_spec = self._build_job_spec(configuration)\n    with configuration.credentials.get_job_service_client(\n        client_options=client_options\n    ) as job_service_client:\n        job_run = await self._create_and_begin_job(\n            job_name, job_spec, job_service_client, configuration, logger\n        )\n\n        if task_status:\n            task_status.started(job_run.name)\n\n        final_job_run = await self._watch_job_run(\n            job_name=job_name,\n            full_job_name=job_run.name,\n            job_service_client=job_service_client,\n            current_state=job_run.state,\n            until_states=(\n                JobState.JOB_STATE_SUCCEEDED,\n                JobState.JOB_STATE_FAILED,\n                JobState.JOB_STATE_CANCELLED,\n                JobState.JOB_STATE_EXPIRED,\n            ),\n            configuration=configuration,\n            logger=logger,\n            timeout=int(\n                datetime.timedelta(\n                    hours=configuration.job_spec[\"maximum_run_time_hours\"]\n                ).total_seconds()\n            ),\n        )\n\n    error_msg = final_job_run.error.message\n\n    # Vertex will include an error message upon valid\n    # flow cancellations, so we'll avoid raising an error in that case\n    if error_msg and \"CANCELED\" not in error_msg:\n        raise RuntimeError(error_msg)\n\n    status_code = 0 if final_job_run.state == JobState.JOB_STATE_SUCCEEDED else 1\n\n    return VertexAIWorkerResult(\n        identifier=final_job_run.display_name, status_code=status_code\n    )\n</code></pre>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorkerJobConfiguration","title":"<code>VertexAIWorkerJobConfiguration</code>","text":"<p>             Bases: <code>BaseJobConfiguration</code></p> <p>Configuration class used by the Vertex AI Worker to create a Job.</p> <p>An instance of this class is passed to the Vertex AI Worker's <code>run</code> method for each flow run. It contains all information necessary to execute the flow run as a Vertex AI Job.</p> <p>Attributes:</p> Name Type Description <code>region</code> <code>str</code> <p>The region where the Vertex AI Job resides.</p> <code>credentials</code> <code>Optional[GcpCredentials]</code> <p>The GCP Credentials used to connect to Vertex AI.</p> <code>job_spec</code> <code>Dict[str, Any]</code> <p>The Vertex AI Job spec used to create the Job.</p> <code>job_watch_poll_interval</code> <code>float</code> <p>The interval between GCP API calls to check Job state.</p> Source code in <code>prefect_gcp/workers/vertex.py</code> <pre><code>class VertexAIWorkerJobConfiguration(BaseJobConfiguration):\n    \"\"\"\n    Configuration class used by the Vertex AI Worker to create a Job.\n\n    An instance of this class is passed to the Vertex AI Worker's `run` method\n    for each flow run. It contains all information necessary to execute\n    the flow run as a Vertex AI Job.\n\n    Attributes:\n        region: The region where the Vertex AI Job resides.\n        credentials: The GCP Credentials used to connect to Vertex AI.\n        job_spec: The Vertex AI Job spec used to create the Job.\n        job_watch_poll_interval: The interval between GCP API calls to check Job state.\n    \"\"\"\n\n    region: str = Field(\n        description=\"The region where the Vertex AI Job resides.\",\n        example=\"us-central1\",\n    )\n    credentials: Optional[GcpCredentials] = Field(\n        title=\"GCP Credentials\",\n        default_factory=GcpCredentials,\n        description=\"The GCP Credentials used to initiate the \"\n        \"Vertex AI Job. If not provided credentials will be \"\n        \"inferred from the local environment.\",\n    )\n\n    job_spec: Dict[str, Any] = Field(\n        template={\n            \"service_account_name\": \"{{ service_account_name }}\",\n            \"network\": \"{{ network }}\",\n            \"reserved_ip_ranges\": \"{{ reserved_ip_ranges }}\",\n            \"maximum_run_time_hours\": \"{{ maximum_run_time_hours }}\",\n            \"worker_pool_specs\": [\n                {\n                    \"replica_count\": 1,\n                    \"container_spec\": {\n                        \"image_uri\": \"{{ image }}\",\n                        \"command\": \"{{ command }}\",\n                        \"args\": [],\n                    },\n                    \"machine_spec\": {\n                        \"machine_type\": \"{{ machine_type }}\",\n                        \"accelerator_type\": \"{{ accelerator_type }}\",\n                        \"accelerator_count\": \"{{ accelerator_count }}\",\n                    },\n                    \"disk_spec\": {\n                        \"boot_disk_type\": \"{{ boot_disk_type }}\",\n                        \"boot_disk_size_gb\": \"{{ boot_disk_size_gb }}\",\n                    },\n                }\n            ],\n        }\n    )\n    job_watch_poll_interval: float = Field(\n        default=5.0,\n        title=\"Poll Interval (Seconds)\",\n        description=(\n            \"The amount of time to wait between GCP API calls while monitoring the \"\n            \"state of a Vertex AI Job.\"\n        ),\n    )\n\n    @property\n    def project(self) -&gt; str:\n        \"\"\"property for accessing the project from the credentials.\"\"\"\n        return self.credentials.project\n\n    @property\n    def job_name(self) -&gt; str:\n        \"\"\"\n        The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference:\n        https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name\n        \"\"\"  # noqa\n        unique_suffix = uuid4().hex\n        job_name = f\"{self.name}-{unique_suffix}\"\n        return job_name\n\n    def prepare_for_flow_run(\n        self,\n        flow_run: \"FlowRun\",\n        deployment: Optional[\"DeploymentResponse\"] = None,\n        flow: Optional[\"Flow\"] = None,\n    ):\n        super().prepare_for_flow_run(flow_run, deployment, flow)\n\n        self._inject_formatted_env_vars()\n        self._inject_formatted_command()\n        self._ensure_existence_of_service_account()\n\n    def _inject_formatted_env_vars(self):\n        \"\"\"Inject environment variables in the Vertex job_spec configuration,\n        in the correct format, which is sourced from the BaseJobConfiguration.\n        This method is invoked by `prepare_for_flow_run()`.\"\"\"\n        worker_pool_specs = self.job_spec[\"worker_pool_specs\"]\n        formatted_env_vars = [\n            {\"name\": key, \"value\": value} for key, value in self.env.items()\n        ]\n        worker_pool_specs[0][\"container_spec\"][\"env\"] = formatted_env_vars\n\n    def _inject_formatted_command(self):\n        \"\"\"Inject shell commands in the Vertex job_spec configuration,\n        in the correct format, which is sourced from the BaseJobConfiguration.\n        Here, we'll ensure that the default string format\n        is converted to a list of strings.\"\"\"\n        worker_pool_specs = self.job_spec[\"worker_pool_specs\"]\n\n        existing_command = worker_pool_specs[0][\"container_spec\"].get(\"command\")\n        if existing_command is None:\n            worker_pool_specs[0][\"container_spec\"][\"command\"] = [\n                \"python\",\n                \"-m\",\n                \"prefect.engine\",\n            ]\n        elif isinstance(existing_command, str):\n            worker_pool_specs[0][\"container_spec\"][\"command\"] = shlex.split(\n                existing_command\n            )\n\n    def _ensure_existence_of_service_account(self):\n        \"\"\"Verify that a service account was provided, either in the credentials\n        or as a standalone service account name override.\"\"\"\n\n        provided_service_account_name = self.job_spec.get(\"service_account_name\")\n        credential_service_account = self.credentials._service_account_email\n\n        service_account_to_use = (\n            provided_service_account_name or credential_service_account\n        )\n\n        if service_account_to_use is None:\n            raise ValueError(\n                \"A service account is required for the Vertex job. \"\n                \"A service account could not be detected in the attached credentials \"\n                \"or in the service_account_name input. \"\n                \"Please pass in valid GCP credentials or a valid service_account_name\"\n            )\n\n        self.job_spec[\"service_account_name\"] = service_account_to_use\n\n    @validator(\"job_spec\")\n    def _ensure_job_spec_includes_required_attributes(cls, value: Dict[str, Any]):\n        \"\"\"\n        Ensures that the job spec includes all required components.\n        \"\"\"\n        patch = JsonPatch.from_diff(value, _get_base_job_spec())\n        missing_paths = sorted([op[\"path\"] for op in patch if op[\"op\"] == \"add\"])\n        if missing_paths:\n            raise ValueError(\n                \"Job is missing required attributes at the following paths: \"\n                f\"{', '.join(missing_paths)}\"\n            )\n        return value\n</code></pre>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorkerJobConfiguration-attributes","title":"Attributes","text":""},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorkerJobConfiguration.job_name","title":"<code>job_name: str</code>  <code>property</code>","text":"<p>The name can be up to 128 characters long and can be consist of any UTF-8 characters. Reference: https://cloud.google.com/python/docs/reference/aiplatform/latest/google.cloud.aiplatform.CustomJob#google_cloud_aiplatform_CustomJob_display_name</p>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorkerJobConfiguration.project","title":"<code>project: str</code>  <code>property</code>","text":"<p>property for accessing the project from the credentials.</p>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorkerResult","title":"<code>VertexAIWorkerResult</code>","text":"<p>             Bases: <code>BaseWorkerResult</code></p> <p>Contains information about the final state of a completed process</p> Source code in <code>prefect_gcp/workers/vertex.py</code> <pre><code>class VertexAIWorkerResult(BaseWorkerResult):\n    \"\"\"Contains information about the final state of a completed process\"\"\"\n</code></pre>"},{"location":"vertex_worker/#prefect_gcp.workers.vertex.VertexAIWorkerVariables","title":"<code>VertexAIWorkerVariables</code>","text":"<p>             Bases: <code>BaseVariables</code></p> <p>Default variables for the Vertex AI worker.</p> <p>The schema for this class is used to populate the <code>variables</code> section of the default base job template.</p> Source code in <code>prefect_gcp/workers/vertex.py</code> <pre><code>class VertexAIWorkerVariables(BaseVariables):\n    \"\"\"\n    Default variables for the Vertex AI worker.\n\n    The schema for this class is used to populate the `variables` section of the default\n    base job template.\n    \"\"\"\n\n    region: str = Field(\n        description=\"The region where the Vertex AI Job resides.\",\n        example=\"us-central1\",\n    )\n    image: str = Field(\n        title=\"Image Name\",\n        description=(\n            \"The URI of a container image in the Container or Artifact Registry, \"\n            \"used to run your Vertex AI Job. Note that Vertex AI will need access\"\n            \"to the project and region where the container image is stored. See \"\n            \"https://cloud.google.com/vertex-ai/docs/training/create-custom-container\"\n        ),\n        example=\"gcr.io/your-project/your-repo:latest\",\n    )\n    credentials: Optional[GcpCredentials] = Field(\n        title=\"GCP Credentials\",\n        default_factory=GcpCredentials,\n        description=\"The GCP Credentials used to initiate the \"\n        \"Vertex AI Job. If not provided credentials will be \"\n        \"inferred from the local environment.\",\n    )\n    machine_type: str = Field(\n        title=\"Machine Type\",\n        description=(\n            \"The machine type to use for the run, which controls \"\n            \"the available CPU and memory. \"\n            \"See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec\"\n        ),\n        default=\"n1-standard-4\",\n    )\n    accelerator_type: Optional[str] = Field(\n        title=\"Accelerator Type\",\n        description=(\n            \"The type of accelerator to attach to the machine. \"\n            \"See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec\"\n        ),\n        example=\"NVIDIA_TESLA_K80\",\n        default=None,\n    )\n    accelerator_count: Optional[int] = Field(\n        title=\"Accelerator Count\",\n        description=(\n            \"The number of accelerators to attach to the machine. \"\n            \"See https://cloud.google.com/vertex-ai/docs/reference/rest/v1/MachineSpec\"\n        ),\n        example=1,\n        default=None,\n    )\n    boot_disk_type: str = Field(\n        title=\"Boot Disk Type\",\n        description=\"The type of boot disk to attach to the machine.\",\n        default=\"pd-ssd\",\n    )\n    boot_disk_size_gb: int = Field(\n        title=\"Boot Disk Size (GB)\",\n        description=\"The size of the boot disk to attach to the machine, in gigabytes.\",\n        default=100,\n    )\n    maximum_run_time_hours: int = Field(\n        default=1,\n        title=\"Maximum Run Time (Hours)\",\n        description=\"The maximum job running time, in hours\",\n    )\n    network: Optional[str] = Field(\n        default=None,\n        title=\"Network\",\n        description=\"The full name of the Compute Engine network\"\n        \"to which the Job should be peered. Private services access must \"\n        \"already be configured for the network. If left unspecified, the job \"\n        \"is not peered with any network. \"\n        \"For example: projects/12345/global/networks/myVPC\",\n    )\n    reserved_ip_ranges: Optional[List[str]] = Field(\n        default=None,\n        title=\"Reserved IP Ranges\",\n        description=\"A list of names for the reserved ip ranges under the VPC \"\n        \"network that can be used for this job. If set, we will deploy the job \"\n        \"within the provided ip ranges. Otherwise, the job will be deployed to \"\n        \"any ip ranges under the provided VPC network.\",\n    )\n    service_account_name: Optional[str] = Field(\n        default=None,\n        title=\"Service Account Name\",\n        description=(\n            \"Specifies the service account to use \"\n            \"as the run-as account in Vertex AI. The worker submitting jobs must have \"\n            \"act-as permission on this run-as account. If unspecified, the AI \"\n            \"Platform Custom Code Service Agent for the CustomJob's project is \"\n            \"used. Takes precedence over the service account found in GCP credentials, \"\n            \"and required if a service account cannot be detected in GCP credentials.\"\n        ),\n    )\n    job_watch_poll_interval: float = Field(\n        default=5.0,\n        title=\"Poll Interval (Seconds)\",\n        description=(\n            \"The amount of time to wait between GCP API calls while monitoring the \"\n            \"state of a Vertex AI Job.\"\n        ),\n    )\n</code></pre>"},{"location":"deployments/steps/","title":"Deployment Steps","text":""},{"location":"deployments/steps/#prefect_gcp.deployments.steps","title":"<code>prefect_gcp.deployments.steps</code>","text":"<p>Prefect deployment steps for code storage in and retrieval from Google Cloud Storage.</p>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps-classes","title":"Classes","text":""},{"location":"deployments/steps/#prefect_gcp.deployments.steps.PullFromGcsOutput","title":"<code>PullFromGcsOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>The output of the <code>pull_from_gcs</code> step.</p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>class PullFromGcsOutput(TypedDict):\n    \"\"\"\n    The output of the `pull_from_gcs` step.\n    \"\"\"\n\n    bucket: str\n    folder: str\n    directory: str\n</code></pre>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps.PullProjectFromGcsOutput","title":"<code>PullProjectFromGcsOutput</code>","text":"<p>             Bases: <code>PullFromGcsOutput</code></p> <p>Deprecated. Use <code>PullFromGcsOutput</code> instead.</p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>@deprecated_callable(start_date=\"Jun 2023\", help=\"Use `PullFromGcsOutput` instead.\")\nclass PullProjectFromGcsOutput(PullFromGcsOutput):\n    \"\"\"Deprecated. Use `PullFromGcsOutput` instead.\"\"\"\n</code></pre>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps.PushProjectToGcsOutput","title":"<code>PushProjectToGcsOutput</code>","text":"<p>             Bases: <code>PushToGcsOutput</code></p> <p>Deprecated. Use <code>PushToGcsOutput</code> instead.</p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>@deprecated_callable(start_date=\"Jun 2023\", help=\"Use `PushToGcsOutput` instead.\")\nclass PushProjectToGcsOutput(PushToGcsOutput):\n    \"\"\"Deprecated. Use `PushToGcsOutput` instead.\"\"\"\n</code></pre>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps.PushToGcsOutput","title":"<code>PushToGcsOutput</code>","text":"<p>             Bases: <code>TypedDict</code></p> <p>The output of the <code>push_to_gcs</code> step.</p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>class PushToGcsOutput(TypedDict):\n    \"\"\"\n    The output of the `push_to_gcs` step.\n    \"\"\"\n\n    bucket: str\n    folder: str\n</code></pre>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps-functions","title":"Functions","text":""},{"location":"deployments/steps/#prefect_gcp.deployments.steps.pull_from_gcs","title":"<code>pull_from_gcs</code>","text":"<p>Pulls the contents of a project from an GCS bucket to the current working directory.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The name of the GCS bucket where files are stored.</p> required <code>folder</code> <code>str</code> <p>The folder in the GCS bucket where files are stored.</p> required <code>project</code> <code>Optional[str]</code> <p>The GCP project the bucket belongs to. If not provided, the project will be inferred from the credentials or the local environment.</p> <code>None</code> <code>credentials</code> <code>Optional[Dict]</code> <p>A dictionary containing the service account information and project used for authentication. If not provided, the application default credentials will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>PullProjectFromGcsOutput</code> <p>A dictionary containing the bucket, folder, and local directory where files were downloaded.</p> <p>Examples:</p> <p>Pull from GCS using the default environment credentials: <pre><code>build:\n    - prefect_gcp.deployments.steps.pull_from_gcs:\n        requires: prefect-gcp\n        bucket: my-bucket\n        folder: my-folder\n</code></pre></p> <p>Pull from GCS using credentials stored in a block: <pre><code>build:\n    - prefect_gcp.deployments.steps.pull_from_gcs:\n        requires: prefect-gcp\n        bucket: my-bucket\n        folder: my-folder\n        credentials: \"{{ prefect.blocks.gcp-credentials.dev-credentials }}\"\n</code></pre></p> <p>Pull from to an GCS bucket using credentials stored in a service account file: <pre><code>build:\n    - prefect_gcp.deployments.steps.pull_from_gcs:\n        requires: prefect-gcp\n        bucket: my-bucket\n        folder: my-folder\n        credentials:\n            project: my-project\n            service_account_file: /path/to/service_account.json\n</code></pre></p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>def pull_from_gcs(\n    bucket: str,\n    folder: str,\n    project: Optional[str] = None,\n    credentials: Optional[Dict] = None,\n) -&gt; PullProjectFromGcsOutput:\n    \"\"\"\n    Pulls the contents of a project from an GCS bucket to the current working directory.\n\n    Args:\n        bucket: The name of the GCS bucket where files are stored.\n        folder: The folder in the GCS bucket where files are stored.\n        project: The GCP project the bucket belongs to. If not provided, the project will be\n            inferred from the credentials or the local environment.\n        credentials: A dictionary containing the service account information and project\n            used for authentication. If not provided, the application default\n            credentials will be used.\n\n    Returns:\n        A dictionary containing the bucket, folder, and local directory where files were downloaded.\n\n    Examples:\n        Pull from GCS using the default environment credentials:\n        ```yaml\n        build:\n            - prefect_gcp.deployments.steps.pull_from_gcs:\n                requires: prefect-gcp\n                bucket: my-bucket\n                folder: my-folder\n        ```\n\n        Pull from GCS using credentials stored in a block:\n        ```yaml\n        build:\n            - prefect_gcp.deployments.steps.pull_from_gcs:\n                requires: prefect-gcp\n                bucket: my-bucket\n                folder: my-folder\n                credentials: \"{{ prefect.blocks.gcp-credentials.dev-credentials }}\"\n        ```\n\n        Pull from to an GCS bucket using credentials stored in a service account file:\n        ```yaml\n        build:\n            - prefect_gcp.deployments.steps.pull_from_gcs:\n                requires: prefect-gcp\n                bucket: my-bucket\n                folder: my-folder\n                credentials:\n                    project: my-project\n                    service_account_file: /path/to/service_account.json\n        ```\n\n    \"\"\"  # noqa\n    local_path = Path.cwd()\n    project = credentials.get(\"project\") if credentials else None\n\n    gcp_creds = None\n    if credentials is not None:\n        if credentials.get(\"service_account_info\") is not None:\n            gcp_creds = Credentials.from_service_account_info(\n                credentials.get(\"service_account_info\"),\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n        elif credentials.get(\"service_account_file\") is not None:\n            gcp_creds = Credentials.from_service_account_file(\n                credentials.get(\"service_account_file\"),\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n\n    gcp_creds = gcp_creds or google.auth.default()[0]\n\n    storage_client = StorageClient(credentials=gcp_creds, project=project)\n\n    blobs = storage_client.list_blobs(bucket, prefix=folder)\n\n    for blob in blobs:\n        if blob.name.endswith(\"/\"):\n            # object is a folder and will be created if it contains any objects\n            continue\n        local_blob_download_path = PurePosixPath(\n            local_path\n            / relative_path_to_current_platform(blob.name).relative_to(folder)\n        )\n        Path.mkdir(Path(local_blob_download_path.parent), parents=True, exist_ok=True)\n\n        blob.download_to_filename(local_blob_download_path)\n\n    return {\n        \"bucket\": bucket,\n        \"folder\": folder,\n        \"directory\": str(local_path),\n    }\n</code></pre>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps.pull_project_from_gcs","title":"<code>pull_project_from_gcs</code>","text":"<p>Deprecated. Use <code>pull_from_gcs</code> instead.</p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>@deprecated_callable(start_date=\"Jun 2023\", help=\"Use `pull_from_gcs` instead.\")\ndef pull_project_from_gcs(*args, **kwargs) -&gt; PullProjectFromGcsOutput:\n    \"\"\"\n    Deprecated. Use `pull_from_gcs` instead.\n    \"\"\"\n    return pull_from_gcs(*args, **kwargs)\n</code></pre>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps.push_project_to_gcs","title":"<code>push_project_to_gcs</code>","text":"<p>Deprecated. Use <code>push_to_gcs</code> instead.</p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>@deprecated_callable(start_date=\"Jun 2023\", help=\"Use `push_to_gcs` instead.\")\ndef push_project_to_gcs(*args, **kwargs) -&gt; PushToGcsOutput:\n    \"\"\"\n    Deprecated. Use `push_to_gcs` instead.\n    \"\"\"\n    return push_to_gcs(*args, **kwargs)\n</code></pre>"},{"location":"deployments/steps/#prefect_gcp.deployments.steps.push_to_gcs","title":"<code>push_to_gcs</code>","text":"<p>Pushes the contents of the current working directory to a GCS bucket, excluding files and folders specified in the ignore_file.</p> <p>Parameters:</p> Name Type Description Default <code>bucket</code> <code>str</code> <p>The name of the GCS bucket where files will be uploaded.</p> required <code>folder</code> <code>str</code> <p>The folder in the GCS bucket where files will be uploaded.</p> required <code>project</code> <code>Optional[str]</code> <p>The GCP project the bucket belongs to. If not provided, the project will be inferred from the credentials or the local environment.</p> <code>None</code> <code>credentials</code> <code>Optional[Dict]</code> <p>A dictionary containing the service account information and project used for authentication. If not provided, the application default credentials will be used.</p> <code>None</code> <code>ignore_file</code> <p>The name of the file containing ignore patterns.</p> <code>'.prefectignore'</code> <p>Returns:</p> Type Description <code>PushToGcsOutput</code> <p>A dictionary containing the bucket and folder where files were uploaded.</p> <p>Examples:</p> <p>Push to a GCS bucket: <pre><code>build:\n    - prefect_gcp.deployments.steps.push_to_gcs:\n        requires: prefect-gcp\n        bucket: my-bucket\n        folder: my-project\n</code></pre></p> <p>Push  to a GCS bucket using credentials stored in a block: <pre><code>build:\n    - prefect_gcp.deployments.steps.push_to_gcs:\n        requires: prefect-gcp\n        bucket: my-bucket\n        folder: my-folder\n        credentials: \"{{ prefect.blocks.gcp-credentials.dev-credentials }}\"\n</code></pre></p> <p>Push to a GCS bucket using credentials stored in a service account file: <pre><code>build:\n    - prefect_gcp.deployments.steps.push_to_gcs:\n        requires: prefect-gcp\n        bucket: my-bucket\n        folder: my-folder\n        credentials:\n            project: my-project\n            service_account_file: /path/to/service_account.json\n</code></pre></p> Source code in <code>prefect_gcp/deployments/steps.py</code> <pre><code>def push_to_gcs(\n    bucket: str,\n    folder: str,\n    project: Optional[str] = None,\n    credentials: Optional[Dict] = None,\n    ignore_file=\".prefectignore\",\n) -&gt; PushToGcsOutput:\n    \"\"\"\n    Pushes the contents of the current working directory to a GCS bucket,\n    excluding files and folders specified in the ignore_file.\n\n    Args:\n        bucket: The name of the GCS bucket where files will be uploaded.\n        folder: The folder in the GCS bucket where files will be uploaded.\n        project: The GCP project the bucket belongs to. If not provided, the project\n            will be inferred from the credentials or the local environment.\n        credentials: A dictionary containing the service account information and project\n            used for authentication. If not provided, the application default\n            credentials will be used.\n        ignore_file: The name of the file containing ignore patterns.\n\n    Returns:\n        A dictionary containing the bucket and folder where files were uploaded.\n\n    Examples:\n        Push to a GCS bucket:\n        ```yaml\n        build:\n            - prefect_gcp.deployments.steps.push_to_gcs:\n                requires: prefect-gcp\n                bucket: my-bucket\n                folder: my-project\n        ```\n\n        Push  to a GCS bucket using credentials stored in a block:\n        ```yaml\n        build:\n            - prefect_gcp.deployments.steps.push_to_gcs:\n                requires: prefect-gcp\n                bucket: my-bucket\n                folder: my-folder\n                credentials: \"{{ prefect.blocks.gcp-credentials.dev-credentials }}\"\n        ```\n\n        Push to a GCS bucket using credentials stored in a service account\n        file:\n        ```yaml\n        build:\n            - prefect_gcp.deployments.steps.push_to_gcs:\n                requires: prefect-gcp\n                bucket: my-bucket\n                folder: my-folder\n                credentials:\n                    project: my-project\n                    service_account_file: /path/to/service_account.json\n        ```\n\n    \"\"\"\n    project = credentials.get(\"project\") if credentials else None\n\n    gcp_creds = None\n    if credentials is not None:\n        if credentials.get(\"service_account_info\") is not None:\n            gcp_creds = Credentials.from_service_account_info(\n                credentials.get(\"service_account_info\"),\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n        elif credentials.get(\"service_account_file\") is not None:\n            gcp_creds = Credentials.from_service_account_file(\n                credentials.get(\"service_account_file\"),\n                scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n            )\n\n    gcp_creds = gcp_creds or google.auth.default()[0]\n\n    storage_client = StorageClient(credentials=gcp_creds, project=project)\n    bucket_resource = storage_client.bucket(bucket)\n\n    local_path = Path.cwd()\n\n    included_files = None\n    if ignore_file and Path(ignore_file).exists():\n        with open(ignore_file, \"r\") as f:\n            ignore_patterns = f.readlines()\n        included_files = filter_files(str(local_path), ignore_patterns)\n\n    for local_file_path in local_path.expanduser().rglob(\"*\"):\n        relative_local_file_path = local_file_path.relative_to(local_path)\n        if (\n            included_files is not None\n            and str(relative_local_file_path) not in included_files\n        ):\n            continue\n        elif not local_file_path.is_dir():\n            remote_file_path = (folder / relative_local_file_path).as_posix()\n\n            blob_resource = bucket_resource.blob(remote_file_path)\n            blob_resource.upload_from_filename(local_file_path)\n\n    return {\n        \"bucket\": bucket,\n        \"folder\": folder,\n    }\n</code></pre>"}]}